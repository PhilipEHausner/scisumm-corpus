This corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of Collins [9]. In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features. In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus. In keeping with the standard methodology [5, 9,10,15,17], we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training, section 23 for testing, and section 24 for development (debugging and tuning). That the previous three best parsers on this test [5,9,17] all perform within a percentage point of each other, despite quite different basic mechanisms, led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training, and to conjecture that perhaps we were at it. In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features. We comment on this because in our example we can substantially speed up the process by choosing values picked so that, when the maximum-entropy equation is expressed in the form of Equation 4, the gi have as their initial values the values of the corresponding terms in Equation 7. In a pure maximum-entropy model this is done by feature selection, as in Ratnaparkhi's maximum-entropy parser [17]. For example, in the Penn Treebank a vp with both main and auxiliary verbs has the structure shown in Figure 3. Given we are already at the 88% level of accuracy, we judge a 0.6% improvement to be very much worth while. We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100. In our work we assume that any feature can occur at most once, so features are boolean-valued: 0 if the pattern does not occur, 1 if it does. We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal tree-bank. As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.
