The parser error , parser coverage , and the uninterpolated model perplexity ( A = 1 ) all suffered substantially from a narrower search , but the interpolated perplexity remained quite good even at the extremes . A new language model that utilizes probabilistic top-down parsing is then outlined , and empirical results show that it improves upon previous work in test corpus perplexity . The small size of our training data , as well as the fact that we are rescoring n-best lists , rather than working directly on lattices , makes comparison with the other models not particularly informative . The small size of our training data , as well as the fact that we are rescoring n-best lists , rather than working directly on lattices , makes comparison with the other models not particularly informative . The rest of this paper will present our parsing model , its application to language modeling for speech recognition , and empirical results . The rest of this paper will present our parsing model , its application to language modeling for speech recognition , and empirical results . Splitting of the contractions is critical for parsing , since the two parts oftentimes ( as in the previous example ) fall in different constituents . A lexicalized probabilistic topdown parser is then presented , which performs very well , in terms of both the accuracy of returned parses and the efficiency with which they are found , relative to the best broad-coverage