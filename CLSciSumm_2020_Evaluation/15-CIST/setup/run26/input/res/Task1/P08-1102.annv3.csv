Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,P08-1102,C08-1049,0,2008,0,"Following Jiang et al (2008), we describe segmentation and Joint S& amp; T as below: For a given Chinese sentence appearing as a character sequence: C 1: n= C 1 C 2.","Following Jiang et al (2008), we describe segmentation and Joint S& amp; T as below: For a given Chinese sentence appearing as a character sequence: C 1: n= C 1 C 2.","[22, 32, 77, 78]","['<S sid=""22"" ssid=""18"">2 Segmentation and POS Tagging Given a Chinese character sequence: while the segmentation and POS tagging result can be depicted as: Here, Ci (i = L.n) denotes Chinese character, ti (i = L.m) denotes POS tag, and Cl:r (l &lt; r) denotes character sequence ranges from Cl to Cr.</S>', '<S sid=""32"" ssid=""4"">We trained a character-based perceptron for Chinese Joint S&amp;T, and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&amp;T.</S>', '<S sid=""77"" ssid=""2"">In Chinese Joint S&amp;T, the mission of the decoder is to find the boundary-POS labelled sequence with the highest score.</S>', '<S sid=""78"" ssid=""3"">Given a Chinese character sequence C1:n, the decoding procedure can proceed in a left-right fashion with a dynamic programming approach.</S>']",Method_Citation
2,P08-1102,C08-1049,0,2008,0,"As described in Ng and Low (2004 )andJiang et al (2008), we use s indicating a single character word, while b, m and e indicating the be gin, middle and end of a word respectively","As described in Ng and Low (2004) and Jiang et al (2008), we use s indicating a single character word, while b, m and e indicating the be gin, middle and end of a word respectively","[25, 31, 34, 71]","['<S sid=""25"" ssid=""21"">According to Ng and Low (2004), the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.</S>', '<S sid=""31"" ssid=""3"">The perceptron has been used in many NLP tasks, such as POS tagging (Collins, 2002), Chinese word segmentation (Ng and Low, 2004; Zhang and Clark, 2007) and so on.</S>', '<S sid=""34"" ssid=""6"">The feature templates we adopted are selected from those of Ng and Low (2004).</S>', '<S sid=""71"" ssid=""22"">Using W = w1:m to denote the word sequence, T = t1:m to denote the corresponding POS sequence, P (T |W) to denote the probability that W is labelled as T, and P(W|T) to denote the probability that T generates W, we can define the cooccurrence model as follows: &#955;wt and &#955;tw denote the corresponding weights of the two components.</S>']",Method_Citation
3,P08-1102,C08-1049,0,2008,0,plates called lexical-target in the column below areintroduced by Jiang et al (2008),plates called lexical-target in the column below are introduced by Jiang et al (2008),"[39, 40, 41, 42]","['<S sid=""39"" ssid=""11"">We called them non-lexical-target because predications derived from them can predicate without considering the current character C0.</S>', '<S sid=""40"" ssid=""12"">Templates in the column below are expanded from the upper ones.</S>', '<S sid=""41"" ssid=""13"">We add a field C0 to each template in the upper column, so that it can carry out predication according to not only the context but also the current character itself.</S>', '<S sid=""42"" ssid=""14"">As predications generated from such templates depend on the current character, we name these templates lexical-target.</S>']",Method_Citation
4,P08-1102,P12-1110,0,2008,0,"For CTB-5, we refer to the split by Duan et al (2007) as CTB-5d, and to the split by Jiang et al (2008) as CTB-5j","For CTB-5, we refer to the split by Duan et al (2007) as CTB-5d, and to the split by Jiang et al (2008) as CTB-5j","[15, 31, 101, 141]","['<S sid=""15"" ssid=""11"">To cope with this problem, we propose a cascaded linear model inspired by the log-linear model (Och and Ney, 2004) widely used in statistical machine translation to incorporate different kinds of knowledge sources.</S>', '<S sid=""31"" ssid=""3"">The perceptron has been used in many NLP tasks, such as POS tagging (Collins, 2002), Chinese word segmentation (Ng and Low, 2004; Zhang and Clark, 2007) and so on.</S>', '<S sid=""101"" ssid=""12"">On the three corpora, it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S>', '<S sid=""141"" ssid=""4"">We would also like to Hwee-Tou Ng for sharing his code, and Yang Liu and Yun Huang for suggestions.</S>']",Method_Citation
5,P08-1102,D12-1126,0,2008,0,Jiang et al (2008) proposes a cascaded linear model for joint Chinese word segmentation and POS tagging,Jiang et al (2008) proposes a cascaded linear model for joint Chinese word segmentation and POS tagging,"[0, 1, 3, 130]","['<S sid=""0"">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S>', '<S sid=""1"" ssid=""1"">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S>', '<S sid=""3"" ssid=""3"">Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging.</S>', '<S sid=""130"" ssid=""1"">We proposed a cascaded linear model for Chinese Joint S&amp;T.</S>']",Method_Citation
6,P08-1102,C10-1135,0,2008,0,"We use the feature templates the same as Jiang et al, (2008) to extract features form E model","We use the feature templates the same as Jiang et al, (2008) to extract features form E model","[36, 51, 113, 122]","['<S sid=""36"" ssid=""8"">All feature templates and their instances are shown in Table 1.</S>', '<S sid=""51"" ssid=""2"">Additional features most widely used are related to word or POS ngrams.</S>', '<S sid=""113"" ssid=""24"">Besides this perceptron, other sub-models are trained and used as additional features of the outside-layer linear model.</S>', '<S sid=""122"" ssid=""33"">Another important feature is the labelling model.</S>']",Method_Citation
8,P08-1102,P12-1025,0,"Jiangetal., 2008a",0,"approach, where basic processing units are characters which compose words (Jiangetal., 2008a)","basic processing units are characters which compose words (Jiangetal., 2008a)","[24, 42, 85, 86]","['<S sid=""24"" ssid=""20"">It is a better idea to perform segmentation and POS tagging jointly in a uniform framework.</S>', '<S sid=""42"" ssid=""14"">As predications generated from such templates depend on the current character, we name these templates lexical-target.</S>', '<S sid=""85"" ssid=""10"">Lines 3 &#8212; 11 generate a N-best list for each character position i.</S>', '<S sid=""86"" ssid=""11"">Line 4 scans words of all possible lengths l (l = 1.. min(i, K), where i points to the current considering character).</S>']",Method_Citation
9,P08-1102,C10-2096,0,2008b,0,"The solid lines show the 1-best result, which is wrong. Jiang et al (2008b) stress the problems in re ranking phase","The solid lines show the 1-best result, which is wrong. Jiang et al (2008b) stress the problems in re ranking phase","[85, 88, 100, 107]","['<S sid=""85"" ssid=""10"">Lines 3 &#8212; 11 generate a N-best list for each character position i.</S>', '<S sid=""88"" ssid=""13"">Line 8 considers each candidate result in N-best list at prior position of the current word.</S>', '<S sid=""100"" ssid=""11"">Test results listed in Table 2 shows that this model obtains higher accuracy than the best of SIGHAN Bakeoff 2 in three corpora (AS, CityU and MSR).</S>', '<S sid=""107"" ssid=""18"">The evaluation results are shown in Table 3.</S>']",Method_Citation
10,P08-1102,C10-2096,0,"Jiang et al, 2008a",0,"6.1.1 Baseline Forest-based System We first segment the Chinese sentences into the1-best segmentations using a state-of-the-art system (Jiang et al, 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results","We first segment the Chinese sentences into the 1-best segmentations using a state-of-the-art system (Jiang et al, 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results","[22, 24, 31, 105]","['<S sid=""22"" ssid=""18"">2 Segmentation and POS Tagging Given a Chinese character sequence: while the segmentation and POS tagging result can be depicted as: Here, Ci (i = L.n) denotes Chinese character, ti (i = L.m) denotes POS tag, and Cl:r (l &lt; r) denotes character sequence ranges from Cl to Cr.</S>', '<S sid=""24"" ssid=""20"">It is a better idea to perform segmentation and POS tagging jointly in a uniform framework.</S>', '<S sid=""31"" ssid=""3"">The perceptron has been used in many NLP tasks, such as POS tagging (Collins, 2002), Chinese word segmentation (Ng and Low, 2004; Zhang and Clark, 2007) and so on.</S>', '<S sid=""105"" ssid=""16"">At the first step, we conducted a group of contrasting experiments on the core perceptron, the first concentrated on the segmentation regardless of the POS information and reported the F-measure on segmentation only, while the second performed Joint S&amp;T using POS information and reported the F-measure both on segmentation and on Joint S&amp;T.</S>']",Results_Citation
11,P08-1102,C10-2096,0,"Jiang et al, 2008a",0,"6.1.2 Lattice-forest SystemWe first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al, 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm","We first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al, 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm","[5, 22, 58, 70]","['<S sid=""5"" ssid=""1"">Word segmentation and part-of-speech (POS) tagging are important tasks in computer processing of Chinese and other Asian languages.</S>', '<S sid=""22"" ssid=""18"">2 Segmentation and POS Tagging Given a Chinese character sequence: while the segmentation and POS tagging result can be depicted as: Here, Ci (i = L.n) denotes Chinese character, ti (i = L.m) denotes POS tag, and Cl:r (l &lt; r) denotes character sequence ranges from Cl to Cr.</S>', '<S sid=""58"" ssid=""9"">Instead of incorporating all features into the perceptron directly, we first trained the perceptron using character-based features, and several other sub-models using additional ones such as word or POS n-grams, then trained the outside-layer linear model using the outputs of these sub-models, including the perceptron.</S>', '<S sid=""70"" ssid=""21"">Given a training corpus with POS tags, we can train a word-POS co-occurrence model to approximate the probability that the word sequence of the labelled result co-exists with its corresponding POS sequence.</S>']",Method_Citation
12,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"However, when we repeat the work of (Jiang et al, 2008), which reports to achieve the state-of-art performance in the data-sets that we adopt, it has been found that some features (e.g., C0) are unnoticeably trained several times in their model (which are implicitly generated from different feature templates used in the paper)","However, when we repeat the work of (Jiang et al, 2008), which reports to achieve the state-of-art performance in the data-sets that we adopt, it has been found that some features (e.g., C0) are unnoticeably trained several times in their model (which are implicitly generated from different feature templates used in the paper)","[8, 14, 114, 133]","['<S sid=""8"" ssid=""4"">Another widely used discriminative method is the perceptron algorithm (Collins, 2002), which achieves comparable performance to CRFs with much faster training, so we base this work on the perceptron.</S>', '<S sid=""14"" ssid=""10"">As a result, many theoretically useful features such as higherorder word or POS n-grams are difficult to be incorporated in the model efficiently.</S>', '<S sid=""114"" ssid=""25"">We used SRI Language Modelling Toolkit (Stolcke and Andreas, 2002) to train a 3gram word LM with modified Kneser-Ney smoothing (Chen and Goodman, 1998), and a 4-gram POS LM with Witten-Bell smoothing, and we trained a word-POS co-occurrence model simply by MLE without smoothing.</S>', '<S sid=""133"" ssid=""4"">However, can the perceptron incorporate all the knowledge used in the outside-layer linear model?</S>']",Method_Citation
13,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"Unicode/CP936 1.1M/55K 104K/13K 0.035 Table 3: Corpus statistics for the second SIGHAN Bakeoff appears twice, which is generated from two different templates Cn (with n=0, generates C0) and [C0Cn] (used in (Jiang et al, 2008), with n=0, generates [C0C0])","Table 3: Corpus statistics for the second SIGHAN Bakeoff appears twice, which is generated from two different templates Cn (with n=0, generates C0) and [C0Cn] (used in (Jiang et al, 2008), with n=0, generates [C0C0])","[71, 72, 74, 100]","['<S sid=""71"" ssid=""22"">Using W = w1:m to denote the word sequence, T = t1:m to denote the corresponding POS sequence, P (T |W) to denote the probability that W is labelled as T, and P(W|T) to denote the probability that T generates W, we can define the cooccurrence model as follows: &#955;wt and &#955;tw denote the corresponding weights of the two components.</S>', '<S sid=""72"" ssid=""23"">Suppose the conditional probability Pr(t|w) describes the probability that the word w is labelled as the POS t, while Pr(w|t) describes the probability that the POS t generates the word w, then P(T|W) can be approximated by: Pr(w|t) and Pr(t|w) can be easily acquired by Maximum Likelihood Estimates (MLE) over the corpus.</S>', '<S sid=""74"" ssid=""25"">To facilitate tuning the weights, we use two components of the co-occurrence model Co(W,T) to represent the co-occurrence probability of W and T, rather than use Co(W, T) itself.</S>', '<S sid=""100"" ssid=""11"">Test results listed in Table 2 shows that this model obtains higher accuracy than the best of SIGHAN Bakeoff 2 in three corpora (AS, CityU and MSR).</S>']",Method_Citation
14,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"As all the features adopted in (Jiang et al, 2008) possess binary values, if a binary feature is repeated n times, then it should behave like a real-valued feature with its value to be? n?, at least in principle","As all the features adopted in (Jiang et al, 2008) possess binary values, if a binary feature is repeated n times, then it should behave like a real-valued feature with its value to be n, at least in principle","[16, 34, 50, 51]","['<S sid=""16"" ssid=""12"">Shown in Figure 1, the cascaded model has a two-layer architecture, with a characterbased perceptron as the core combined with other real-valued features such as language models.</S>', '<S sid=""34"" ssid=""6"">The feature templates we adopted are selected from those of Ng and Low (2004).</S>', '<S sid=""50"" ssid=""1"">In theory, any useful knowledge can be incorporated into the perceptron directly, besides the characterbased features already adopted.</S>', '<S sid=""51"" ssid=""2"">Additional features most widely used are related to word or POS ngrams.</S>']",Method_Citation
15,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"Inspired by (Jiang et al, 2008), we set the real d Although Table 5 has shown that the proposed all the value of C0 to be 2.0, the value of C-1C0anC0C1 to be 3.0, and the values of all other features to be 1.0 for the character-based discriminative-plus model","Inspired by (Jiang et al, 2008), we set the real d Although Table 5 has shown that the proposed all the value of C0 to be 2.0, the value of C-1C0anC0C1 to be 3.0, and the values of all other features to be 1.0 for the character-based discriminative-plus model","[2, 36, 65, 129]","['<S sid=""2"" ssid=""2"">With a character-based perceptron as the core, combined with realvalued features such as language models, the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S>', '<S sid=""36"" ssid=""8"">All feature templates and their instances are shown in Table 1.</S>', '<S sid=""65"" ssid=""16"">As shown in Figure 1, the character-based perceptron is used as the inside-layer linear model and sends its output to the outside-layer.</S>', '<S sid=""129"" ssid=""40"">Experimental results show that, it achieves obvious improvement over the perceptron-only model, about from 0.973 to 0.978 on segmentation, and from 0.925 to 0.934 on Joint S&amp;T, with error reductions of 18.5% and 12% respectively.</S>']",Results_Citation
17,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"Last, (Jiang et al, 2008) 5 adds repeated features implicitly based on (Ng and Low, 2004)","Last, (Jiang et al, 2008) 5 adds repeated features implicitly based on (Ng and Low, 2004)","[11, 15, 34, 43]","['<S sid=""11"" ssid=""7"">Compared to performing segmentation and POS tagging one at a time, Joint S&amp;T can achieve higher accuracy not only on segmentation but also on POS tagging (Ng and Low, 2004).</S>', '<S sid=""15"" ssid=""11"">To cope with this problem, we propose a cascaded linear model inspired by the log-linear model (Och and Ney, 2004) widely used in statistical machine translation to incorporate different kinds of knowledge sources.</S>', '<S sid=""34"" ssid=""6"">The feature templates we adopted are selected from those of Ng and Low (2004).</S>', '<S sid=""43"" ssid=""15"">Note that the templates of Ng and Low (2004) have already contained some lexical-target ones.</S>']",Method_Citation
20,P08-1102,D12-1046,0,Jiang et al2008a,0,"Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al2010), cascaded linear model (Jiang et al2008a) ,perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), re ranking (Jiang et al2008b)","Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al2010), cascaded linear model (Jiang et al2008a), perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), re ranking (Jiang et al2008b)","[0, 1, 10, 31]","['<S sid=""0"">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S>', '<S sid=""1"" ssid=""1"">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S>', '<S sid=""10"" ssid=""6"">Since the typical approach of discriminative models treats segmentation as a labelling problem by assigning each character a boundary tag (Xue and Shen, 2003), Joint S&amp;T can be conducted in a labelling fashion by expanding boundary tags to include POS information (Ng and Low, 2004).</S>', '<S sid=""31"" ssid=""3"">The perceptron has been used in many NLP tasks, such as POS tagging (Collins, 2002), Chinese word segmentation (Ng and Low, 2004; Zhang and Clark, 2007) and so on.</S>']",Method_Citation
