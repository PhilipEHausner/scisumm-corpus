This represents a 13% decrease in error rate over the best single-parser results on this corpus [9]. This is as opposed to the &amp;quot;Markovgrammar&amp;quot; approach used in the current parser. Also, the earlier parser uses two techniques not employed in the current parser. First, it uses a clustering scheme on words to give the system a &amp;quot;soft&amp;quot; clustering of heads and sub-heads. We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less, and for of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal treebank. We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal tree-bank. As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model. The results of [13] achieved by combining the aforementioned three-best parsers also suggest that the limit on tree-bank trained parsers is much higher than previously thought. That is, for all sentences s and all parses 7r, the parser assigns a probability p(s , 7r) = p(r),