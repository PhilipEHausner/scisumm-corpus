8211 ; i.e. More than 350 topics in the Finnish LDA model have zero tokens assigned to them , and almost all tokens are assigned to the largest 200 topics . 8220 ; glue & Smoothed histograms of inter & Figure 2 shows the most probable words in all languages for four example topics , from PLTM with 400 topics . We introduced a polylingual topic model ( PLTM ) that discovers topics aligned across multiple languages . 8220 ; glue & We introduce a polylingual topic model that discovers topics aligned across multiple languages . 8221 ; language ( E ) : Wte and Wtt , respectively . No paper is exactly comparable to any other paper , but they are all roughly topically similar . 8220 ; glue & Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model ( Zhao and Xing , 2007 ) . 8212 ; it is clear from the examples in figure 2 that the sets of high probability words in different languages for a given topic are likely to include translations . 8212 ; it has no way to represent syntax or even multi-word phrases & ( Ni et al. , 2009 ) , discusses a multilingual topic model similar to the one presented here . We then add the Cartesian product of these sets for every topic to a set of candidate translations C. Figure 2 shows the most probable words in all languages