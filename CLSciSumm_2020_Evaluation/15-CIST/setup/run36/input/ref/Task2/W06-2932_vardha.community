The first stage of our system creates an unlabeled parse y for an input sentence x. However, in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input. Performance is measured through unlabeled accuracy, which is the percentage of words that modify the correct head in the dependency graph, and labeled accuracy, which is the percentage of words that modify the correct head and label the dependency edge correctly in the graph. For instance, sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish. Based on performance from a held-out section of the training data, we used non-projective parsing algorithms for Czech, Danish, Dutch, German, Japanese, Portuguese and Slovene, and projective parsing algorithms for Arabic, Bulgarian, Chinese, Spanish, Swedish and Turkish. We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al., 2005b; McDonald and Pereira, 2006) generalizes well to languages other than English. In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler. These results show that the discriminative spanning tree parsing framework (McDonald et al., 2005b; McDonald and Pereira, 2006) is easily adapted across all these languages. An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge. To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm&#65533;1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm&#8722;1) in the tree y. That work extends the maximum spanning tree dependency parsing framework (McDonald et al., 2005a; McDonald et al., 2005b) to incorporate features over multiple edges in the dependency graph. N/P: Allow non-projective/Force projective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available. Performance is measured through unlabeled accuracy, which is the percentage of words that modify the correct head in the dependency graph, and labeled accuracy, which is the percentage of words that modify the correct head and label the edge correctly in the graph. For score functions, we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation, we can find the highest scoring label sequence with Viterbi&#8217;s algorithm.
