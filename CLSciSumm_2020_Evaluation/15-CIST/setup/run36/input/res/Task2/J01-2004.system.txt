A second key feature of our approach is that top-down guidance improves the efficiency of the search as more and more conditioning events are extracted from the derivation for use in the probabilistic model . Thus , our top-down parser allows for the incremental calculation of generative conditional word probabilities , a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers ( Earley 1970 ) or left-corner parsers ( Rosenkrantz and Lewis 11 1970 ) . The parsers with the highest published broad-coverage parsing accuracy , which include Charniak ( 1997 , 2000 ) , Collins ( 1997 , 1999 ) , and Ratnaparkhi ( 1997 ) , all utilize simple and straightforward statistically based search heuristics , pruning the search-space quite dramatically ! \ . The small size of our training data , as well as the fact that we are rescoring n-best lists , rather than working directly on lattices , makes comparison with the other models not particularly informative . The model allows us to assign probabilities to derivations , which can be used by the parsing algorithm to decide heuristically which candidates are promising and should be expanded , and which are less promising and should be pruned . In addition , as mentioned above , we would like to further test our language model in speech recognition tasks , to see if the perplexity improvement that we have seen can lead to significant reductions in word error rate . A