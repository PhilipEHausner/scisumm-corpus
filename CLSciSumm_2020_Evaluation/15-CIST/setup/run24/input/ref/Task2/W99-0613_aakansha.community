This paper discusses the use of unlabeled examples for the problem of named entity classification. Roughly speaking, the new algorithm presented in this paper performs a similar search, but instead minimizes a bound on the number of (unlabeled) examples on which two classifiers disagree. The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel. d="79" ssid="12">2 We now introduce a new algorithm for learning from unlabeled examples, which we will call DLCoTrain (DL stands for decision list, the term Cotrain is taken from (Blum and Mitchell 98)).</S> The task is to learn a function from an input string (proper name) to its type, which we will assume to be one of the categories Person, Organization, or Location. But we will show that the use of unlabeled data can drastically reduce the need for supervision. We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories. The numbers falling into the location, person, organization categories were 186, 289 and 402 respectively. We present two algorithms. The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98). But we will show that the use of unlabeled data can drastically reduce the need for supervision. (If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper. Recent results (e.g., (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.
