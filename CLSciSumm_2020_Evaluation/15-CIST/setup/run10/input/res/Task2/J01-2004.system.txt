In the past few years, however, some improvements have been made over these language models through the use of statistical methods of natural language processing, and the development of innovative, linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists. While language models built around shallow local dependencies are still the standard in state-of-the-art speech recognition systems, there is reason to hope that better language models can and will be developed by computational linguists for this task. We will focus our very brief review, however, on those that use grammars or parsing for their language models. One approach to syntactic language modeling is to use this distribution directly as a language model. Perhaps one reason for this is that, until relatively recently, few methods have come out of the natural language processing community that were shown to improve upon the very simple language models still standardly in use in speech recognition systems. The rest of this paper will present our parsing model, its application to language modeling for speech recognition, and empirical results. Perplexity is a standard measure within the speech recognition community for comparing language models. A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers. A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned