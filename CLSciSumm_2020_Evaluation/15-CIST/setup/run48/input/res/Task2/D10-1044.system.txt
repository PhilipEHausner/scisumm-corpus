We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set . Domain adaptation is a common concern when optimizing empirical NLP applications . We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set . Domain adaptation is a common concern when optimizing empirical NLP applications . 8212 ; is also available . This best instance-weighting model beats the equivalant model without instance weights by between 0.6 BLEU and 1.8 BLEU , and beats the log-linear baseline by a large margin . We will also directly compare with a baseline similar to the Matsoukas et al approach in order to measure the benefit from weighting phrase pairs ( or ngrams ) rather than full sentences . We focus here instead on adapting the two most important features : the language model ( LM ) , which estimates the probability p ( wIh ) of a target word w following an ngram h ; and the translation models ( TM ) p ( slt ) and p ( t1s ) , which give the probability of source phrase s translating to target phrase t , and vice versa . It is difficult to directly compare the Matsoukas et al results with ours , since our out-of-domain corpus is homogeneous ;