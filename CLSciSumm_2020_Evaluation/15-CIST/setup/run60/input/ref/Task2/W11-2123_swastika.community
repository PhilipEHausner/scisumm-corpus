We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs. This paper presents methods to query N-gram language models, minimizing time and space costs. Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations. However, TRIE partitions storage by n-gram length, so walking the trie reads N disjoint pages. The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher. We attain these results using several optimizations: hashing, custom lookup tables, bit-level packing, and state for left-to-right query patterns.
