We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs. The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task. Training and testing is based on the Europarl corpus. The BLEU metric, as all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use. We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU. Confidence Interval: To estimate confidence intervals for the average mean scores for the systems, we use standard significance testing. The human judges were presented with the following definition of adequacy and fluency, but no additional instructions: To lower the barrier of entrance to the competition, we provided a complete baseline MT system, along with data resources. The test set included 2000 sentences from the Europarl corpus, but also 1064 sentences out-ofdomain test data. Out-of-domain test data is from the Project Syndicate web site, a compendium of political commentary. Another way to view the judgements is that they are less quality judgements of machine translation systems per se, but rankings of machine translation systems. &#8226; We evaluated translation from English, in addition to into English. English was again paired with German, French, and Spanish.
