This paper discusses the use of unlabeled examples for the problem of named entity classification. To prevent this we &amp;quot;smooth&amp;quot; the confidence by adding a small value, e, to both W+ and W_, giving at = Plugging the value of at from Equ. The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel. There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features. S sid="213" ssid="80">Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.</S> Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules. Roughly speaking, the new algorithm presented in this paper performs a similar search, but instead minimizes a bound on the number of (unlabeled) examples on which two classifiers disagree. Unfortunately, Yarowsky's method is not well understood from a theoretical viewpoint: we would like to formalize the notion of redundancy in unlabeled data, and set up the learning task as optimization of some appropriate objective function. Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision, in the form of labeled training examples. (If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper. (Specifically, the limit n starts at 5 and increases by 5 at each iteration.) Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.
