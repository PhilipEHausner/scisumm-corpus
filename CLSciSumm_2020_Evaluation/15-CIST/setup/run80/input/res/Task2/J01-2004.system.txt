A lexicalized probabilistic topdown parser is then presented , which performs very well , in terms of both the accuracy of returned parses and the efficiency with which they are found , relative to the best broad-coverage statistical parsers . In addition , as mentioned above , we would like to further test our language model in speech recognition tasks , to see if the perplexity improvement that we have seen can lead to significant reductions in word error rate . This interpolation is recursively applied to the smaller-order n-grams until the bigram is finally interpolated with the unigram , i.e. The point of this small experiment was to see if our parsing model could provide useful information even in the case that recognition errors occur , as opposed to the ( generally ) fully grammatical strings upon which the perplexity results were obtained . ( 1997 ) , Johnson ( 1998 ) & The point of this small experiment was to see if our parsing model could provide useful information even in the case that recognition errors occur , as opposed to the ( generally ) fully grammatical strings upon which the perplexity results were obtained . A lexicalized probabilistic topdown parser is then presented , which performs very well , in terms of both the accuracy of returned parses and the efficiency with which they are found , relative to the best broad-coverage statistical parsers . After 1,000 candidates , the beam has narrowed to 10-2p . The