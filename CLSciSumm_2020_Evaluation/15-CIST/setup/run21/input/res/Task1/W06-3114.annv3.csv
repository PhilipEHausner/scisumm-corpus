Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,W06-3114,W06-3120,0,"Koehn and Monz, 2006",0,"The official results were slightly better because a lowercase evaluation was used, see (Koehn and Monz, 2006)","The official results were slightly better because a lowercase evaluation was used, see (Koehn and Monz, 2006)","'35','68','70','154'","<S sid=""35"" ssid=""1"">For the automatic evaluation, we used BLEU, since it is the most established metric in the field.</S><S sid=""68"" ssid=""7"">We asked participants to each judge 200&#8211;300 sentences in terms of fluency and adequacy, the most commonly used manual evaluation metrics.</S><S sid=""70"" ssid=""9"">See Figure 3 for a screenshot of the evaluation tool.</S><S sid=""154"" ssid=""47"">While we used the standard metrics of the community, the we way presented translations and prompted for assessment differed from other evaluation campaigns.</S>",Method_Citation
2,W06-3114,D07-1092,0,"Koehn and Monz, 2006",0,"We are further focusing on the shared task of the workshop on Statistical Machine Translation, which took place last year (Koehn and Monz, 2006) and consisted in translating Spanish, German, and French texts from and to English","We are further focusing on the shared task of the workshop on Statistical Machine Translation, which took place last year (Koehn and Monz, 2006) and consisted in translating Spanish, German, and French texts from and to English","'6','7','29','75'","<S sid=""6"" ssid=""4"">English was again paired with German, French, and Spanish.</S><S sid=""7"" ssid=""5"">We dropped, however, one of the languages, Finnish, partly to keep the number of tracks manageable, partly because we assumed that it would be hard to find enough Finnish speakers for the manual evaluation.</S><S sid=""29"" ssid=""22"">About half of the participants of last year&#8217;s shared task participated again.</S><S sid=""75"" ssid=""14"">Our initial experimentation with the evaluation tool showed that this is often too overwhelming.</S>","Hypothesis_Citation,Method_Citation"
3,W06-3114,C08-1074,0,"Koehn and Monz, 2006",0,"For our training and test data we used the English-French subset of the Europarl corpus provided for the shared task (Koehn and Monz, 2006) at the Statistical Machine Translation workshop held in conjunction with the 2006 HLT-NAACL conference","For our training and test data we used the English-French subset of the Europarl corpus provided for the shared task (Koehn and Monz, 2006) at the Statistical Machine Translation workshop held in conjunction with the 2006 HLT-NAACL conference","'9','10','16','47'","<S sid=""9"" ssid=""2"">Training and testing is based on the Europarl corpus.</S><S sid=""10"" ssid=""3"">Figure 1 provides some statistics about this corpus.</S><S sid=""16"" ssid=""9"">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000, which is excluded from the training data.</S><S sid=""47"" ssid=""13"">Because of this, we retokenized and lowercased submitted output with our own tokenizer, which was also used to prepare the training and test data.</S>",Hypothesis_Citation
4,W06-3114,W07-0718,0,"Koehn and Monz, 2006",0,"The results of last year? s workshop further suggested that Bleu systematically underestimated the quality of rule-based machine translation systems (Koehn and Monz, 2006)","The results of last year's workshop further suggested that Bleu systematically underestimated the quality of rule-based machine translation systems (Koehn and Monz, 2006)","'13','27','90','140'","<S sid=""13"" ssid=""6"">We are currently working on a complete open source implementation of a training and decoding system, which should become available over the summer. pus, from which also the in-domain test set is taken.</S><S sid=""27"" ssid=""20"">Microsoft&#8217;s approach uses dependency trees, others use hierarchical phrase models.</S><S sid=""90"" ssid=""6"">Another way to view the judgements is that they are less quality judgements of machine translation systems per se, but rankings of machine translation systems.</S><S sid=""140"" ssid=""33"">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>",Hypothesis_Citation
5,W06-3114,P07-1083,0,"Koehn and Monz, 2006",0,"For the bi text-based annotation, we use publicly available word alignments from the Europarl corpus, automatically generated by GIZA++ for FrenchEnglish (Fr), Spanish-English (Es) and GermanEnglish (De) (Koehn and Monz, 2006)","For the bi text-based annotation, we use publicly available word alignments from the Europarl corpus, automatically generated by GIZA++ for FrenchEnglish (Fr), Spanish-English (Es) and German-English (De) (Koehn and Monz, 2006)","'0','9','17','28'","<S sid=""0"">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid=""9"" ssid=""2"">Training and testing is based on the Europarl corpus.</S><S sid=""17"" ssid=""10"">Participants were also provided with two sets of 2,000 sentences of parallel text to be used for system development and tuning.</S><S sid=""28"" ssid=""21"">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S>",Method_Citation
6,W06-3114,W07-0738,0,2006,0,"Evaluation results recently reported by Callison-Burch et al (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MTquality indicator","Evaluation results recently reported by Callison-Burch et al (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MTquality indicator","'35','36','37','39'","<S sid=""35"" ssid=""1"">For the automatic evaluation, we used BLEU, since it is the most established metric in the field.</S><S sid=""36"" ssid=""2"">The BLEU metric, as all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use.</S><S sid=""37"" ssid=""3"">It rewards matches of n-gram sequences, but measures only at most indirectly overall grammatical coherence.</S><S sid=""39"" ssid=""5"">However, a recent study (Callison-Burch et al., 2006), pointed out that this correlation may not always be strong.</S>",Hypothesis_Citation
7,W06-3114,W07-0738,0,2006,0,"For instance, Callison-Burch et al (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al, 2001)","For instance, Callison-Burch et al (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al, 2001)","'11','39','139','140'","<S sid=""11"" ssid=""4"">To lower the barrier of entrance to the competition, we provided a complete baseline MT system, along with data resources.</S><S sid=""39"" ssid=""5"">However, a recent study (Callison-Burch et al., 2006), pointed out that this correlation may not always be strong.</S><S sid=""139"" ssid=""32"">Given the closeness of most systems and the wide over-lapping confidence intervals it is hard to make strong statements about the correlation between human judgements and automatic scoring methods such as BLEU.</S><S sid=""140"" ssid=""33"">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>","Aim_Citation,Hypothesis_Citation"
8,W06-3114,W07-0738,0,2006,0,Wepresenta comparative study on the behavior of several metric representatives from each linguistic level in the context of some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006) (see Section 3),We present a comparative study on the behavior of several metric representatives from each linguistic level in the context of some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006) (see Section 3),"'39','47','83','140'","<S sid=""39"" ssid=""5"">However, a recent study (Callison-Burch et al., 2006), pointed out that this correlation may not always be strong.</S><S sid=""47"" ssid=""13"">Because of this, we retokenized and lowercased submitted output with our own tokenizer, which was also used to prepare the training and test data.</S><S sid=""83"" ssid=""22"">The number of judgements is additionally fragmented by our breakup of sentences into in-domain and out-of-domain.</S><S sid=""140"" ssid=""33"">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>",Method_Citation
9,W06-3114,W07-0738,0,2006,0,Weanalyze some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006),We analyze some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006),"'22','39','49','140'","<S sid=""22"" ssid=""15"">The text type are editorials instead of speech transcripts.</S><S sid=""39"" ssid=""5"">However, a recent study (Callison-Burch et al., 2006), pointed out that this correlation may not always be strong.</S><S sid=""49"" ssid=""15"">Hence, we use the bootstrap resampling method described by Koehn (2004).</S><S sid=""140"" ssid=""33"">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>",Aim_Citation
10,W06-3114,D07-1030,0,"Koehn and Monz, 2006",0,"We use the same method described in (Koehn and Monz, 2006) to perform the significance test","We use the same method described in (Koehn and Monz, 2006) to perform the significance test","'49','52','57','58'","<S sid=""49"" ssid=""15"">Hence, we use the bootstrap resampling method described by Koehn (2004).</S><S sid=""52"" ssid=""18"">Pairwise comparison: We can use the same method to assess the statistical significance of one system outperforming another.</S><S sid=""57"" ssid=""23"">We are therefore applying a different method, which has been used at the 2005 DARPA/NIST evaluation.</S><S sid=""58"" ssid=""24"">We divide up each test set into blocks of 20 sentences (100 blocks for the in-domain test set, 53 blocks for the out-of-domain test set), check for each block, if one system has a higher BLEU score than the other, and then use the sign test.</S>",Method_Citation
11,W06-3114,D07-1030,0,"Koehn and Monz, 2016",0,"We also manually evaluated the RBMT systems and SMT systems in terms of both adequacy and fluency as defined in (Koehn and Monz, 2006)","We also manually evaluated the RBMT systems and SMT systems in terms of both adequacy and fluency as defined in (Koehn and Monz, 2006)","'46','119','123','130'","<S sid=""46"" ssid=""12"">By taking the ratio of matching n-grams to the total number of n-grams in the system output, we obtain the precision pn for each n-gram order n. These values for n-gram precision are combined into a BLEU score: The formula for the BLEU metric also includes a brevity penalty for too short output, which is based on the total number of words in the system output c and in the reference r. BLEU is sensitive to tokenization.</S><S sid=""119"" ssid=""12"">There may be occasionally a system clearly at the top or at the bottom, but most systems are so close that it is hard to distinguish them.</S><S sid=""123"" ssid=""16"">For the manual scoring, we can distinguish only half of the systems, both in terms of fluency and adequacy.</S><S sid=""130"" ssid=""23"">This is demonstrated by average scores over all systems, in terms of BLEU, fluency and adequacy, as displayed in Figure 5.</S>",Method_Citation
12,W06-3114,W08-0406,0,"Koehn and Monz, 2017",0,"The baseline is the PSMT system used for the 2006 NAACL SMT workshop (Koehn and Monz, 2006) with phrase length 3 and a trigram language model (Stolcke, 2002)","The baseline is the PSMT system used for the 2006 NAACL SMT workshop (Koehn and Monz, 2006) with phrase length 3 and a trigram language model (Stolcke, 2002)","'13','14','27','149'","<S sid=""13"" ssid=""6"">We are currently working on a complete open source implementation of a training and decoding system, which should become available over the summer. pus, from which also the in-domain test set is taken.</S><S sid=""14"" ssid=""7"">There is twice as much language modelling data, since training data for the machine translation system is filtered against sentences of length larger than 40 words.</S><S sid=""27"" ssid=""20"">Microsoft&#8217;s approach uses dependency trees, others use hierarchical phrase models.</S><S sid=""149"" ssid=""42"">This is not completely surprising, since all systems use very similar technology.</S>",Aim_Citation
13,W06-3114,W11-1002,0,2006,0,"Callison-Burch et al (2006 )andKoehn and Monz (2006), 0,for example, study situations where BLEU strongly disagrees with human judgment of translation quality","Callison-Burch et al (2006) and Koehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality","'37','39','139','140'","<S sid=""37"" ssid=""3"">It rewards matches of n-gram sequences, but measures only at most indirectly overall grammatical coherence.</S><S sid=""39"" ssid=""5"">However, a recent study (Callison-Burch et al., 2006), pointed out that this correlation may not always be strong.</S><S sid=""139"" ssid=""32"">Given the closeness of most systems and the wide over-lapping confidence intervals it is hard to make strong statements about the correlation between human judgements and automatic scoring methods such as BLEU.</S><S sid=""140"" ssid=""33"">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>",Hypothesis_Citation
14,W06-3114,D07-1091,0,"Koehn and Monz, 2006",0,"The English? German systems were trained on the full 751,088 sentence Europarl corpus and evaluated on the WMT 2006 test set (Koehn and Monz, 2006)","The English German systems were trained on the full 751,088 sentence Europarl corpus and evaluated on the WMT 2006 test set (Koehn and Monz, 2006)","'17','21','54','126'","<S sid=""17"" ssid=""10"">Participants were also provided with two sets of 2,000 sentences of parallel text to be used for system development and tuning.</S><S sid=""21"" ssid=""14"">The out-of-domain test set differs from the Europarl data in various ways.</S><S sid=""54"" ssid=""20"">To check for this, we do pairwise bootstrap resampling: Again, we repeatedly sample sets of sentences, this time from both systems, and compare their BLEU scores on these sets.</S><S sid=""126"" ssid=""19"">The test set included 2000 sentences from the Europarl corpus, but also 1064 sentences out-ofdomain test data.</S>",Aim_Citation
15,W06-3114,D07-1091,0,"Koehn and Monz, 2006",0,"We report results on the development test set, which is also the out-of-domain test set of the WMT06 workshop shared task (Koehn and Monz, 2006)","We report results on the development test set, which is also the out-of-domain test set of the WMT06 workshop shared task (Koehn and Monz, 2006)","'16','17','18','21'","<S sid=""16"" ssid=""9"">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000, which is excluded from the training data.</S><S sid=""17"" ssid=""10"">Participants were also provided with two sets of 2,000 sentences of parallel text to be used for system development and tuning.</S><S sid=""18"" ssid=""11"">In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.</S><S sid=""21"" ssid=""14"">The out-of-domain test set differs from the Europarl data in various ways.</S>",Aim_Citation
16,W06-3114,P07-1108,0,"Koehn and Monz, 2006",0,"A shared task to evaluate machine translation performance was organized as part of the NAACL/HLT 2006 Workshop on Statistical Ma chine Translation (Koehn and Monz, 2006)","A shared task to evaluate machine translation performance was organized as part of the NAACL/HLT 2006 Workshop on Statistical Machine Translation (Koehn and Monz, 2006)","'0','26','39','64'","<S sid=""0"">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid=""26"" ssid=""19"">Most of these groups follow a phrase-based statistical approach to machine translation.</S><S sid=""39"" ssid=""5"">However, a recent study (Callison-Burch et al., 2006), pointed out that this correlation may not always be strong.</S><S sid=""64"" ssid=""3"">Also, the argument has been made that machine translation performance should be evaluated via task-based evaluation metrics, i.e. how much it assists performing a useful task, such as supporting human translators or aiding the analysis of texts.</S>",Method_Citation
18,W06-3114,E12-3010,0,"Koehn and Monz, 2006",0,"For the same reason, human evaluation metrics based on adequacy and fluency were not suitable either (Koehn and Monz, 2006)","For the same reason, human evaluation metrics based on adequacy and fluency were not suitable either (Koehn and Monz, 2006)","'63','64','68','175'","<S sid=""63"" ssid=""2"">Many human evaluation metrics have been proposed.</S><S sid=""64"" ssid=""3"">Also, the argument has been made that machine translation performance should be evaluated via task-based evaluation metrics, i.e. how much it assists performing a useful task, such as supporting human translators or aiding the analysis of texts.</S><S sid=""68"" ssid=""7"">We asked participants to each judge 200&#8211;300 sentences in terms of fluency and adequacy, the most commonly used manual evaluation metrics.</S><S sid=""175"" ssid=""6"">Replacing this with an ranked evaluation seems to be more suitable.</S>",Hypothesis_Citation
19,W06-3114,W09-0402,0,"Koehn and Monz, 2006",0,"The correlations on the document level were computed on the English, French, Spanish and German texts generated by various translation systems in the framework of the first (Koehn and Monz, 2006), second (Callison-Burch et al, 2007) and third shared translation task (Callison-Burchet al, 2008)","The correlations on the document level were computed on the English, French, Spanish and German texts generated by various translation systems in the framework of the first (Koehn and Monz, 2006), second (Callison-Burch et al, 2007) and third shared translation task (Callison-Burchet al, 2008)","'0','6','18','95'","<S sid=""0"">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid=""6"" ssid=""4"">English was again paired with German, French, and Spanish.</S><S sid=""18"" ssid=""11"">In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.</S><S sid=""95"" ssid=""11"">The judgement of 4 in the first case will go to a vastly better system output than in the second case.</S>","Aim_Citation,Hypothesis_Citation,Method_Citation"
