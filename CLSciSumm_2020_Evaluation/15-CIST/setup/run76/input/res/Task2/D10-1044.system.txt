Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translatio. This is a standard adaptation problem for SMT. There is a fairly large body of work on SMT adaptation. For comparison to information-retrieval inspired baselines, eg (L&#168;u et al., 2007), we select sentences from OUT using language model perplexities from IN. Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities. Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models), and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples, with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set. We have already mentioned the closely related work by Matsoukas et al (2009) on discriminative corpus weighting, and Jiang and Zhai (2007) on (nondiscriminative) instance weighting. We will also directly compare with a baseline similar to the Matsoukas et al approach in order to measure the benefit from weighting phrase pairs (or ngrams) rather than full sentences. Domain adaptation is a common concern when optimizing empirical NLP applications. Moving beyond directly related work, major themes in SMT adaptation include the IR (Hildebrand et al., 2005; L&#168;u et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L&#168;u et al., 2007) approaches for LMs and TMs described above, as