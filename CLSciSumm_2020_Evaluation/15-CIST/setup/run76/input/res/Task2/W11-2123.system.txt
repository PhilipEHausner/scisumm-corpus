Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders. Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated. It is generally considered to be fast (Pauls 29 &#8722; 1 probabilities and 2\' &#8722; 2 non-zero backoffs. and Klein, 2011), with a default implementation based on hash tables within each trie node. For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed. KenLM: Faster and Smaller Language Model Querie. The PROBING model was designed to improve upon SRILM by using linear probing hash tables (though not arranged in a trie), allocating memory all at once (eliminating the need for full pointers), and being easy to compile. The binary language model from Section 5.2 and text phrase table were forced into disk cache before each run. Time for Moses itself to load, including loading the language model and phrase table, is included. Therefore, we want state to encode the minimum amount of information necessary to properly compute language model scores, so that the decoder will be faster and make fewer search errors. We offer a state function s(wn1) = wn&#65533; where substring wn&#65533; is guaranteed to extend (to the