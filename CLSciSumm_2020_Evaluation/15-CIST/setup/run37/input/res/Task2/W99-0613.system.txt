This paper discusses the use of unlabeled examples for the problem of named entity classification. The first method uses a similar algorithm to that of (Yarowsky 95), with modifications motivated by (Blum and Mitchell 98). This paper discusses the use of unlabeled examples for the problem of named entity classification. (Blum and Mitchell 98) offer a promising formulation of redundancy, also prove some results about how the use of unlabeled examples can help classification, and suggest an objective function when training with unlabeled examples. The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98). The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel. AdaBoost was first introduced in (Freund and Schapire 97); (Schapire and Singer 98) gave a generalization of AdaBoost which we will use in this paper. The two new terms force the two classifiers to agree, as much as possible, on the unlabeled examples. There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features. The DL-CoTrain algorithm can be motivated as being a greedy method of satisfying the above 2 constraints. (We would like to note though that unlike previous boosting algorithms, the CoBoost algorithm presented here is not a boosting algorithm under Valiant\'s (Valiant 84) Probably Approximately Correct (PAC) model.. This section describes AdaBoost, which is the basis for the CoBoost algorithm. Unsupervised Models