1512 ; & Although the topics in figure 2 seem highly focused , it is interesting to ask whether the model is genuinely learning mixtures of topics or simply assigning entire document tuples to single topics . 1491 ; & 8212 ; documents that are topically similar but are not direct translations of one another & tokens in each of the languages . 8722 ; language JS divergence A topic model specifies a probability distribution over documents , or in the case of PLTM , document tuples . 1495 ; & 943 ; & 960 ; & 1077 ; & 947 ; & 943 ; & 1500 ; & 1080 ; & 1080 ; & 8220 ; topic & 1089 ; & 1089 ; & 1084 ; & 964 ; & 934 ; 1 , ... 961 ; & 957 ; & 945 ; and base measure m : Then , for each language l , a latent topic assignment is drawn for each token in that language : Finally , the observed tokens are themselves drawn using the language-specific topic parameters : wl & 8220 ; translation & 1509 ; & 942 ; & 945 ; & 1087 ; & 959 ; & 1504 ; & 1086 ; & 1082 ; & 1079 ; & 945 ; & 945 ; & 963 ; & We explore the model & 1094 ; & 963 ; & 1081 ; & 953 ; & 1086 ; & 1086 ; & 1089 ;