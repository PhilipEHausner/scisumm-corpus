The top-down guidance that is provided makes this approach quite efficient in practice . A parser that is not left to right , but which has rooted derivations , e.g. 8212 ; e.g. Two features of our top-down parsing approach will emerge as key to its success . A new language model , based on probabilistic top-down parsing , will be outlined and compared with the previous literature , and extensive empirical results will be presented which demonstrate its utility . The differences between a k-best and a beam-search parser ( not to mention the use of dynamic programming ) make a running time difference unsurprising . 8212 ; where the lexical heads of each constituent are annotated on both the right- and left-hand sides of the context-free rules , under the constraint that every constituent inherits the lexical head from exactly one of its children , and the lexical head of a POS is its terminal item . We will then present empirical results in two domains : one to compare with previous work in the parsing literature , and the other to compare with previous work using parsing for language modeling for speech recognition , in particular with the Chelba and Jelinek results mentioned above . Table 5 reports the word and sentence error rates for five different models : ( i ) the trigram model that comes with the lattices , trained on approximately 40M words , with a vocabulary of 20,000 ; ( ii ) the