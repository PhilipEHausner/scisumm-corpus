8212 ; just change the set of features used . First , as already implicit in our discussion , factoring the probability computation into a sequence of values corresponding to various & amp ; quot ; features & amp ; quot ; suggests that the probability model should be easily changeable & We present a new parser for parsing down to Penn tree-bank style parse trees [ 16 ] that achieves 90.1 % average precision/recall for sentences of length & lt ; 40 , and 89.5 % for sentences of length & lt ; 100 , when trained and tested on the previously established [ 5,9,10,15,17 ] & amp ; quot ; standard & amp ; quot ; sections of the Wall Street Journal tree-bank . ( For example , part-ofspeech tagging using the most probable preterminal for each word is 90 % accurate [ 8 ] .. For example , in the Penn Treebank a vp with both main and auxiliary verbs has the structure shown in Figure 3 . The major problem confronting the author of a generative parser is what information to use to condition the probabilities required in the model , and how to smooth the empirically obtained probabilities to take the sting out of the sparse data problems that are inevitable with even the most modest conditioning . L and R are conditioned on three previous labels so we are using a third-order Markov grammar . Thus the probability of a parse is given by