Phrase-level granularity distinguishes our work from previous work by Matsoukas et al ( 2009 ) , who weight sentences according to sub-corpus and genre membership . Realizing gains in practice can be challenging , however , particularly when the target domain is distant from the background data . Realizing gains in practice can be challenging , however , particularly when the target domain is distant from the background data . Realizing gains in practice can be challenging , however , particularly when the target domain is distant from the background data . In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance . Linear weights are difficult to incorporate into the standard MERT procedure because they are & Section 2 describes our baseline techniques for SMT adaptation , and section 3 describes the instance-weighting approach . Finally , we incorporate the instance-weighting model into a general linear combination , and learn weights and mixing parameters simultaneously . 8212 ; though adequate for reasonable performance & For comparison to information-retrieval inspired baselines , eg ( L & # 168 ; u et al. , 2007 ) , we select sentences from OUT using language model perplexities from IN . A similar maximumlikelihood approach was used by Foster and Kuhn ( 2007 ) , but for language models only . Domain adaptation is a common concern when optimizing empirical NLP applications . We focus here instead on adapting the two