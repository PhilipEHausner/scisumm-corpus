Like the nonlexicalized parser in Roark and Johnson ( 1999 ) , we found that the search efficiency , in terms of number of rule expansions considered or number of analyses advanced , also improved as we increased the amount of conditioning . The paper first introduces key notions in language modeling and probabilistic parsing , and briefly reviews some previous approaches to using syntactic structure for language modeling . A new language model that utilizes probabilistic top-down parsing is then outlined , and empirical results show that it improves upon previous work in test corpus perplexity . The small size of our training data , as well as the fact that we are rescoring n-best lists , rather than working directly on lattices , makes comparison with the other models not particularly informative . 8212 ; that conditioning the probabilities of structures on the context within which they appear , for example on the lexical head of a constituent ( Charniak 1997 ; Collins 1997 ) , on the label of its parent nonterminal ( Johnson 1998 ) , or , ideally , on both and many other things besides , leads to a much better parsing model and results in higher parsing accuracies . In addition , we show the average number of rule expansions considered per word , that is , the number of rule expansions for which a probability was calculated ( see Roark and Charniak [ 2000 ] ) , and the average number of