For the automatic evaluation, we used BLEU, since it is the most established metric in the field. We asked participants to each judge 200&#8211;300 sentences in terms of fluency and adequacy, the most commonly used manual evaluation metrics. See Figure 3 for a screenshot of the evaluation tool. While we used the standard metrics of the community, the we way presented translations and prompted for assessment differed from other evaluation campaigns. English was again paired with German, French, and Spanish. We dropped, however, one of the languages, Finnish, partly to keep the number of tracks manageable, partly because we assumed that it would be hard to find enough Finnish speakers for the manual evaluation. About half of the participants of last year&#8217;s shared task participated again. Our initial experimentation with the evaluation tool showed that this is often too overwhelming. Training and testing is based on the Europarl corpus. Figure 1 provides some statistics about this corpus. The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000, which is excluded from the training data. Because of this, we retokenized and lowercased submitted output with our own tokenizer, which was also used to prepare the training and test data. We are currently working on a complete open source implementation of a training and decoding system, which should become available over the summer. pus, from which also the in-domain test set is taken. Microsoft&#8217;s approach uses dependency trees, others use hierarchical phrase models.