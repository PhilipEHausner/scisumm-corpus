On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives. Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs. We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics. The logical forms in DCS are called DCS trees, where nodes are labeled with predicates, and edges are labeled with relations. It is impossible to represent the semantics of this phrase with just a CSP, so we introduce a new aggregate relation, notated E. Consider a tree hE:ci, whose root is connected to a child c via E. If the denotation of c is a set of values s, the parent&#8217;s denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example. Our employed (Zettlemoyer and Collins, 2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language&#8212;think grounded al., 2010). compositional semantics. On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives. Figure 1 shows our probabilistic model: with respect to a world w (database of facts), producing an answer y. After training, given a new utterance x, our system outputs the most likely y, summing out the latent logical form z: argmaxy p&#952;(T)(y  |x, z &#8712; &#732;ZL,&#952;(T)). We now turn to the task of mapping natural language For the example in Figure 4(b), the de- utterances to DCS trees. Eisenciations due to data sparsity, and having an insuffi- stein et al. (2009) induces conjunctive formulae and ciently large K. uses them as features in another learning problem. Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(&#952;) = E(x,y)ED log p&#952;(JzKw = y  |x, z &#8712; ZL(x)) &#8722; &#955;k&#952;k22, which sums over all DCS trees z that evaluate to the target answer y. In DCS, we start with lexical triggers, which are 6 Conclusion more basic than CCG lexical entries. Next, we compared our systems (DCS and DCS+) with the state-of-the-art semantic parsers on the full dataset for both GEO and JOBS (see Table 3). The CSP has two types of constraints: (i) x &#8712; w(p) for each node x labeled with predicate p &#8712; P; and (ii) xj = yj0 (the j-th component of x must equal the j'-th component of y) for each edge (x, y) labeled with j0j &#8712; R. A solution to the CSP is an assignment of nodes to values that satisfies all the constraints. This yields a more system is based on a new semantic representation, factorized and flexible representation that is easier DCS, which offers a simple and expressive alterto search through and parametrize using features. native to lambda calculus.
