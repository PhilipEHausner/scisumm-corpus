As predications generated from such templates depend on the current character , we name these templates lexical-target . However , as such features are generated dynamically during the decoding procedure , two limitation arise : on the one hand , the amount of parameters increases rapidly , which is apt to overfit on training corpus ; on the other hand , exact inference by dynamic programming is intractable because the current predication relies on the results of prior predications . The first was conducted to test the performance of the perceptron on segmentation on the corpus from SIGHAN Bakeoff 2 , including the Academia Sinica Corpus ( AS ) , the Hong Kong City University Corpus ( CityU ) , the Peking University Corpus ( PKU ) and the Microsoft Research Corpus ( MSR ) . We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging . Besides this perceptron , other sub-models are trained and used as additional features of the outside-layer linear model . As predications generated from such templates depend on the current character , we name these templates lexical-target . On the Penn Chinese Treebank 5.0 , we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline . The perceptron has been used in many NLP tasks , such as POS tagging ( Collins , 2002 ) , Chinese word segmentation ( Ng and Low , 2004 ; Zhang and Clark , 2007 ) and