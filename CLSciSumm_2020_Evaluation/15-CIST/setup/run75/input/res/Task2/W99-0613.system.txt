( If fewer than n rules have Precision greater than pin , we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events , hence we do not use smoothing , allowing low-count high-precision features to be chosen on later iterations . keep only those rules which exceed the precision threshold . ) pm , n was fixed at 0.95 in all experiments in this paper . ( Yarowsky 95 ) describes an algorithm for word-sense disambiguation that exploits redundancy in contextual features , and gives impressive performance . contains ( York ) and is never seen with a feature such as contains ( Group ) ) . A large number of rules is needed for coverage of the domain , suggesting that a fairly large number of labeled examples should be required to train a classi- However , we show that the use of data can reduce the requirements for supervision to just 7 simple & amp ; quot ; seed & amp ; quot ; rules . ( If fewer than n rules have Precision greater than pin , we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events , hence we do not use smoothing , allowing low-count high-precision features to be chosen on later iterations . keep only those rules which exceed the precision threshold . ) pm , n was fixed at 0.95 in all experiments in this