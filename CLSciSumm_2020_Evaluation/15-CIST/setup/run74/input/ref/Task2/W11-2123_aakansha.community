This paper presents methods to query N-gram language models, minimizing time and space costs. The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models. We offer a state function s(wn1) = wn&#65533; where substring wn&#65533; is guaranteed to extend (to the right) in the same way that wn1 does for purposes of language modeling. We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs. Queries take the form p(wn|wn&#8722;1 1 ) where wn1 is an n-gram. S sid="205" ssid="24">We evaluate the time and memory consumption of each data structure by computing perplexity on 4 billion tokens from the English Gigaword corpus (Parker et al., 2009).</S> We have described two data structures for language modeling that achieve substantial reductions in time and memory cost. For RandLM, we used the settings in the documentation: 8 bits per value and false positive probability 1 256. Then we ran binary search to determine the least amount of memory with which it would run. The cost of storing these averages, in bits, is Because there are comparatively few unigrams, we elected to store them byte-aligned and unquantized, making every query faster. S sid="199" ssid="18">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>
