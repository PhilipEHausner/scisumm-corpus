This paper discusses the use of unlabeled examples for the problem of named entity classification . This paper discusses the use of unlabeled examples for the problem of named entity classification . There are two differences between this method and the DL-CoTrain algorithm : spelling and contextual features , alternating between labeling and learning with the two types of features . The task is to learn a function from an input string ( proper name ) to its type , which we will assume to be one of the categories Person , Organization , or Location . Our derivation is slightly different from the one presented in ( Schapire and Singer 98 ) as we restrict at to be positive . More recently , ( Riloff and Jones 99 ) describe a method they term & amp ; quot ; mutual bootstrapping & amp ; quot ; for simultaneously constructing a lexicon and contextual extraction patterns . In this paper k = 3 ( the three labels are person , organization , location ) , and we set a = 0.1 . Supervised methods have been applied quite successfully to the full MUC named-entity task ( Bikel et al . 97 ) . A large number of rules is needed for coverage of the domain , suggesting that a fairly large number of labeled examples should be required to train a classi- However , we show that the use of data can reduce the requirements for supervision to just 7