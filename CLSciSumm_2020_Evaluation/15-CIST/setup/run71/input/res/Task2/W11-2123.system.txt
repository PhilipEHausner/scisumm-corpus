Language models are widely applied in natural language processing , and applications such as machine translation make very frequent queries . MITLM 0.4 ( Hsu and Glass , 2008 ) is mostly designed for accurate model estimation , but can also compute perplexity . Performance improvements transfer to the Moses ( Koehn et al. , 2007 ) , cdec ( Dyer et al. , 2010 ) , and Joshua ( Li et al. , 2009 ) translation systems where our code has been integrated . Dynamic programming efficiently scores many hypotheses by exploiting the fact that an N-gram language model conditions on at most N & Applications such as machine translation use language model probability as a feature to assist in choosing between hypotheses . Language models are widely applied in natural language processing , and applications such as machine translation make very frequent queries . MITLM 0.4 ( Hsu and Glass , 2008 ) is mostly designed for accurate model estimation , but can also compute perplexity . The binary language model from Section 5.2 and text phrase table were forced into disk cache before each run . Non-empty buckets contain an entry belonging to them or to a preceding bucket where a conflict occurred . Language models are widely applied in natural language processing , and applications such as machine translation make very frequent queries . This paper describes the several performance techniques used and presents benchmarks against alternative implementations . Dynamic programming efficiently scores many hypotheses by