945 ; i for each domain ( just IN and OUT in our case ) , pi are the corresponding domain-specific models , and & A similar maximumlikelihood approach was used by Foster and Kuhn ( 2007 ) , but for language models only . This leads to a linear combination of domain-specific probabilities , with weights in [ 0 , 1 ] , normalized to sum to 1 . Other work includes transferring latent topic distributions from source to target language for LM adaptation , ( Tam et al. , 2007 ) and adapting features at the sentence level to different categories of sentence ( Finch and Sumita , 2008 ) . This leads to a linear combination of domain-specific probabilities , with weights in [ 0 , 1 ] , normalized to sum to 1 . We extend the Matsoukas et al approach in several ways . Within this framework , we use features intended to capture degree of generality , including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples . There is a fairly large body of work on SMT adaptation . When OUT is large and distinct , its contribution can be controlled by training separate IN and OUT models , and weighting their combination . Other work includes transferring latent topic distributions from source to target language for LM adaptation , ( Tam et al. , 2007 ) and adapting features at the sentence level to