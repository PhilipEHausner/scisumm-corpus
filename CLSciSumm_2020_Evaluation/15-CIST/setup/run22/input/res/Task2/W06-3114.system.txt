Hence, we use the bootstrap resampling method described by Koehn (2004). We are therefore applying a different method, which has been used at the 2005 DARPA/NIST evaluation. We asked participants to each judge 200&#8211;300 sentences in terms of fluency and adequacy, the most commonly used manual evaluation metrics. While we used the standard metrics of the community, the we way presented translations and prompted for assessment differed from other evaluation campaigns. The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task. Most of these groups follow a phrase-based statistical approach to machine translation. About half of the participants of last year&#8217;s shared task participated again. Compared to last year&#8217;s shared task, the participants represent more long-term research efforts. Training and testing is based on the Europarl corpus. The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000, which is excluded from the training data. Because of this, we retokenized and lowercased submitted output with our own tokenizer, which was also used to prepare the training and test data. The test set included 2000 sentences from the Europarl corpus, but also 1064 sentences out-ofdomain test data. The BLEU score has been shown to correlate well with human judgement, when statistical machine translation systems are compared (Doddington, 2002; Przybocki, 2004; Li, 2005). While automatic measures are an invaluable tool for the day-to-day development of machine translation systems, they are only a imperfect substitute for human