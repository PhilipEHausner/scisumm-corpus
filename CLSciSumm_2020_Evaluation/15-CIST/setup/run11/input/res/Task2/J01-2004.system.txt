We will focus our very brief review , however , on those that use grammars or parsing for their language models . And so on until the end of the string . The small size of our training data , as well as the fact that we are rescoring n-best lists , rather than working directly on lattices , makes comparison with the other models not particularly informative . We will focus our very brief review , however , on those that use grammars or parsing for their language models . We will focus our very brief review , however , on those that use grammars or parsing for their language models . While language models built around shallow local dependencies are still the standard in state-of-the-art speech recognition systems , there is reason to hope that better language models can and will be developed by computational linguists for this task . This is a subset of the possible leftmost partial derivations with respect to the prefix string W. Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models , demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model . Thus the rule S NP VP becomes , for instance , S [ barks ] NP [ dog ] VP [ barks ] . The following section will provide some background in probabilistic context-free grammars and language modeling for speech recognition