A Maximum-Entropy-Inspired Parser . As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [ 2,7 ] , we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model . A Maximum-Entropy-Inspired Parser . ( For example , part-ofspeech tagging using the most probable preterminal for each word is 90 % accurate [ 8 ] .. For example , in the Penn Treebank a vp with both main and auxiliary verbs has the structure shown in Figure 3 . The major problem confronting the author of a generative parser is what information to use to condition the probabilities required in the model , and how to smooth the empirically obtained probabilities to take the sting out of the sparse data problems that are inevitable with even the most modest conditioning . L and R are conditioned on three previous labels so we are using a third-order Markov grammar . Thus the probability of a parse is given by the equation where 1 ( c ) is the label of c ( e.g. , whether it is a noun phrase ( np ) , verb-phrase , etc . 8212 ; information outside c that our probability model deems important in determining the probability in question . The author would like to thank Mark Johnson and all the rest of the Brown Laboratory for Linguistic Information Processing . A Maximum-Entropy-Inspired Parser .