This paper proposes a framework for representing the meaning of phrases and sentences in vector space. Vector addition is by far the most common method for representing the meaning of linguistic sequences. Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination). Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition. In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations. Combining the multiplicative model with an additive model, which does not suffer from this problem, could mitigate this problem: pi = &#945;ui +&#946;vi +&#947;uivi (11) where &#945;, &#946;, and &#947; are weighting constants. The resulting vector is sparser but expresses more succinctly the meaning of the predicate-argument structure, and thus allows semantic similarity to be modelled more accurately. Further research is needed to gain a deeper understanding of vector composition, both in terms of modeling a wider range of structures (e.g., adjectivenoun, noun-noun) and also in terms of exploring the space of models more fully. Central in these models is the notion of compositionality &#8212; the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them. For example, assuming