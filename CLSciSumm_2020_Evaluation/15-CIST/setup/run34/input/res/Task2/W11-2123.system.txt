Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders. Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated. Applications such as machine translation use language model probability as a feature to assist in choosing between hypotheses. For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed. KenLM: Faster and Smaller Language Model Querie. We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs. The binary language model from Section 5.2 and text phrase table were forced into disk cache before each run. Time for Moses itself to load, including loading the language model and phrase table, is included. Therefore, we want state to encode the minimum amount of information necessary to properly compute language model scores, so that the decoder will be faster and make fewer search errors. The returned state s(wn1) may then be used in a followon query p(wn+1js(wn1)) that extends the previous query by one word. All language model queries issued by machine translation decoders follow a left-to-right pattern, starting with either the begin of sentence token or null context for mid-sentence fragments. For