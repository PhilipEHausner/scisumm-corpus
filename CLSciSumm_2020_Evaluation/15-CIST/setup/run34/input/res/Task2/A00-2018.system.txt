A Maximum-Entropy-Inspired Parser . This is as opposed to the &amp;quot;Markovgrammar&amp;quot; approach used in the current parser. Also, the earlier parser uses two techniques not employed in the current parser. First, it uses a clustering scheme on words to give the system a &amp;quot;soft&amp;quot; clustering of heads and sub-heads. We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less, and for of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal treebank. We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal tree-bank. The results of [13] achieved by combining the aforementioned three-best parsers also suggest that the limit on tree-bank trained parsers is much higher than previously thought. The method that gives the best results, however, uses a Markov grammar &#8212; a method for assigning probabilities to any possible expansion using statistics gathered from the training corpus [6,10,15]. To compute a probability in a log-linear model one first defines a set of &amp;quot;features&amp;quot;, functions from the space of configurations over which one is trying to compute probabilities to integers that denote the number of times some pattern occurs in the input. In the