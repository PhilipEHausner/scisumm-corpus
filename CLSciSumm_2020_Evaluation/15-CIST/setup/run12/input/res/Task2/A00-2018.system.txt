As noted in [ 5 ] , that system is based upon a & amp ; quot ; tree-bank grammar & amp ; quot ; - a grammar read directly off the training corpus . Also , the earlier parser uses two techniques not employed in the current parser . As noted in [ 5 ] , that system is based upon a & amp ; quot ; tree-bank grammar & amp ; quot ; - a grammar read directly off the training corpus . This corresponds to an error reduction of 13 % over the best previously published single parser results on this test set , those of Collins [ 9 ] . This parser achieves an average precision/recall of 86.2 % . Also , the earlier parser uses two techniques not employed in the current parser . A Maximum-Entropy-Inspired Parser . Also , the earlier parser uses two techniques not employed in the current parser . For runs with the generative model based upon Markov grammar statistics , the first pass uses the same statistics , but conditioned only on standard PCFG information . ( For example , part-ofspeech tagging using the most probable preterminal for each word is 90 % accurate [ 8 ] .. As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [ 2,7 ] , we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by