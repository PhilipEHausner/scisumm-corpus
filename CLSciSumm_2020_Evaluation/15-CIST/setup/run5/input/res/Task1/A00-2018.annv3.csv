Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
2,A00-2018,N10-1002,0,"Charniak, 2000",0,"As a benchmark VPC extraction system, we use the Charniak parser (Charniak, 2000)","As a benchmark VPC extraction system, we use the Charniak parser (Charniak, 2000)","[88, 115, 116, 117]","['<S sid=""88"" ssid=""57"">While we could have smoothed in the same fashion, we choose instead to use standard deleted interpolation.</S>', '<S sid=""115"" ssid=""6"">As noted in [5], that system is based upon a &amp;quot;tree-bank grammar&amp;quot; - a grammar read directly off the training corpus.</S>', '<S sid=""116"" ssid=""7"">This is as opposed to the &amp;quot;Markovgrammar&amp;quot; approach used in the current parser.</S>', '<S sid=""117"" ssid=""8"">Also, the earlier parser uses two techniques not employed in the current parser.</S>']",Method_Citation
3,A00-2018,W11-0610,0,"Charniak, 2000",0,"Each of these scores can be calculated from a provided syntactic parse tree, and to generate these we made use of the Charniakparser (Charniak, 2000), also trained on the Switch board tree bank","Each of these scores can be calculated from a provided syntactic parse tree, and to generate these we made use of the Charniak parser (Charniak, 2000), also trained on the Switch board tree bank","[2, 28, 117, 178]","['<S sid=""2"" ssid=""2"">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S>', '<S sid=""28"" ssid=""17"">More generally, one can condition on the m previously generated labels, thereby obtaining an mth-order Markov grammar.</S>', '<S sid=""117"" ssid=""8"">Also, the earlier parser uses two techniques not employed in the current parser.</S>', '<S sid=""178"" ssid=""5"">The results of [13] achieved by combining the aforementioned three-best parsers also suggest that the limit on tree-bank trained parsers is much higher than previously thought.</S>']",Results_Citation
4,A00-2018,W06-3119,0,"Charniak, 2000",0,"We then use Charniak? s parser (Charniak, 2000) to generate the most likely parse tree for each English target sentence in the training corpus","We then use Charniak's parser (Charniak, 2000) to generate the most likely parse tree for each English target sentence in the training corpus","[18, 93, 115, 170]","['<S sid=""18"" ssid=""7"">The method that gives the best results, however, uses a Markov grammar &#8212; a method for assigning probabilities to any possible expansion using statistics gathered from the training corpus [6,10,15].</S>', '<S sid=""93"" ssid=""4"">This allows the second pass to see expansions not present in the training corpus.</S>', '<S sid=""115"" ssid=""6"">As noted in [5], that system is based upon a &amp;quot;tree-bank grammar&amp;quot; - a grammar read directly off the training corpus.</S>', '<S sid=""170"" ssid=""61"">That is, the PCFG grammar rules are read directly off the training corpus.</S>']",Method_Citation
5,A00-2018,N03-2024,0,"Charniak, 2000",0,"We were interested in the occurrence of features such as type and number of premodifiers, presence and type of post modifiers, and form of name reference for people. We constructed a large, automatically annotated corpus by merging the output of Charniak? s statistical parser (Charniak, 2000) with that of the IBM named entity recognition system Nominator (Wacholder et al,1997)","We were interested in the occurrence of features such as type and number of premodifiers, presence and type of post modifiers, and form of name reference for people. We constructed a large, automatically annotated corpus by merging the output of Charniak's statistical parser (Charniak, 2000) with that of the IBM named entity recognition system Nominator (Wacholder et al,1997)","[39, 40, 87, 116]","['<S sid=""39"" ssid=""8"">In our work we assume that any feature can occur at most once, so features are boolean-valued: 0 if the pattern does not occur, 1 if it does.</S>', '<S sid=""40"" ssid=""9"">In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.</S>', '<S sid=""87"" ssid=""56"">In a pure maximum-entropy model this is done by feature selection, as in Ratnaparkhi\'s maximum-entropy parser [17].</S>', '<S sid=""116"" ssid=""7"">This is as opposed to the &amp;quot;Markovgrammar&amp;quot; approach used in the current parser.</S>']",Method_Citation
6,A00-2018,N06-1039,0,"Charniak, 2000",0,"After getting a set of basic clusters, we pass them to an existing statistical parser (Charniak, 2000) and rule-based tree normalizer to obtain a GLARFstructure for each sentence in every article","After getting a set of basic clusters, we pass them to an existing statistical parser (Charniak, 2000) and rule-based tree normalizer to obtain a GLARF structure for each sentence in every article","[40, 92, 115, 175]","['<S sid=""40"" ssid=""9"">In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.</S>', '<S sid=""92"" ssid=""3"">For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information.</S>', '<S sid=""115"" ssid=""6"">As noted in [5], that system is based upon a &amp;quot;tree-bank grammar&amp;quot; - a grammar read directly off the training corpus.</S>', '<S sid=""175"" ssid=""2"">This corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of Collins [9].</S>']",Method_Citation
7,A00-2018,C04-1180,0,2000,0,"The levels of accuracy and robustness recently achieved by statistical parsers (e.g. Collins (1999), Charniak (2000)) have led to their use in a number of NLP applications, such as question-answering (Pasca and Harabagiu, 2001), machine translation (Charniak et al, 2003), sentence simplification (Carroll et al, 1999), and a linguist? s search engine (Resnik and Elkiss, 2003)","The levels of accuracy and robustness recently achieved by statistical parsers (e.g. Collins (1999), Charniak (2000)) have led to their use in a number of NLP applications, such as question-answering (Pasca and Harabagiu, 2001), machine translation (Charniak et al, 2003), sentence simplification (Carroll et al, 1999), and a linguist? s search engine (Resnik and Elkiss, 2003)","[104, 114, 130, 175]","['<S sid=""104"" ssid=""15"">Again as standard, we take separate measurements for all sentences of length &lt; 40 and all sentences of length &lt; 100.</S>', '<S sid=""114"" ssid=""5"">That parser, as stated in Figure 1, achieves an average precision/recall of 87.5.</S>', '<S sid=""130"" ssid=""21"">This parser achieves an average precision/recall of 86.2%.</S>', '<S sid=""175"" ssid=""2"">This corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of Collins [9].</S>']",Method_Citation
8,A00-2018,W05-0638,0,"Charniak, 2000",0,"In CoNLL-2005, full parsing trees are provided by two full parsers: the Collins parser (Collins, 1999) and the Charniak parser (Charniak, 2000)","In CoNLL-2005, full parsing trees are provided by two full parsers: the Collins parser (Collins, 1999) and the Charniak parser (Charniak, 2000)","[10, 48, 117, 136]","['<S sid=""10"" ssid=""6"">That is, the parser implements the function arg maxrp(7r s) = arg maxirp(7r, s) = arg maxrp(w).</S>', '<S sid=""48"" ssid=""17"">Maximum-entropy models have two benefits for a parser builder.</S>', '<S sid=""117"" ssid=""8"">Also, the earlier parser uses two techniques not employed in the current parser.</S>', '<S sid=""136"" ssid=""27"">It turns out that usefulness of this process had already been discovered by Collins [10], who in turn notes (personal communication) that it was previously used by Eisner [12].</S>']",Method_Citation
9,A00-2018,P05-1065,0,"Charniak, 2000",0,"We also use a standard statistical parser (Charniak, 2000) to provide syntactic analysis","We also use a standard statistical parser (Charniak, 2000) to provide syntactic analysis","[89, 90, 116, 117]","['<S sid=""89"" ssid=""58"">(Actually, we use a minor variant described in [4].)</S>', '<S sid=""90"" ssid=""1"">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>', '<S sid=""116"" ssid=""7"">This is as opposed to the &amp;quot;Markovgrammar&amp;quot; approach used in the current parser.</S>', '<S sid=""117"" ssid=""8"">Also, the earlier parser uses two techniques not employed in the current parser.</S>']",Method_Citation
10,A00-2018,P05-1065,0,"Charniak, 2000",0,"For each article, we calculated the per cent age of a) all word instances (tokens) and b) all unique words (types) not on these lists, resulting in three token OOV rate features and three type OOV rate features per article. The parse features are generated using the Charniak parser (Charniak, 2000) trained on the standard Wall Street Journal Treebank corpus","For each article, we calculated the percentage of a) all word instances (tokens) and b) all unique words (types) not on these lists, resulting in three token OOV rate features and three type OOV rate features per article. The parse features are generated using the Charniak parser (Charniak, 2000) trained on the standard Wall Street Journal Treebank corpus","[1, 5, 6, 101]","['<S sid=""1"" ssid=""1"">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less, and for of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal treebank.</S>', '<S sid=""5"" ssid=""1"">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal tree-bank.</S>', '<S sid=""6"" ssid=""2"">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S>', '<S sid=""101"" ssid=""12"">In keeping with the standard methodology [5, 9,10,15,17], we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training, section 23 for testing, and section 24 for development (debugging and tuning).</S>']",Method_Citation
11,A00-2018,P04-1040,0,2000,0,"The evaluation of the transformed output of the parsers of Charniak (2000) and Collins (1999) gives 90 %unlabelled and 84 %labelled accuracy with respect to dependencies, when measured against a dependency corpus derived from the Penn Treebank.The paper is organized as follows","The evaluation of the transformed output of the parsers of Charniak (2000) and Collins (1999) gives 90 % unlabelled and 84 % labelled accuracy with respect to dependencies, when measured against a dependency corpus derived from the Penn Treebank. The paper is organized as follows","[102, 108, 155, 174]","['<S sid=""102"" ssid=""13"">Performance on the test corpus is measured using the standard measures from [5,9,10,17].</S>', '<S sid=""108"" ssid=""19"">As is typical, all of the standard measures tell pretty much the same story, with the new parser outperforming the other three parsers.</S>', '<S sid=""155"" ssid=""46"">For example, in the Penn Treebank a vp with both main and auxiliary verbs has the structure shown in Figure 3.</S>', '<S sid=""174"" ssid=""1"">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100.</S>']",Method_Citation
12,A00-2018,P04-1040,0,2000,0,"Blaheta and Charniak (2000) presented the first method for assigning Pennfunctional tags to constituents identified by a parser. Pattern-matching approaches were used in (John son, 2002) and (Jijkoun, 2003) to recover non-local dependencies in phrase trees","Blaheta and Charniak (2000) presented the first method for assigning Penn functional tags to constituents identified by a parser. Pattern-matching approaches were used in (Johnson, 2002) and (Jijkoun, 2003) to recover non-local dependencies in phrase trees","[17, 18, 116, 134]","['<S sid=""17"" ssid=""6"">In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.</S>', '<S sid=""18"" ssid=""7"">The method that gives the best results, however, uses a Markov grammar &#8212; a method for assigning probabilities to any possible expansion using statistics gathered from the training corpus [6,10,15].</S>', '<S sid=""116"" ssid=""7"">This is as opposed to the &amp;quot;Markovgrammar&amp;quot; approach used in the current parser.</S>', '<S sid=""134"" ssid=""25"">As already noted, Char97 first guesses the lexical head of a constituent and then, given the head, guesses the PCFG rule used to expand the constituent in question.</S>']",Method_Citation
13,A00-2018,P04-1040,0,2000,0,"As an alternative to hard coded heuristics, Blaheta and Charniak (2000) proposed to recover the Penn functional tags automatically","As an alternative to hard coded heuristics, Blaheta and Charniak (2000) proposed to recover the Penn functional tags automatically","[0, 1, 141, 155]","['<S sid=""0"">A Maximum-Entropy-Inspired Parser *</S>', '<S sid=""1"" ssid=""1"">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less, and for of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal treebank.</S>', '<S sid=""141"" ssid=""32"">(For example, part-ofspeech tagging using the most probable preterminal for each word is 90% accurate [8].)</S>', '<S sid=""155"" ssid=""46"">For example, in the Penn Treebank a vp with both main and auxiliary verbs has the structure shown in Figure 3.</S>']",Method_Citation
17,A00-2018,N06-1022,0,2000,0,"The parser of Charniak (2000) is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents","The parser of Charniak (2000) is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents","[28, 91, 92, 98]","['<S sid=""28"" ssid=""17"">More generally, one can condition on the m previously generated labels, thereby obtaining an mth-order Markov grammar.</S>', '<S sid=""91"" ssid=""2"">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>', '<S sid=""92"" ssid=""3"">For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information.</S>', '<S sid=""98"" ssid=""9"">L and R are conditioned on three previous labels so we are using a third-order Markov grammar.</S>']",Method_Citation
18,A00-2018,N06-1022,0,2000,0,"Most recently, McDonald et al (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000))","Most recently, McDonald et al (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000))","[10, 136, 174, 175]","['<S sid=""10"" ssid=""6"">That is, the parser implements the function arg maxrp(7r s) = arg maxirp(7r, s) = arg maxrp(w).</S>', '<S sid=""136"" ssid=""27"">It turns out that usefulness of this process had already been discovered by Collins [10], who in turn notes (personal communication) that it was previously used by Eisner [12].</S>', '<S sid=""174"" ssid=""1"">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100.</S>', '<S sid=""175"" ssid=""2"">This corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of Collins [9].</S>']",Method_Citation
19,A00-2018,H05-1035,0,"Charniak, 2000",0,"The feature set contains complex information extracted automatically from candidate syntax trees generated by parsing (Charniak, 2000), trees that will be improved by more accurate PP-attachment decisions","The feature set contains complex information extracted automatically from candidate syntax trees generated by parsing (Charniak, 2000), trees that will be improved by more accurate PP-attachment decisions","[9, 27, 42, 91]","['<S sid=""9"" ssid=""5"">The author would like to thank Mark Johnson and all the rest of the Brown Laboratory for Linguistic Information Processing. is s. Then for any s the parser returns the parse ir that maximizes this probability.</S>', '<S sid=""27"" ssid=""16"">In the simplest of such models, a zeroorder Markov grammar, each label on the righthand side is generated conditioned only on / &#8212; that is, according to the distributions p(Li j1), p(M I 1), and p(Ri I 1).</S>', '<S sid=""42"" ssid=""11"">This feature is obviously composed of two sub-features, one recognizing t, the other 1.</S>', '<S sid=""91"" ssid=""2"">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>']",Method_Citation
20,A00-2018,P04-1042,0,2000,0,"Note that the dependency figures of Dienes lag behind even the parsed results for Johnson? s model; this may well be due to the fact that Dienes built his model as an extension of Collins (1999), which lags behind Charniak (2000) by about 1.3-1.5% .Manual investigation of errors on English gold standard data revealed two major issues that suggest further potential for improvement in performance without further increase in algorithmic complexity or training set size","Note that the dependency figures of Dienes lag behind even the parsed results for Johnson's model; this may well be due to the fact that Dienes built his model as an extension of Collins (1999), which lags behind Charniak (2000) by about 1.3-1.5%. Manual investigation of errors on English gold standard data revealed two major issues that suggest further potential for improvement in performance without further increase in algorithmic complexity or training set size","[0, 175, 180, 184]","['<S sid=""0"">A Maximum-Entropy-Inspired Parser *</S>', '<S sid=""175"" ssid=""2"">This corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of Collins [9].</S>', '<S sid=""180"" ssid=""7"">From our perspective, perhaps the two most important numbers to come out of this research are the overall error reduction of 13% over the results in [9] and the intermediateresult improvement of nearly 2% on labeled precision/recall due to the simple idea of guessing the head\'s pre-terminal before guessing the head.</S>', '<S sid=""184"" ssid=""11"">The first is the slight, but important, improvement achieved by using this model over conventional deleted interpolation, as indicated in Figure 2.</S>']",Method_Citation
