While we could have smoothed in the same fashion, we choose instead to use standard deleted interpolation. As noted in [5], that system is based upon a &amp;quot;tree-bank grammar&amp;quot; - a grammar read directly off the training corpus. This is as opposed to the &amp;quot;Markovgrammar&amp;quot; approach used in the current parser. Also, the earlier parser uses two techniques not employed in the current parser. This represents a 13% decrease in error rate over the best single-parser results on this corpus [9]. More generally, one can condition on the m previously generated labels, thereby obtaining an mth-order Markov grammar. The results of [13] achieved by combining the aforementioned three-best parsers also suggest that the limit on tree-bank trained parsers is much higher than previously thought. The method that gives the best results, however, uses a Markov grammar &#8212; a method for assigning probabilities to any possible expansion using statistics gathered from the training corpus [6,10,15]. This allows the second pass to see expansions not present in the training corpus. That is, the PCFG grammar rules are read directly off the training corpus. In our work we assume that any feature can occur at most once, so features are boolean-valued: 0 if the pattern does not occur, 1 if it does. In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features. In a pure maximum-entropy model this is done by feature selection, as in Ratnaparkhi\'s maximum-entropy parser [17].