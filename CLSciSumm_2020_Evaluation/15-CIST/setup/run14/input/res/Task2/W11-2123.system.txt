The binary language model from Section 5.2 and text phrase table were forced into disk cache before each run . Nicola Bertoldi and Marcello Federico assisted with IRSTLM . Unigram lookup is dense so we use an array of probability and backoff values . Nicola Bertoldi and Marcello Federico assisted with IRSTLM . We have described two data structures for language modeling that achieve substantial reductions in time and memory cost . We have described two data structures for language modeling that achieve substantial reductions in time and memory cost . Unigram lookup is dense so we use an array of probability and backoff values . These performance gains transfer to improved system runtime performance ; though we focused on Moses , our code is the best lossless option with cdec and Joshua . The same numbers were used for each data structure . In addition to the optimizations specific to each datastructure described in Section 2 , we implement several general optimizations for language modeling . Unigram lookup is dense so we use an array of probability and backoff values . The returned state s ( wn1 ) may then be used in a followon query p ( wn+1js ( wn1 ) ) that extends the previous query by one word . Throughout this paper we compare with several packages : SRILM 1.5.12 ( Stolcke , 2002 ) is a popular toolkit based on tries used in several decoders . KenLM : Faster and Smaller Language Model Querie .