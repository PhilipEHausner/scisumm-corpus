About half of the participants of last year & Most of these groups follow a phrase-based statistical approach to machine translation . Most of these groups follow a phrase-based statistical approach to machine translation . BLEU is sensitive to tokenization . Most of these groups follow a phrase-based statistical approach to machine translation . Manual and Automatic Evaluation of Machine Translation between European Language . For the automatic evaluation , we used BLEU , since it is the most established metric in the field . For the automatic evaluation , we used BLEU , since it is the most established metric in the field . Figure 1 provides some statistics about this corpus . Also , the argument has been made that machine translation performance should be evaluated via task-based evaluation metrics , i.e . Most of these groups follow a phrase-based statistical approach to machine translation . 8211 ; 300 sentences in terms of fluency and adequacy , the most commonly used manual evaluation metrics . 8217 ; s shared task participated again . We are therefore applying a different method , which has been used at the 2005 DARPA/NIST evaluation . Our initial experimentation with the evaluation tool showed that this is often too overwhelming . ( 2006 ) that the rule-based system of Systran is not adequately appreciated by BLEU . The out-of-domain test set differs from the Europarl data in various ways . However , a recent study ( Callison-Burch et al. , 2006 ) ,