As noted in [ 5 ] , that system is based upon a & amp ; quot ; tree-bank grammar & amp ; quot ; - a grammar read directly off the training corpus . This allows the second pass to see expansions not present in the training corpus . This allows the second pass to see expansions not present in the training corpus . This allows the second pass to see expansions not present in the training corpus . This allows the second pass to see expansions not present in the training corpus . A Maximum-Entropy-Inspired Parser . Again as standard , we take separate measurements for all sentences of length & lt ; 40 and all sentences of length & lt ; 100 . This is as opposed to the & amp ; quot ; Markovgrammar & amp ; quot ; approach used in the current parser . Also , the earlier parser uses two techniques not employed in the current parser . For runs with the generative model based upon Markov grammar statistics , the first pass uses the same statistics , but conditioned only on standard PCFG information . It turns out that usefulness of this process had already been discovered by Collins [ 10 ] , who in turn notes ( personal communication ) that it was previously used by Eisner [ 12 ] . While we could have smoothed in the same fashion , we choose instead to use standard deleted interpolation . (