Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,P04-1036,W04-0837,0,"McCarthy et al, 2004",0,"The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding con text into account (McCarthy et al, 2004)","The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account (McCarthy et al, 2004)","'8','14','21','107'","<S sid=""8"" ssid=""1"">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S><S sid=""14"" ssid=""7"">Even systems which show superior performance to this heuristic often make use of the heuristic where evidence from the context is not sufficient (Hoste et al., 2001).</S><S sid=""21"" ssid=""14"">We believe that an automatic means of finding a predominant sense would be useful for systems that use it as a means of backing-off (Wilks and Stevenson, 1998; Hoste et al., 2001) and for systems that use it in lexical acquisition (McCarthy, 1997; Merlo and Leybold, 2001; Korhonen, 2002) because of the limited size of hand-tagged resources.</S><S sid=""107"" ssid=""5"">To disambiguate senses a system should take context into account.</S>","Aim_Citation,Hypothesis_Citation,Implication_Citation"
2,P04-1036,W04-0837,0,"McCarthy et al, 2004",0,"Association for Computational Linguistics for the Semantic Analysis of Text, Barcelona, Spain, July 2004 SENSEVAL-3: Third International Workshop on the Evaluation of Systems PoS precision recall baseline Noun 95 73 45 Verb 79 43 22 Adjective 88 59 44 Adverb 91 72 59 All PoS 90 63 41Table 2: The SENSEVAL-2 first sense on the SEN SEVAL-2 English all-words data system can be tuned to a given genre or domain (McCarthy et al, 2004) and also because there will be words that occur with insufficient frequency inthe hand-tagged resources available","Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to a given genre or domain (McCarthy et al., 2004) and also because there will be words that occur with insufficient frequency in the hand-tagged resources available","'15','21','22','175'","<S sid=""15"" ssid=""8"">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S><S sid=""21"" ssid=""14"">We believe that an automatic means of finding a predominant sense would be useful for systems that use it as a means of backing-off (Wilks and Stevenson, 1998; Hoste et al., 2001) and for systems that use it in lexical acquisition (McCarthy, 1997; Merlo and Leybold, 2001; Korhonen, 2002) because of the limited size of hand-tagged resources.</S><S sid=""22"" ssid=""15"">More importantly, when working within a specific domain one would wish to tune the first sense heuristic to the domain at hand.</S><S sid=""175"" ssid=""23"">We are currently investigating the performance of the first sense heuristic, and this method, for other PoS on SENSEVAL-3 data (McCarthy et al., 2004), although not yet with rankings from domain specific corpora.</S>","Aim_Citation,Hypothesis_Citation"
3,P04-1036,W04-0837,0,"McCarthy et al, 2004",0,"The method is described in (McCarthy et al, 2004), which we summarise here","The method is described in (McCarthy et al, 2004), which we summarise here","'29','30','41','175'","<S sid=""29"" ssid=""22"">We discuss our method in the following section.</S><S sid=""30"" ssid=""23"">Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively.</S><S sid=""41"" ssid=""34"">In this paper we describe and evaluate a method for ranking senses of nouns to obtain the predominant sense of a word using the neighbours from automatically acquired thesauruses.</S><S sid=""175"" ssid=""23"">We are currently investigating the performance of the first sense heuristic, and this method, for other PoS on SENSEVAL-3 data (McCarthy et al., 2004), although not yet with rankings from domain specific corpora.</S>","Aim_Citation,Hypothesis_Citation,Method_Citation"
5,P04-1036,I08-2105,0,"McCarthy et al, 2004",0,"McCarthy et al (2004) use a corpus and word similarities to induce a ranking of word senses from an untagged corpus to be used in WSD.We build upon this previous research, and pro pose an unsupervised WSD method in which senses for two grammatically related words in the sentence will be connected through directed edges",McCarthy et al (2004) use a corpus and word similarities to induce a ranking of word senses from an untagged corpus to be used in WSD,"'0','47','166','171'","<S sid=""0"">Finding Predominant Word Senses in Untagged Text</S><S sid=""47"" ssid=""3"">We then use the WordNet similarity package (Patwardhan and Pedersen, 2003) to give us a semantic similarity measure (hereafter referred to as the WordNet similarity measure) to weight the contribution that each neighbour makes to the various senses of the target word.</S><S sid=""166"" ssid=""14"">There has been some related work on using automatic thesauruses for discovering word senses from corpora Pantel and Lin (2002).</S><S sid=""171"" ssid=""19"">In contrast, we use the neighbours lists and WordNet similarity measures to impose a prevalence ranking on the WordNet senses.</S>",Aim_Citation
6,P04-1036,I08-2105,0,"McCarthy et al, 2004",0,"Previous re search in inducing sense rankings from an untagged corpus (McCarthy et al, 2004), and inducing selectional preferences at the word level (for other applications) (Erk, 2007) will provide the starting point for research in this direction","Previous research in inducing sense rankings from an untagged corpus (McCarthy et al, 2004), and inducing selectional preferences at the word level (for other applications) (Erk, 2007) will provide the starting point for research in this direction","'0','14','166','175'","<S sid=""0"">Finding Predominant Word Senses in Untagged Text</S><S sid=""14"" ssid=""7"">Even systems which show superior performance to this heuristic often make use of the heuristic where evidence from the context is not sufficient (Hoste et al., 2001).</S><S sid=""166"" ssid=""14"">There has been some related work on using automatic thesauruses for discovering word senses from corpora Pantel and Lin (2002).</S><S sid=""175"" ssid=""23"">We are currently investigating the performance of the first sense heuristic, and this method, for other PoS on SENSEVAL-3 data (McCarthy et al., 2004), although not yet with rankings from domain specific corpora.</S>",Results_Citation
7,P04-1036,I08-2105,0,2004,0,"McCarthy et al (2004) report a disambiguation precision of 53.0% and recall of 49.0% on the Senseval-2 test data, using an approach that derives sense ranking based on word similarity and distributional analysis in a corpus","McCarthy et al (2004) report a disambiguation precision of 53.0% and recall of 49.0% on the Senseval-2 test data, using an approach that derives sense ranking based on word similarity and distributional analysis in a corpus","'112','166','175','176'","<S sid=""112"" ssid=""10"">We compare results using the first sense listed in SemCor, and the first sense according to the SENSEVAL-2 English all-words test data itself.</S><S sid=""166"" ssid=""14"">There has been some related work on using automatic thesauruses for discovering word senses from corpora Pantel and Lin (2002).</S><S sid=""175"" ssid=""23"">We are currently investigating the performance of the first sense heuristic, and this method, for other PoS on SENSEVAL-3 data (McCarthy et al., 2004), although not yet with rankings from domain specific corpora.</S><S sid=""176"" ssid=""24"">The lesk measure can be used when ranking adjectives, and adverbs as well as nouns and verbs (which can also be ranked using jcn).</S>",Method_Citation
8,P04-1036,P06-1012,0,"McCarthy et al, 2004",0,"Research by (McCarthy et al, 2004) highlighted that the sense priors of a word in a corpus depend on the domain from which the corpus is drawn","Research by (McCarthy et al, 2004) highlighted that the sense priors of a word in a corpus depend on the domain from which the corpus is drawn","'147','151','166','175'","<S sid=""147"" ssid=""24"">The word share is among the words whose predominant sense remained the same for all three corpora.</S><S sid=""151"" ssid=""28"">This figure shows the domain labels assigned to the predominant senses for the set of 38 words after ranking the words using the SPORTS and the FINANCE corpora.</S><S sid=""166"" ssid=""14"">There has been some related work on using automatic thesauruses for discovering word senses from corpora Pantel and Lin (2002).</S><S sid=""175"" ssid=""23"">We are currently investigating the performance of the first sense heuristic, and this method, for other PoS on SENSEVAL-3 data (McCarthy et al., 2004), although not yet with rankings from domain specific corpora.</S>",Method_Citation
9,P04-1036,P06-1012,0,"McCarthy et al, 2004",0,"In addition, we implemented the unsupervised method of (McCarthy et al, 2004), which cal cu lates a prevalence score for each sense of a word to predict the predominant sense","In addition, we implemented the unsupervised method of (McCarthy et al, 2004), which calculates a prevalence score for each sense of a word to predict the predominant sense","'0','46','48','106'","<S sid=""0"">Finding Predominant Word Senses in Untagged Text</S><S sid=""46"" ssid=""2"">This provides the nearest neighbours to each target word, along with the distributional similarity score between the target word and its neighbour.</S><S sid=""48"" ssid=""4"">To find the first sense of a word ( ) we take each sense in turn and obtain a score reflecting the prevalence which is used for ranking.</S><S sid=""106"" ssid=""4"">We do not assume that the predominant sense is a method of WSD in itself.</S>",Method_Citation
11,P04-1036,P10-1155,0,2004,0,"McCarthy et al (2004) reported that the best results were obtained using k= 50 neighbors and the Wordnet Similarityjcn measure (Jiang and Conrath, 1997)","McCarthy et al (2004) reported that the best results were obtained using k= 50 neighbors and the Wordnet Similarity jcn measure (Jiang and Conrath, 1997)","'78','79','83','189'","<S sid=""78"" ssid=""7"">The variation in counts had negligible affect on the results.</S><S sid=""79"" ssid=""8"">3 The experimental results reported here are obtained using IC counts from the BNC corpus.</S><S sid=""83"" ssid=""12"">The results in table 1 show the accuracy of the ranking with respect to SemCor over the entire set of 2595 polysemous nouns in SemCor with the jcn and lesk WordNet similarity measures.</S><S sid=""189"" ssid=""12"">Additionally, we need to determine whether senses which do not occur in a wide variety of grammatical contexts fare badly using distributional measures of similarity, and what can be done to combat this problem using relation specific thesauruses.</S>",Method_Citation
12,P04-1036,W12-3401,0,2004,0,"In doing so, we provide first results on the application to French parsing of WordNet automatic sense ranking (ASR), using the method of McCarthy et al (2004)","In doing so, we provide first results on the application to French parsing of WordNet automatic sense ranking (ASR), using the method of McCarthy et al (2004)","'31','93','150','175'","<S sid=""31"" ssid=""24"">In section 5 we present results of the method on two domain specific sections of the Reuters corpus for a sample of words.</S><S sid=""93"" ssid=""22"">The automatic ranking from the BNC data lists the latter tube sense first.</S><S sid=""150"" ssid=""27"">Figure 2 displays the results of the second experiment with the domain specific corpora.</S><S sid=""175"" ssid=""23"">We are currently investigating the performance of the first sense heuristic, and this method, for other PoS on SENSEVAL-3 data (McCarthy et al., 2004), although not yet with rankings from domain specific corpora.</S>","Aim_Citation,Hypothesis_Citation,Implication_Citation"
13,P04-1036,W12-3401,0,2004,0,"To define an appropriate categorical distribution over synsets for each 2 lemma x in our source vocabulary, we first use the WordNet resource to identify the set Sx of different senses of x. We then use a distributional thesaurus to perform ASR, which determines the prevalence with respect to x of each sense s? Sx, following the approach of McCarthy et al (2004)","We then use a distributional thesaurus to perform ASR, which determines the prevalence with respect to x of each senses' Sx, following the approach of McCarthy et al (2004)","'47','48','171','189'","<S sid=""47"" ssid=""3"">We then use the WordNet similarity package (Patwardhan and Pedersen, 2003) to give us a semantic similarity measure (hereafter referred to as the WordNet similarity measure) to weight the contribution that each neighbour makes to the various senses of the target word.</S><S sid=""48"" ssid=""4"">To find the first sense of a word ( ) we take each sense in turn and obtain a score reflecting the prevalence which is used for ranking.</S><S sid=""171"" ssid=""19"">In contrast, we use the neighbours lists and WordNet similarity measures to impose a prevalence ranking on the WordNet senses.</S><S sid=""189"" ssid=""12"">Additionally, we need to determine whether senses which do not occur in a wide variety of grammatical contexts fare badly using distributional measures of similarity, and what can be done to combat this problem using relation specific thesauruses.</S>",Method_Citation
14,P04-1036,W12-3401,0,2004,0,"As explained in Section 2.2, ASR is performed using the method of McCarthy et al (2004)","As explained in Section 2.2, ASR is performed using the method of McCarthy et al (2004)","'29','30','31','168'","<S sid=""29"" ssid=""22"">We discuss our method in the following section.</S><S sid=""30"" ssid=""23"">Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively.</S><S sid=""31"" ssid=""24"">In section 5 we present results of the method on two domain specific sections of the Reuters corpus for a sample of words.</S><S sid=""168"" ssid=""16"">They evaluate using the lin measure described above in section 2.2 to determine the precision and recall of these discovered classes with respect to WordNet synsets.</S>",Method_Citation
16,P04-1036,S12-1097,0,"McCarthy et al, 2004",0,"This approach is commonly used as a baseline for word sense disambiguation (McCarthy et al, 2004)","This approach is commonly used as a baseline for word sense disambiguation (McCarthy et al, 2004)","'1','13','42','176'","<S sid=""1"" ssid=""1"">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S><S sid=""13"" ssid=""6"">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S><S sid=""42"" ssid=""35"">The neighbours for a word in a thesaurus are words themselves, rather than senses.</S><S sid=""176"" ssid=""24"">The lesk measure can be used when ranking adjectives, and adverbs as well as nouns and verbs (which can also be ranked using jcn).</S>","Hypothesis_Citation,Method_Citation"
17,P04-1036,W10-2803,0,"McCarthy et al, 2004",0,"More radical solutions than sense grouping that have been proposed are to restrict the task to determining predominant sense in a given domain (McCarthy et al, 2004), or to work directly with para phrases (McCarthy and Navigli, 2009)","More radical solutions than sense grouping that have been proposed are to restrict the task to determining predominant sense in a given domain (McCarthy et al, 2004), or to work directly with para phrases (McCarthy and Navigli, 2009)","'4','154','155','174'","<S sid=""4"" ssid=""4"">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S sid=""154"" ssid=""2"">In contrast, our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one, and because handtagged data is not always available.</S><S sid=""155"" ssid=""3"">A major benefit of our work, rather than reliance on hand-tagged training data such as SemCor, is that this method permits us to produce predominant senses for the domain and text type required.</S><S sid=""174"" ssid=""22"">We have restricted ourselves to nouns in this work, since this PoS is perhaps most affected by domain.</S>","Hypothesis_Citation,Method_Citation"
18,P04-1036,W08-2107,0,2004,0,"In addition, we can also infer a positive contribution of the frequency of a sense with the choice of the first synset returned by Word net resulting in a reasonable WSD heuristic (which is compatible with the results by McCarthy et al (2004))","In addition, we can also infer a positive contribution of the frequency of a sense with the choice of the first synset returned by Word net resulting in a reasonable WSD heuristic (which is compatible with the results by McCarthy et al (2004))","'1','13','81','82'","<S sid=""1"" ssid=""1"">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S><S sid=""13"" ssid=""6"">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S><S sid=""81"" ssid=""10"">4 We calculate the accuracy of finding the predominant sense, when there is indeed one sense with a higher frequency than the others for this word in SemCor ( ).</S><S sid=""82"" ssid=""11"">We also calculate the WSD accuracy that would be obtained on SemCor, when using our first sense in all contexts ( ).</S>","Method_Citation,Results_Citation"
19,P04-1036,D07-1026,0,"McCarthy et al, 2004",0,"It is worthwhile to remark here that, being the IBLE algorithm fully unsupervised, improving the most frequent baseline is an excellent result, rarely achieved in the literature on unsupervised methods for WSD (McCarthy et al, 2004)","It is worthwhile to remark here that, being the IBLE algorithm fully unsupervised, improving the most frequent baseline is an excellent result, rarely achieved in the literature on unsupervised methods for WSD (McCarthy et al, 2004)","'8','79','106','111'","<S sid=""8"" ssid=""1"">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S><S sid=""79"" ssid=""8"">3 The experimental results reported here are obtained using IC counts from the BNC corpus.</S><S sid=""106"" ssid=""4"">We do not assume that the predominant sense is a method of WSD in itself.</S><S sid=""111"" ssid=""9"">We give the results for this WSD task in table 2.</S>",Method_Citation
20,P04-1036,W12-2429,0,"McCarthy et al, 2004",0,"The first, most frequent sense (MFS) (McCarthy et al, 2004), is widely used baseline for supervised WSD systems","The first, most frequent sense (MFS) (McCarthy et al, 2004), is widely used baseline for supervised WSD systems","'8','13','14','23'","<S sid=""8"" ssid=""1"">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S><S sid=""13"" ssid=""6"">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S><S sid=""14"" ssid=""7"">Even systems which show superior performance to this heuristic often make use of the heuristic where evidence from the context is not sufficient (Hoste et al., 2001).</S><S sid=""23"" ssid=""16"">The first sense of star in SemCor is celestial body, however, if one were disambiguating popular news celebrity would be preferred.</S>","Aim_Citation,Hypothesis_Citation"
