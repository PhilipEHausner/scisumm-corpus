Outside of the field of topic modeling, Kawaba et al. (Kawaba et al., 2008) use a Wikipedia-based model to perform sentiment analysis of blog posts. Additionally, PLTM assumes that each &#8220;topic&#8221; consists of a set of discrete distributions over words&#8212;one for each language l = 1, ... , L. In other words, rather than using a single set of topics &#934; = {&#966;1, ... , &#966;T}, as in LDA, there are L sets of language-specific topics, &#934;1, ... , &#934;L, each of which is drawn from a language-specific symmetric Dirichlet with concentration parameter &#946;l. We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica. When applied to comparable document collections such as Wikipedia, PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language. In order to simulate this scenario we create a set of variations of the EuroParl corpus by treating some documents as if they have no parallel/comparable texts &#8211; i.e., we put each of these documents in a single-document tuple. Although the PLTM is clearly not a substitute for a machine translation system&#8212;it has no way to represent syntax or even multi-word phrases&#8212;it is clear from the examples in figure 2 that the sets of high probability words in different languages for a given topic are likely to include translations. Divergence drops significantly when the proportion of &#8220;glue&#8221; tuples increases from 0.01 to 0.25. The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples. Continuing with the example above, one might extract a set of connected Wikipedia articles related to the focus of the journal and then train PLTM on a joint corpus consisting of journal papers and Wikipedia articles. However, they evaluate their model on only two languages (English and Chinese), and do not use the model to detect differences between languages. In addition to enhancing lexicons by aligning topic-specific vocabulary, PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus. Maximum topic probability in document Although the posterior distribution over topics for each tuple is not concentrated on one topic, it is worth checking that this is not simply because the model is assigning a single topic to the 1We use the R density function. tokens in each of the languages.</ We use both Jensen-Shannon divergence and cosine distance. They also provide little analysis of the differences between polylingual and single-language topic models. We employ a set of direct translations, the EuroParl corpus, to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content. We evaluate sets of high-probability words in each topic and multilingual &#8220;synsets&#8221; by comparing them to entries in human-constructed bilingual dictionaries, as done by Haghighi et al. (2008). When applied to comparable document collections such as Wikipedia, PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language. We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages. Additionally, PLTM can support the creation of bilingual lexica for low resource language pairs, providing candidate translations for more computationally intense alignment processes without the sentence-aligned translations typically used in such tasks. Topic models have been used for analyzing topic trends in research literature (Mann et al., 2006; Hall et al., 2008), inferring captions for images (Blei and Jordan, 2003), social network analysis in email (McCallum et al., 2005), and expanding queries with topically related words in information retrieval (Wei and Croft, 2006).
