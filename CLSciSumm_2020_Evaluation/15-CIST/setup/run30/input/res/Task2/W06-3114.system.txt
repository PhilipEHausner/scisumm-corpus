Microsoft & Most of these groups follow a phrase-based statistical approach to machine translation . While automatic measures are an invaluable tool for the day-to-day development of machine translation systems , they are only a imperfect substitute for human assessment of translation quality , or as the acronym BLEU puts it , a bilingual evaluation understudy . Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus . To summarize , we provided : The performance of the baseline system is similar to the best submissions in last year & For instance , in the recent IWSLT evaluation , first fluency annotations were solicited ( while withholding the source sentence ) , and then adequacy annotations . Microsoft & Following this method , we repeatedly & We asked participants to each judge 200 & how much it assists performing a useful task , such as supporting human translators or aiding the analysis of texts . The BLEU score has been shown to correlate well with human judgement , when statistical machine translation systems are compared ( Doddington , 2002 ; Przybocki , 2004 ; Li , 2005 ) . We are therefore applying a different method , which has been used at the 2005 DARPA/NIST evaluation . We collected around 300 & Many human evaluation metrics have been proposed . ( 2006 ) that the rule-based system of Systran is not adequately appreciated by BLEU . Training and testing is based on the Europarl corpus .