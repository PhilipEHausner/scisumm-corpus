Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders. Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated. For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed. For example, syntactic decoders (Koehn et al., 2007; Dyer et al., 2010; Li et al., 2009) perform dynamic programming parametrized by both backward- and forward-looking state. The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models. The PROBING model was designed to improve upon SRILM by using linear probing hash tables (though not arranged in a trie), allocating memory all at once (eliminating the need for full pointers), and being easy to compile. The binary language model from Section 5.2 and text phrase table were forced into disk cache before each run. Time for Moses itself to load, including loading the language model and phrase table, is included. We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs. Therefore, we want state to encode the minimum amount of information necessary to properly compute language model scores, so that the decoder will