This paper proposes a framework for representing the meaning of phrases and sentences in vector space. Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination). Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition. Importantly, additive models capture composition by considering all vector components representing the meaning of the verb and its subject, whereas multiplicative models consider a subset, namely non-zero components. In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations. This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure. Vector addition is by far the most common method for representing the meaning of linguistic sequences. Combining the multiplicative model with an additive model, which does not suffer from this problem, could mitigate this problem: pi = &#945;ui +&#946;vi +&#947;uivi (11) where &#945;, &#946;, and &#947; are weighting constants. Central in these models is the notion of compositionality &#8212; the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them. For example, assuming that individual words are represented by vectors, we can compute the meaning of a sentence by taking their