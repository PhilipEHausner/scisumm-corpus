Compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms. In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs. As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers. Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs. Learning Dependency-Based Compositional Semantic. However, we still model the logical form (now as a latent variable) to capture the complexities of language. In tackling this challenging learning problem, we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms. In each dataset, each sentence x is annotated with a Prolog logical form, which we use only to evaluate and get an answer y. The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2). We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics. Define a special predicate &#248; with w(&#248;) = V. We represent functions by a set of inputoutput pairs, e.g., w(count) = {(S, n) : n = |S|}. Although a DCS tree is a logical