This is as opposed to the &amp;quot;Markovgrammar&amp;quot; approach used in the current parser. Also, the earlier parser uses two techniques not employed in the current parser. First, it uses a clustering scheme on words to give the system a &amp;quot;soft&amp;quot; clustering of heads and sub-heads. (It is &amp;quot;soft&amp;quot; clustering in that a word can belong to more than one cluster with different weights - the weights express the probability of producing the word given that one is going to produce a word from that cluster.. As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model. It makes no use of special maximum-entropyinspired features (though their presence made it much easier to perform these experiments), it does not guess the pre-terminal before guessing the lexical head, and it uses a tree-bank grammar rather than a Markov grammar. The results of [13] achieved by combining the aforementioned three-best parsers also suggest that the limit on tree-bank trained parsers is much higher than previously thought. We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less, and for of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal treebank. We present a new parser for parsing