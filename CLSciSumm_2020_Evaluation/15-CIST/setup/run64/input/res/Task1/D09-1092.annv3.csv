Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,D09-1092,P14-1004,0,"Mimno et al, 2009",0,"This configuration is similar to PolyLDA (Mimno et al, 2009) or LinkLDA (Yano et al, 2009), such that utterances from different parties are treated as different languages or blog-post and comments pairs","This configuration is similar to PolyLDA (Mimno et al, 2009) or LinkLDA (Yano et al, 2009), such that utterances from different parties are treated as different languages or blog-post and comments pairs","'21','142','185','186'","<S sid=""21"" ssid=""17"">The second corpus, Wikipedia articles in twelve languages, contains sets of documents that are not translations of one another, but are very likely to be about similar concepts.</S><S sid=""142"" ssid=""91"">EN &#8220;comitology&#8221; and IT lang Topics at P = 0.01 &#8220;comitalogia&#8221;) that simply were not in the lexica.</S><S sid=""185"" ssid=""19"">Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission &#948;&#953;&#945;&#963;&#964;&#951;&#956;&#953;&#954;&#972; sts nasa &#945;&#947;&#947;&#955; small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimm&#352;inen space lento spatiale mission orbite mars satellite spatial &#1514;&#1497;&#1504;&#1499;&#1493;&#1514; &#1488; &#1512;&#1493;&#1491;&#1499; &#1500;&#1500; &#1495; &#1509;&#1512; &#1488;&#1492; &#1500;&#1500;&#1495;&#1492; spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa &#1082;&#1086;&#1089;&#1084;&#1080;&#1095;&#1077;&#1089;&#1082;&#1080;&#1081; &#1089;&#1086;&#1102;&#1079; &#1082;&#1086;&#1089;&#1084;&#1080;&#1095;&#1077;&#1089;&#1082;&#1086;&#1075;&#1086; &#1089;&#1087;&#1091;&#1090;&#1085;&#1080;&#1082; &#1089;&#1090;&#1072;&#1085;&#1094;&#1080;&#1080; uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la &#953;&#963;&#960;&#945;&#957;&#943;&#945;&#962; &#953;&#963;&#960;&#945;&#957;&#943;&#945; de &#953;&#963;&#960;&#945;&#957;&#972;&#962; &#957;&#964;&#949; &#956;&#945;&#948;&#961;&#943;&#964;&#951; de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpa&#324;ski hiszpanii la juan y &#1076;&#1077; &#1084;&#1072;&#1076;&#1088;&#1080;&#1076; &#1080;&#1089;&#1087;&#1072;&#1085;&#1080;&#1080; &#1080;&#1089;&#1087;&#1072;&#1085;&#1080;&#1103; &#1080;&#1089;&#1087;&#1072;&#1085;&#1089;&#1082;&#1080;&#1081; de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk &#960;&#959;&#953;&#951;&#964;&#942;&#962; &#960;&#959;&#943;&#951;&#963;&#951; &#960;&#959;&#953;&#951;&#964;&#942; &#941;&#961;&#947;&#959; &#960;&#959;&#953;&#951;&#964;&#941;&#962; &#960;&#959;&#953;&#942;&#956;&#945;&#964;&#945; poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.</S><S sid=""186"" ssid=""20"">Although we find that if Wikipedia contains an article on a particular subject in some language, the article will tend to be topically similar to the articles about that subject in other languages, we also find that across the whole collection different languages emphasize topics to different extents.</S>",Method_Citation
2,D09-1092,P10-1044,0,"Mimno et al, 2009",0,"Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al, 2009), modeling topic-aligned articles in different languages (Mimno et al, 2009), and word sense induction (Brody and Lapata, 2009)","Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al, 2009), modeling topic-aligned articles in different languages (Mimno et al, 2009), and word sense induction (Brody and Lapata, 2009)","'78','172','177','186'","<S sid=""78"" ssid=""27"">Although the model does not distinguish between topic assignment variables within a given document tuple (so it is technically incorrect to speak of different posterior distributions over topics for different documents in a given tuple), we can nevertheless divide topic assignment variables between languages and use them to estimate a Dirichlet-multinomial posterior distribution for each language in each tuple.</S><S sid=""172"" ssid=""6"">Second, because comparable texts may not use exactly the same topics, it becomes crucially important to be able to characterize differences in topic prevalence at the document level (do different languages have different perspectives on the same article?) and at the language-wide level (which topics do particular languages focus on?).</S><S sid=""177"" ssid=""11"">In the English version of Wikipedia we dropped all articles that were not linked to by any other language in our set.</S><S sid=""186"" ssid=""20"">Although we find that if Wikipedia contains an article on a particular subject in some language, the article will tend to be topically similar to the articles about that subject in other languages, we also find that across the whole collection different languages emphasize topics to different extents.</S>","Aim_Citation,Method_Citation"
3,D09-1092,P11-2084,0,"Mimno et al, 2009",0,"(Mimno et al, 2009) retrieve a list of potential translations simply by selecting a small number N of the most probable words in both languages and then add the Cartesian product of these sets for every topic to a set of candidate translations","(Mimno et al, 2009) retrieve a list of potential translations simply by selecting a small number N of the most probable words in both languages and then add the Cartesian product of these sets for every topic to a set of candidate translations","'109','128','137','138'","<S sid=""109"" ssid=""58"">One simple way to achieve this topic alignment is to add a small set of comparable document tuples that provide sufficient &#8220;glue&#8221; to bind the topics together.</S><S sid=""128"" ssid=""77"">Although the PLTM is clearly not a substitute for a machine translation system&#8212;it has no way to represent syntax or even multi-word phrases&#8212;it is clear from the examples in figure 2 that the sets of high probability words in different languages for a given topic are likely to include translations.</S><S sid=""137"" ssid=""86"">For every topic t we select a small number K of the most probable words in English (e) and in each &#8220;translation&#8221; language (E): Wte and Wtt, respectively.</S><S sid=""138"" ssid=""87"">We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.</S>",Method_Citation
4,D09-1092,E12-1014,0,"Mimno et al, 2009",0,"Our Wikipedia-based topic similarity feature, w (f, e), is similar in spirit to polylingualtopic models (Mimno et al 2009), but it is scalable to full bilingual lexicon induction","Our Wikipedia-based topic similarity feature, w (f, e), is similar in spirit to polylingual topic models (Mimno et al 2009), but it is scalable to full bilingual lexicon induction","'0','2','25','30'","<S sid=""0"">Polylingual Topic Models</S><S sid=""2"" ssid=""2"">Meanwhile, massive collections of interlinked documents in dozens of languages, such as Wikipedia, are now widely available, calling for tools that can characterize content in many languages.</S><S sid=""25"" ssid=""1"">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing, 2007).</S><S sid=""30"" ssid=""6"">However, they evaluate their model on only two languages (English and Chinese), and do not use the model to detect differences between languages.</S>",Method_Citation
5,D09-1092,D11-1086,0,"Mimno et al, 2009",0,"of English document and the second half of its aligned foreign language document (Mimno et al,2009)","For Europarl data sets, we artificially make them comparable by considering the first half of English document and the second half of its aligned foreign language document (Mimno et al,2009)","'20','21','22','148'","<S sid=""20"" ssid=""16"">We also explore how the characteristics of different languages affect topic model performance.</S><S sid=""21"" ssid=""17"">The second corpus, Wikipedia articles in twelve languages, contains sets of documents that are not translations of one another, but are very likely to be about similar concepts.</S><S sid=""22"" ssid=""18"">We use this corpus to explore the ability of the model both to infer similarities between vocabularies in different languages, and to detect differences in topic emphasis between languages.</S><S sid=""148"" ssid=""97"">To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.</S>","Aim_Citation,Results_Citation"
6,D09-1092,N12-1007,0,"Mimno et al, 2009",0,"Since the PLTM is not a contribution of this paper, we refer the interested reader to (Mimno et al, 2009) for more details","Since the PLTM is not a contribution of this paper, we refer the interested reader to (Mimno et al, 2009) for more details","'9','92','107','192'","<S sid=""9"" ssid=""5"">In this paper, we present the polylingual topic model (PLTM).</S><S sid=""92"" ssid=""41"">We perform five estimation runs for each document and then calculate standard errors using a bootstrap method.</S><S sid=""107"" ssid=""56"">No paper is exactly comparable to any other paper, but they are all roughly topically similar.</S><S sid=""192"" ssid=""1"">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>","Hypothesis_Citation,Method_Citation"
7,D09-1092,N12-1007,0,2009,0,"Evaluation Corpus The automatic evaluation of cross-lingual co reference systems requires annotated 10Mimno et al (2009) showed that so long as the proportion of topically-aligned to non-aligned documents exceeded 0.25, the topic distributions (as measured by mean Jensen-Shannon Divergence between distributions) did not degrade significantly","Mimno et al (2009) showed that so long as the proportion of topically-aligned to non-aligned documents exceeded 0.25, the topic distributions (as measured by mean Jensen-Shannon Divergence between distributions) did not degrade significantly","'118','119','121','122'","<S sid=""118"" ssid=""67"">We calculate the Jensen-Shannon divergence between the topic distributions for each pair of individual documents in S that were originally part of the same tuple prior to separation.</S><S sid=""119"" ssid=""68"">The lower the divergence, the more similar the distributions are to each other.</S><S sid=""121"" ssid=""70"">Table 3 shows mean JS divergences for each value of p. As expected, JS divergence is greater than that obtained when all tuples are left intact.</S><S sid=""122"" ssid=""71"">Divergence drops significantly when the proportion of &#8220;glue&#8221; tuples increases from 0.01 to 0.25.</S>",Method_Citation
8,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"Similarly, Polylingual Topic Models (PLTM) (Mimno et al, 2009) generalized LDA to tuples of documents from multiple languages","Similarly, Polylingual Topic Models (PLTM) (Mimno et al, 2009) generalized LDA to tuples of documents from multiple languages","'3','36','105','192'","<S sid=""3"" ssid=""3"">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid=""36"" ssid=""2"">Each tuple is a set of documents that are loosely equivalent to each other, but written in different languages, e.g., corresponding Wikipedia articles in French, English and German.</S><S sid=""105"" ssid=""54"">An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct, non-comparable documents in multiple languages.</S><S sid=""192"" ssid=""1"">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>","Hypothesis_Citation,Method_Citation"
9,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"Our baseline joint PLSA model (JPLSA) is closely related to the poly-lingual LDA model of (Mimno et al, 2009)","Our baseline joint PLSA model (JPLSA) is closely related to the poly-lingual LDA model of (Mimno et al, 2009)","'3','18','65','99'","<S sid=""3"" ssid=""3"">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid=""18"" ssid=""14"">In this paper, we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid=""65"" ssid=""14"">This topic provides an illustration of the variation in technical terminology captured by PLTM, including the wide array of acronyms used by different languages.</S><S sid=""99"" ssid=""48"">Figure 5 shows the proportion of all tokens in English and Finnish assigned to each topic under LDA and PLTM with 800 topics.</S>",Method_Citation
10,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"We describe the model for two languages, but it is straightforward to generalize to more than two languages, as in (Mimno et al, 2009)","We describe the model for two languages, but it is straightforward to generalize to more than two languages, as in (Mimno et al, 2009)","'3','4','19','30'","<S sid=""3"" ssid=""3"">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid=""4"" ssid=""4"">We explore the model&#8217;s characteristics using two large corpora, each with over ten different languages, and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid=""19"" ssid=""15"">We employ a set of direct translations, the EuroParl corpus, to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S><S sid=""30"" ssid=""6"">However, they evaluate their model on only two languages (English and Chinese), and do not use the model to detect differences between languages.</S>",Method_Citation
11,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"j=1 P (z2j|?) P (w 2 j| ?z2j) The difference between the JPLSA model and the poly-lingual topic model of (Mimno et al, 2009) is that we merge the vocabularies in the two languages and learn topic-specific word distributions over these merged vocabularies, instead of having pairs of topic-specific word distributions, one for each language, like in (Mimno et al, 2009)","The difference between the JPLSA model and the poly-lingual topic model of (Mimno et al, 2009) is that we merge the vocabularies in the two languages and learn topic-specific word distributions over these merged vocabularies, instead of having pairs of topic-specific word distributions, one for each language, like in (Mimno et al, 2009)","'22','30','77','155'","<S sid=""22"" ssid=""18"">We use this corpus to explore the ability of the model both to infer similarities between vocabularies in different languages, and to detect differences in topic emphasis between languages.</S><S sid=""30"" ssid=""6"">However, they evaluate their model on only two languages (English and Chinese), and do not use the model to detect differences between languages.</S><S sid=""77"" ssid=""26"">Maximum topic probability in document Although the posterior distribution over topics for each tuple is not concentrated on one topic, it is worth checking that this is not simply because the model is assigning a single topic to the 1We use the R density function. tokens in each of the languages.</S><S sid=""155"" ssid=""104"">Finally, for each pair of languages (&#8220;query&#8221; and &#8220;target&#8221;) we calculate the difference between the topic distribution for each held-out document in the query language and the topic distribution for each held-out document in the target language.</S>",Method_Citation
12,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"Another difference between our model and the poly-lingual LDA model of (Mimno et al, 2009) is that we use maximum aposteriori (MAP) instead of Bayesian inference","Another difference between our model and the poly-lingual LDA model of (Mimno et al, 2009) is that we use maximum aposteriori (MAP) instead of Bayesian inference","'22','30','31','154'","<S sid=""22"" ssid=""18"">We use this corpus to explore the ability of the model both to infer similarities between vocabularies in different languages, and to detect differences in topic emphasis between languages.</S><S sid=""30"" ssid=""6"">However, they evaluate their model on only two languages (English and Chinese), and do not use the model to detect differences between languages.</S><S sid=""31"" ssid=""7"">They also provide little analysis of the differences between polylingual and single-language topic models.</S><S sid=""154"" ssid=""103"">We then use this model to infer topic distributions for each of the 11 documents in each of the held-out document tuples using a method similar to that used to calculate held-out probabilities (Wallach et al., 2009).</S>",Method_Citation
13,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"For computing distance we used the L1-norm of the difference, which worked a bit better than the Jensen Shannon divergence between the topic vectors used in (Mimno et al, 2009)","For computing distance we used the L1-norm of the difference, which worked a bit better than the Jensen Shannon divergence between the topic vectors used in (Mimno et al, 2009)","'6','154','155','156'","<S sid=""6"" ssid=""2"">Topic models have been used for analyzing topic trends in research literature (Mann et al., 2006; Hall et al., 2008), inferring captions for images (Blei and Jordan, 2003), social network analysis in email (McCallum et al., 2005), and expanding queries with topically related words in information retrieval (Wei and Croft, 2006).</S><S sid=""154"" ssid=""103"">We then use this model to infer topic distributions for each of the 11 documents in each of the held-out document tuples using a method similar to that used to calculate held-out probabilities (Wallach et al., 2009).</S><S sid=""155"" ssid=""104"">Finally, for each pair of languages (&#8220;query&#8221; and &#8220;target&#8221;) we calculate the difference between the topic distribution for each held-out document in the query language and the topic distribution for each held-out document in the target language.</S><S sid=""156"" ssid=""105"">We use both Jensen-Shannon divergence and cosine distance.</S>",Method_Citation
15,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"In previously reported work, (Mimno et al, 2009) evaluate parallel document retrieval using PLTM on Europarl speeches in English and Spanish, using training and test sets of size similar to ours","In previously reported work, (Mimno et al, 2009) evaluate parallel document retrieval using PLTM on Europarl speeches in English and Spanish, using training and test sets of size similar to ours","'19','20','148','154'","<S sid=""19"" ssid=""15"">We employ a set of direct translations, the EuroParl corpus, to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S><S sid=""20"" ssid=""16"">We also explore how the characteristics of different languages affect topic model performance.</S><S sid=""148"" ssid=""97"">To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.</S><S sid=""154"" ssid=""103"">We then use this model to infer topic distributions for each of the 11 documents in each of the held-out document tuples using a method similar to that used to calculate held-out probabilities (Wallach et al., 2009).</S>",Method_Citation
16,D09-1092,W12-3117,0,"Mimno et al, 2009",0,"We assume that a higher dimensionality can lead to a better repartitioning of the vocabulary over the topics. Multilingual LDA has been used before in natural language processing ,e.g .polylingual topic models (Mimno et al, 2009) or multilingual topic models for unaligned text (Boyd-Graber and Blei, 2009)","Multilingual LDA has been used before in natural language processing, e.g. polylingual topic models (Mimno et al, 2009) or multilingual topic models for unaligned text (Boyd-Graber and Blei, 2009)","'6','30','35','105'","<S sid=""6"" ssid=""2"">Topic models have been used for analyzing topic trends in research literature (Mann et al., 2006; Hall et al., 2008), inferring captions for images (Blei and Jordan, 2003), social network analysis in email (McCallum et al., 2005), and expanding queries with topically related words in information retrieval (Wei and Croft, 2006).</S><S sid=""30"" ssid=""6"">However, they evaluate their model on only two languages (English and Chinese), and do not use the model to detect differences between languages.</S><S sid=""35"" ssid=""1"">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.</S><S sid=""105"" ssid=""54"">An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct, non-comparable documents in multiple languages.</S>",Results_Citation
17,D09-1092,W11-2133,0,2009,0,"ji =wjk? M j i? m k=1w j k, (1) where M j is the topic distribution of document j and wk is the number of occurrences of phrase pair Xk in document j. Mimno et al (2009) extend the original concept of LDA to support polylingual topic models (PLTM), both on parallel (such as EuroParl) and partly comparable documents (such as Wikipediaarticles)","Mimno et al (2009) extend the original concept of LDA to support polylingual topic models (PLTM), both on parallel (such as EuroParl) and partly comparable documents (such as Wikipedia articles)","'35','169','170','171'","<S sid=""35"" ssid=""1"">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.</S><S sid=""169"" ssid=""3"">In this section, we explore two questions relating to comparable text corpora and polylingual topic modeling.</S><S sid=""170"" ssid=""4"">First, we explore whether comparable document tuples support the alignment of fine-grained topics, as demonstrated earlier using parallel documents.</S><S sid=""171"" ssid=""5"">This property is useful for building machine translation systems as well as for human readers who are either learning new languages or analyzing texts in languages they do not know.</S>","Hypothesis_Citation,Method_Citation"
18,D09-1092,W11-2133,0,2009,0,Tuple-specific topic distributions arelearned using LDA with distinct topic-word concentration parameters? l. Mimno et al (2009) show that PLTM sufficiently aligns topics in parallel corpora,Mimno et al (2009) show that PLTM sufficiently aligns topics in parallel corpora,"'126','147','192','193'","<S sid=""126"" ssid=""75"">These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.</S><S sid=""147"" ssid=""96"">These aligned document pairs could then be fed into standard machine translation systems as training data.</S><S sid=""192"" ssid=""1"">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S sid=""193"" ssid=""2"">We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.</S>",Method_Citation
19,D09-1092,P14-2110,0,"Mimno et al, 2009",0,"A good candidate for multilingual topic analyses are polylin gual topic models (Mimno et al, 2009), which learn topics for multiple languages, creating tuples of language specific distributions over monolingual vocabularies for each topic","A good candidate for multilingual topic analyses are polylingual topic models (Mimno et al, 2009), which learn topics for multiple languages, creating tuples of language specific distributions over monolingual vocabularies for each topic","'30','85','105','193'","<S sid=""30"" ssid=""6"">However, they evaluate their model on only two languages (English and Chinese), and do not use the model to detect differences between languages.</S><S sid=""85"" ssid=""34"">Smoothed histograms of inter&#8722;language JS divergence A topic model specifies a probability distribution over documents, or in the case of PLTM, document tuples.</S><S sid=""105"" ssid=""54"">An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct, non-comparable documents in multiple languages.</S><S sid=""193"" ssid=""2"">We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.</S>","Hypothesis_Citation,Results_Citation"
20,D09-1092,P14-2110,0,2009,0,"3csLDATo train a polylingual topic model on social media, we make two modifications to the model of Mimno et al (2009): add a token specific language variable, and a process for identifying aligned top ics. First ,polylingual topic models require parallel or comparable corpora in which each document has an assigned language","To train a polylingual topic model on social media, we make two modifications to the model of Mimno et al (2009): add a token specific language variable, and a process for identifying aligned top ics","'3','24','77','192'","<S sid=""3"" ssid=""3"">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid=""24"" ssid=""20"">By linking topics across languages, polylingual topic models can increase cross-cultural understanding by providing readers with the ability to characterize the contents of collections in unfamiliar languages and identify trends in topic prevalence.</S><S sid=""77"" ssid=""26"">Maximum topic probability in document Although the posterior distribution over topics for each tuple is not concentrated on one topic, it is worth checking that this is not simply because the model is assigning a single topic to the 1We use the R density function. tokens in each of the languages.</S><S sid=""192"" ssid=""1"">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>","Results_Citation,Method_Citation"
