( Actually , we use a minor variant described in [ 4 ] .. We created a parser based upon the maximumentropy-inspired model of the last section , smoothed using standard deleted interpolation . In keeping with the standard methodology [ 5 , 9,10,15,17 ] , we used the Penn Wall Street Journal tree-bank [ 16 ] with sections 2-21 for training , section 23 for testing , and section 24 for development ( debugging and tuning ) . In this section we evaluate the effects of the various changes we have made by running various versions of our current program . In our work we assume that any feature can occur at most once , so features are boolean-valued : 0 if the pattern does not occur , 1 if it does . First , it uses a clustering scheme on words to give the system a & amp ; quot ; soft & amp ; quot ; clustering of heads and sub-heads . The results of [ 13 ] achieved by combining the aforementioned three-best parsers also suggest that the limit on tree-bank trained parsers is much higher than previously thought . We have presented a lexicalized Markov grammar parsing model that achieves ( using the now standard training/testing/development sections of the Penn treebank ) an average precision/recall of 91.1 % on sentences of length & lt ; 40 and 89.5 % on sentences of length & lt ; 100 . In the simplest of such models ,