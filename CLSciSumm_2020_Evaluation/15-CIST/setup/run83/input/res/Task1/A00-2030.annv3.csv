Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,A00-2030,W01-0510,0,"Miller et al., 2000",0,"Section 5 compares our approach tooth ers in the literature, in particular that of (Miller et al., 2000)","Section 5 compares our approach too thiers in the literature, in particular that of (Miller et al., 2000)","'11','40','52','65'","<S sid=""11"" ssid=""1"">We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh, 1998).</S><S sid=""40"" ssid=""8"">Further details are discussed in the section Tree Augmentation.</S><S sid=""52"" ssid=""1"">In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machinegenerated syntactic parse trees.</S><S sid=""65"" ssid=""6"">We illustrate the generation process by walking through a few of the steps of the parse shown in Figure 3.</S>","Aim_Citation,Hypothesis_Citation,Method_Citation"
2,A00-2030,W01-0510,0,"Miller et al, 2000",0,"The basic approach we described is very similar to the one presented in (Miller et al, 2000) however there are a few major di erences:  in our approach the augmentation of the syn tactic tags with semantic tags is straightforward due to the fact that the semantic constituents are matched exactly 5. The approach in (Miller","The basic approach we described is very similar to the one presented in (Miller et al, 2000) however there are a few major differences:  in our approach the augmentation of the syn tactic tags with semantic tags is straightforward due to the fact that the semantic constituents are matched exactly 5. The approach in (Miller","'18','23','61','62'","<S sid=""18"" ssid=""1"">Almost all approaches to information extraction &#8212; even at the sentence level &#8212; are based on the divide-and-conquer strategy of reducing a complex problem to a set of simpler ones.</S><S sid=""23"" ssid=""6"">An integrated model can limit the propagation of errors by making all decisions jointly.</S><S sid=""61"" ssid=""2"">The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.</S><S sid=""62"" ssid=""3"">For each constituent, the head is generated first, followed by the modifiers, which are generated from the head outward.</S>",Method_Citation
3,A00-2030,W01-0510,0,"Miller et al, 2000",0,"The semantic annotation required by our task is much simpler than that employed by (Miller et al, 2000)","The semantic annotation required by our task is much simpler than that employed by (Miller et al, 2000)","'10','11','107','108'","<S sid=""10"" ssid=""8"">Instead, our parsing algorithm, trained on the UPenn TREEBANK, was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S><S sid=""11"" ssid=""1"">We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh, 1998).</S><S sid=""107"" ssid=""4"">The semantic training corpus was produced by students according to a simple set of guidelines.</S><S sid=""108"" ssid=""5"">This simple semantic annotation was the only source of task knowledge used to configure the model.</S>",Method_Citation
4,A00-2030,W01-0510,0,"Miller et al, 2000",0,"One possibly bene cial extension of our work suggested by (Miller et al, 2000) would be to add semantic tags describing relations between entities (slots), in which case the semantic constraints would not be structured strictly on the two levels used in the current approach, respectively frame and slot level","One possibly beneficial extension of our work suggested by (Miller et al, 2000) would be to add semantic tags describing relations between entities (slots), in which case the semantic constraints would not be structured strictly on the two levels used in the current approach, respectively frame and slot level","'30','31','52','94'","<S sid=""30"" ssid=""13"">Although each model differed in its detailed probability structure, we believed that the essential elements of all three models could be generalized in a single probability model.</S><S sid=""31"" ssid=""14"">If the single generalized model could then be extended to semantic analysis, all necessary sentence level processing would be contained in that model.</S><S sid=""52"" ssid=""1"">In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machinegenerated syntactic parse trees.</S><S sid=""94"" ssid=""13"">Given a new sentence, the outcome of this search process is a tree structure that encodes both the syntactic and semantic structure of the sentence.</S>",Method_Citation
5,A00-2030,W01-0510,0,"Miller et al, 2000",0,"Similar to the approach in (Miller et al, 2000 )weinitialized the SLM statistics from the UPenn Tree bank parse trees (about 1Mwds of training data) at the rst training stage, see Section 3","Similar to the approach in (Miller et al, 2000) we initialized the SLM statistics from the UPenn Tree bank parse trees","'33','34','41','60'","<S sid=""33"" ssid=""1"">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S sid=""34"" ssid=""2"">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S><S sid=""41"" ssid=""1"">To train our integrated model, we required a large corpus of augmented parse trees.</S><S sid=""60"" ssid=""1"">In our statistical model, trees are generated according to a process similar to that described in (Collins 1996, 1997).</S>",Method_Citation
6,A00-2030,P14-1078,0,"Miller et al, 2000",0,"Rule-based methods (Miller et al, 2000) employ a number of linguistic rules to capture relation patterns","Rule-based methods (Miller et al, 2000) employ a number of linguistic rules to capture relation patterns","'16','18','58','60'","<S sid=""16"" ssid=""6"">For the following example, the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.</S><S sid=""18"" ssid=""1"">Almost all approaches to information extraction &#8212; even at the sentence level &#8212; are based on the divide-and-conquer strategy of reducing a complex problem to a set of simpler ones.</S><S sid=""58"" ssid=""4"">Whenever a relation involves an entity that is not a direct descendant of that relation in the parse tree, semantic pointer labels are attached to all of the intermediate nodes.</S><S sid=""60"" ssid=""1"">In our statistical model, trees are generated according to a process similar to that described in (Collins 1996, 1997).</S>","Hypothesis_Citation,Method_Citation"
7,A00-2030,P05-1061,0,2000,0,"One interesting system that does not belong to the above class is that of Miller et al (2000), who take the view that relation extraction is just a form of probabilistic parsing where parse trees are augmented to identify all relations","One interesting system that does not belong to the above class is that of Miller et al (2000), who take the view that relation extraction is just a form of probabilistic parsing where parse trees are augmented to identify all relations","'1','34','58','61'","<S sid=""1"" ssid=""1"">Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard.</S><S sid=""34"" ssid=""2"">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S><S sid=""58"" ssid=""4"">Whenever a relation involves an entity that is not a direct descendant of that relation in the parse tree, semantic pointer labels are attached to all of the intermediate nodes.</S><S sid=""61"" ssid=""2"">The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.</S>","Hypothesis_Citation,Method_Citation"
8,A00-2030,P05-1053,0,2000,0,"Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees","Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees","'33','34','52','58'","<S sid=""33"" ssid=""1"">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S sid=""34"" ssid=""2"">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S><S sid=""52"" ssid=""1"">In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machinegenerated syntactic parse trees.</S><S sid=""58"" ssid=""4"">Whenever a relation involves an entity that is not a direct descendant of that relation in the parse tree, semantic pointer labels are attached to all of the intermediate nodes.</S>","Hypothesis_Citation,Method_Citation"
9,A00-2030,P05-1053,0,2000,0,"Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model","Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model","'32','61','62','101'","<S sid=""32"" ssid=""15"">Because generative statistical models had already proven successful for each of the first three stages, we were optimistic that some of their properties &#8212; especially their ability to learn from large amounts of data, and their robustness when presented with unexpected inputs &#8212; would also benefit semantic analysis.</S><S sid=""61"" ssid=""2"">The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.</S><S sid=""62"" ssid=""3"">For each constituent, the head is generated first, followed by the modifiers, which are generated from the head outward.</S><S sid=""101"" ssid=""6"">We evaluated part-of-speech tagging and parsing accuracy on the Wall Street Journal using a now standard procedure (see Collins 97), and evaluated name finding accuracy on the MUC7 named entity test.</S>","Hypothesis_Citation,Method_Citation"
10,A00-2030,H05-1094,0,2000,0,"(Miller et al, 2000) have combined entity recognition, parsing, and relation extraction into a jointly-trained single statistical parsing model that achieves improved performance on all the subtasks. Part of the contribution of the current work is to suggest that joint decoding can be effective even when joint training is not possible because jointly-labeled data is unavailable","(Miller et al, 2000) have combined entity recognition, parsing, and relation extraction into a jointly-trained single statistical parsing model that achieves improved performance on all the subtasks. Part of the contribution of the current work is to suggest that joint decoding can be effective even when joint training is not possible because jointly-labeled data is unavailable","'32','58','61','101'","<S sid=""32"" ssid=""15"">Because generative statistical models had already proven successful for each of the first three stages, we were optimistic that some of their properties &#8212; especially their ability to learn from large amounts of data, and their robustness when presented with unexpected inputs &#8212; would also benefit semantic analysis.</S><S sid=""58"" ssid=""4"">Whenever a relation involves an entity that is not a direct descendant of that relation in the parse tree, semantic pointer labels are attached to all of the intermediate nodes.</S><S sid=""61"" ssid=""2"">The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.</S><S sid=""101"" ssid=""6"">We evaluated part-of-speech tagging and parsing accuracy on the Wall Street Journal using a now standard procedure (see Collins 97), and evaluated name finding accuracy on the MUC7 named entity test.</S>",Method_Citation
11,A00-2030,P04-1054,0,2000,0,Miller et al (2000) propose an integrated statistical parsing technique that augments parse trees with semantic labels denoting entity and relation types,Miller et al (2000) propose an integrated statistical parsing technique that augments parse trees with semantic labels denoting entity and relation types,"'2','33','52','58'","<S sid=""2"" ssid=""2"">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S sid=""33"" ssid=""1"">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S sid=""52"" ssid=""1"">In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machinegenerated syntactic parse trees.</S><S sid=""58"" ssid=""4"">Whenever a relation involves an entity that is not a direct descendant of that relation in the parse tree, semantic pointer labels are attached to all of the intermediate nodes.</S>","Hypothesis_Citation,Method_Citation"
12,A00-2030,P04-1054,0,2000,0,"WhereasMiller et al (2000) use a generative model to produce parse information as well as relation information, we hypothesize that a technique discriminatively trained to classify relations will achieve better performance","Whereas Miller et al (2000) use a generative model to produce parse information as well as relation information, we hypothesize that a technique discriminatively trained to classify relations will achieve better performance","'1','2','51','52'","<S sid=""1"" ssid=""1"">Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard.</S><S sid=""2"" ssid=""2"">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S sid=""51"" ssid=""11"">To produce a corpus of augmented parse trees, we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus, now annotated with complete augmented trees like that in Figure 3.</S><S sid=""52"" ssid=""1"">In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machinegenerated syntactic parse trees.</S>","Hypothesis_Citation,Method_Citation"
13,A00-2030,W05-0602,0,"Miller et al, 2000",0,"The syntactic model in (Miller et al, 2000) is similar to Collins?, but doesnot use features like sub cat frames and distance measures","The syntactic model in (Miller et al, 2000) is similar to Collins', but does not use features like subcat frames and distance measures","'28','52','60','105'","<S sid=""28"" ssid=""11"">Finally, our newly constructed parser, like that of (Collins 1997), was based on a generative statistical model.</S><S sid=""52"" ssid=""1"">In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machinegenerated syntactic parse trees.</S><S sid=""60"" ssid=""1"">In our statistical model, trees are generated according to a process similar to that described in (Collins 1996, 1997).</S><S sid=""105"" ssid=""2"">A single model proved capable of performing all necessary sentential processing, both syntactic and semantic.</S>","Hypothesis_Citation,Method_Citation"
14,A00-2030,N07-2041,0,"Miller et al, 2000",0,"Similar to the approach in (Miller et al, 2000) and (Kulick et al, 2004), our parser integrates both syntactic and semantic annotations into a single annotation as shown in Figure 2","Similar to the approach in (Miller et al, 2000) and (Kulick et al, 2004), our parser integrates both syntactic and semantic annotations into a single annotation as shown in Figure 2","'10','50','51','52'","<S sid=""10"" ssid=""8"">Instead, our parsing algorithm, trained on the UPenn TREEBANK, was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S><S sid=""50"" ssid=""10"">Figure 4 shows an example of the semantic annotation, which was the only type of manual annotation we performed.</S><S sid=""51"" ssid=""11"">To produce a corpus of augmented parse trees, we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus, now annotated with complete augmented trees like that in Figure 3.</S><S sid=""52"" ssid=""1"">In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machinegenerated syntactic parse trees.</S>","Hypothesis_Citation,Method_Citation"
15,A00-2030,W10-2924,0,2000,0,Miller et al (2000) adapt a probabilistic context-free parser for information extraction by augmenting syntactic labels with entity and relation labels,Miller et al (2000) adapt a probabilistic context-free parser for information extraction by augmenting syntactic labels with entity and relation labels,"'2','6','58','105'","<S sid=""2"" ssid=""2"">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S sid=""6"" ssid=""4"">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S sid=""58"" ssid=""4"">Whenever a relation involves an entity that is not a direct descendant of that relation in the parse tree, semantic pointer labels are attached to all of the intermediate nodes.</S><S sid=""105"" ssid=""2"">A single model proved capable of performing all necessary sentential processing, both syntactic and semantic.</S>",Method_Citation
16,A00-2030,W06-0508,0,"Miller et al, 2000",0,"Most of the approaches for relation extraction rely on the mapping of syntactic dependencies, such as SVO, onto semantic relations, using either pattern matching or other strategies, such as probabilistic parsing for trees augmented with annotations for entities and relations (Miller et al 2000), or clustering of semantically similar syntactic dependencies, according to their selectional restrictions (Gamallo et al, 2002)","Most of the approaches for relation extraction rely on the mapping of syntactic dependencies, such as SVO, onto semantic relations, using either pattern matching or other strategies, such as probabilistic parsing for trees augmented with annotations for entities and relations (Miller et al 2000), or clustering of semantically similar syntactic dependencies, according to their selectional restrictions (Gamallo et al, 2002)","'10','34','51','58'","<S sid=""10"" ssid=""8"">Instead, our parsing algorithm, trained on the UPenn TREEBANK, was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S><S sid=""34"" ssid=""2"">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S><S sid=""51"" ssid=""11"">To produce a corpus of augmented parse trees, we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus, now annotated with complete augmented trees like that in Figure 3.</S><S sid=""58"" ssid=""4"">Whenever a relation involves an entity that is not a direct descendant of that relation in the parse tree, semantic pointer labels are attached to all of the intermediate nodes.</S>","Hypothesis_Citation,Method_Citation"
17,A00-2030,P07-1055,0,"Miller et al, 2000",0,"This includes parsing and relation extraction (Miller et al, 2000), entity labeling and relation extraction (Roth and Yih, 2004), and part-of-speech tagging and chunking (Sutton et al, 2004)","This includes parsing and relation extraction (Miller et al, 2000), entity labeling and relation extraction (Roth and Yih, 2004), and part-of-speech tagging and chunking (Sutton et al, 2004)","'2','58','61','101'","<S sid=""2"" ssid=""2"">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S sid=""58"" ssid=""4"">Whenever a relation involves an entity that is not a direct descendant of that relation in the parse tree, semantic pointer labels are attached to all of the intermediate nodes.</S><S sid=""61"" ssid=""2"">The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.</S><S sid=""101"" ssid=""6"">We evaluated part-of-speech tagging and parsing accuracy on the Wall Street Journal using a now standard procedure (see Collins 97), and evaluated name finding accuracy on the MUC7 named entity test.</S>","Hypothesis_Citation,Implication_Citation,Method_Citation"
18,A00-2030,W05-0636,0,2000,0,"For example, Miller et al (2000) showed that performing parsing and information extraction in a joint model improves performance on both tasks","For example, Miller et al (2000) showed that performing parsing and information extraction in a joint model improves performance on both tasks","'1','2','11','104'","<S sid=""1"" ssid=""1"">Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard.</S><S sid=""2"" ssid=""2"">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S sid=""11"" ssid=""1"">We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh, 1998).</S><S sid=""104"" ssid=""1"">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>","Aim_Citation,Hypothesis_Citation,Method_Citation"
19,A00-2030,N06-1037,0,2000,0,Miller et al (2000) address the task of relation extraction from the statistical parsing viewpoint,Miller et al (2000) address the task of relation extraction from the statistical parsing viewpoint,"'0','1','2','60'","<S sid=""0"">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid=""1"" ssid=""1"">Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard.</S><S sid=""2"" ssid=""2"">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S sid=""60"" ssid=""1"">In our statistical model, trees are generated according to a process similar to that described in (Collins 1996, 1997).</S>","Hypothesis_Citation,Method_Citation"
