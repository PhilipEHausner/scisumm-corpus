Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination). Under this framework, we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology. Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition. Importantly, additive models capture composition by considering all vector components representing the meaning of the verb and its subject, whereas multiplicative models consider a subset, namely non-zero components. Moreover, the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald, 2000) and word association norms (Denhire and Lemaire, 2004). The downside of this approach is that differences in meaning are qualitative rather than quantitative, and degrees of similarity cannot be expressed easily. This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure. Combining the multiplicative model with an additive model, which does not suffer from this problem, could mitigate this problem: pi = &#945;ui +&#946;vi +&#947;uivi (11) where &#945;, &#946;, and &#947; are weighting constants. Central in these models is the notion of compositionality &#8212; the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them.