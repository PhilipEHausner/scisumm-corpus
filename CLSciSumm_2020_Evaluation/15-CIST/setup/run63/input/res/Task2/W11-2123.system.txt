Therefore , we want state to encode the minimum amount of information necessary to properly compute language model scores , so that the decoder will be faster and make fewer search errors . The trie data structure is commonly used for language modeling . Performance improvements transfer to the Moses ( Koehn et al. , 2007 ) , cdec ( Dyer et al. , 2010 ) , and Joshua ( Li et al. , 2009 ) translation systems where our code has been integrated . The binary language model from Section 5.2 and text phrase table were forced into disk cache before each run . Language models are widely applied in natural language processing , and applications such as machine translation make very frequent queries . Overall , language modeling significantly impacts decoder performance . 8217 ; s improved-kneser-ney option and the default three pieces . To quantize , we use the binning method ( Federico and Bertoldi , 2006 ) that sorts values , divides into equally sized bins , and averages within each bin . To quantize , we use the binning method ( Federico and Bertoldi , 2006 ) that sorts values , divides into equally sized bins , and averages within each bin . Throughout this paper we compare with several packages : SRILM 1.5.12 ( Stolcke , 2002 ) is a popular toolkit based on tries used in several decoders . All language model queries issued by machine translation decoders follow a left-to-right pattern ,