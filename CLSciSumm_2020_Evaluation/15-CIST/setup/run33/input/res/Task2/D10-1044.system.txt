Our second contribution is to apply instance weighting at the level of phrase pairs . 8221 ; within a top-level probability that represents the linear combination.1 Following previous work ( Foster and Kuhn , 2007 ) , we circumvent this problem by choosing weights to optimize corpus loglikelihood , which is roughly speaking the training criterion used by the LM and TM themselves . 945 ; is a weight vector containing an element & It is difficult to directly compare the Matsoukas et al results with ours , since our out-of-domain corpus is homogeneous ; given heterogeneous training data , however , it would be trivial to include Matsoukas-style identity features in our instance-weighting model . where c & We will also directly compare with a baseline similar to the Matsoukas et al approach in order to measure the benefit from weighting phrase pairs ( or ngrams ) rather than full sentences . The corpora for both settings are summarized in table 1 . Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translatio . Second , rather than relying on a division of the corpus into manually-assigned portions , we use features intended to capture the usefulness of each phrase pair . This is less effective in our setting , where IN and OUT are disparate . 8220 ; ideal & First , we learn weights on individual phrase pairs rather than sentences . First , we learn weights on individual phrase pairs rather than sentences . 8220 ;