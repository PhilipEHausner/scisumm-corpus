This means that summing up over derivations of a tree in DOP yields the same probability as summing over all the isomorphic derivations in the PCFG . Bod ( 2001 ) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data . But even with cross-validation , ML-DOP is outperformed by the much simpler DOP1 model on both the ATIS and OVIS treebanks ( Bod 2000b ) . But even with cross-validation , ML-DOP is outperformed by the much simpler DOP1 model on both the ATIS and OVIS treebanks ( Bod 2000b ) . But even with cross-validation , ML-DOP is outperformed by the much simpler DOP1 model on both the ATIS and OVIS treebanks ( Bod 2000b ) . Bod ( 2001 ) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data . In the following section first results of SL-DOP and LS-DOP with a compact PCFG-reduction . The distinctive feature of the DOP approach when it was proposed in 1992 was to model sentence structures on the basis of previously observed frequencies of sentence structure fragments , without imposing any constraints on the size of these fragments . Thus , there are aj= ( bk+ 1 ) ( ci + 1 ) possible subtrees headed