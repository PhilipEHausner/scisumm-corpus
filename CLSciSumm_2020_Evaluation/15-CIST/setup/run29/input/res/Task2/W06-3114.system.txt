Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus . We are therefore applying a different method , which has been used at the 2005 DARPA/NIST evaluation . Most of these groups follow a phrase-based statistical approach to machine translation . Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus . Hence , we use the bootstrap resampling method described by Koehn ( 2004 ) . The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000 , which is excluded from the training data . Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus . The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000 , which is excluded from the training data . For statistics on this test set , refer to Figure 1 . The BLEU metric , as all currently proposed automatic metrics , is occasionally suspected to be biased towards statistical systems , especially the phrase-based systems currently in use . 8212 ; say , 1000 times & While we used the standard metrics of the community , the we way presented translations and prompted for assessment differed from other evaluation campaigns . Replacing this with an ranked evaluation seems to be more suitable . This is demonstrated by average scores over all systems , in terms of BLEU , fluency and adequacy