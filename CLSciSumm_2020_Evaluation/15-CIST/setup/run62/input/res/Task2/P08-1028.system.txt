Combining the multiplicative model with an additive model , which does not suffer from this problem , could mitigate this problem : pi = & Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition . This paper proposes a framework for representing the meaning of phrases and sentences in vector space . It also allows us to derive models in which composition makes use of background knowledge K and models in which composition has a dependence , via the argument R , on syntax . Our results show that the multiplicative models are superior and correlate significantly with behavioral data . Combining the multiplicative model with an additive model , which does not suffer from this problem , could mitigate this problem : pi = & An extreme form of this differential in the contribution of constituents is where one of the vectors , say u , contributes nothing at all to the combination : Admittedly the model in ( 8 ) is impoverished and rather simplistic , however it can serve as a simple baseline against which to compare more sophisticated models . Despite the popularity of additive models , our experimental results showed the superiority of models utilizing multiplicative combinations , at least for the sentence similarity task attempted here . So , if we assume that only the ith components of u and v contribute to the ith component of p , that these components are not