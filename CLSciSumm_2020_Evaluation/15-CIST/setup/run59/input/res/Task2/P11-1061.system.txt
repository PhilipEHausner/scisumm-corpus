We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model ( Berg- Kirkpatrick et al. , 2010 ) . Unsupervised learning approaches appear to be a natural solution to this problem , as they require only unannotated text for training models . Since our graph is built from a parallel corpus , we can use standard word alignment techniques to align the English sentences De 5Note that many combinations are impossible giving a PMI value of 0 ; e.g. ( 2010 ) . Our monolingual similarity function ( for connecting pairs of foreign trigram types ) is the same as the one used by Subramanya et al . By aggregating the POS labels of the English tokens to types , we can generate label distributions for the English vertices . Unsupervised learning approaches appear to be a natural solution to this problem , as they require only unannotated text for training models . Unsupervised learning approaches appear to be a natural solution to this problem , as they require only unannotated text for training models . 65533 ; f. As discussed in more detail in & For monolingual treebank data we relied on the CoNLL-X and CoNLL-2007 shared tasks on dependency parsing ( Buchholz and Marsi , 2006 ; Nivre et al. , 2007 ) . ( 2010 ) . ( 2010 ) found that this direct gradient method performed better ( & gt ; 7 % absolute accuracy )