The PROBING model was designed to improve upon SRILM by using linear probing hash tables ( though not arranged in a trie ) , allocating memory all at once ( eliminating the need for full pointers ) , and being easy to compile . The binary language model from Section 5.2 and text phrase table were forced into disk cache before each run . The PROBING model was designed to improve upon SRILM by using linear probing hash tables ( though not arranged in a trie ) , allocating memory all at once ( eliminating the need for full pointers ) , and being easy to compile . In addition to the optimizations specific to each datastructure described in Section 2 , we implement several general optimizations for language modeling . simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline . For even larger models , storing counts ( Talbot and Osborne , 2007 ; Pauls and Klein , 2011 ; Guthrie and Hepple , 2010 ) is a possibility . For the perplexity and translation tasks , we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 ( Koehn , 2005 ) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed . The set of n-grams appearing in a model is sparse , and we want to efficiently find their associated probabilities and backoff penalties . All language model queries issued