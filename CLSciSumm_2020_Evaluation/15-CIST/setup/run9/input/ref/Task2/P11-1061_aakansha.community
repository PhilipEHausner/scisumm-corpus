We extend Subramanya et al.&#8217;s intuitions to our bilingual setup. To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language. First, we use a novel graph-based framework for projecting syntactic information across language boundaries. Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus. To establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (&#167;3.3).3 Since we have no labeled foreign data, our goal is to project syntactic information from the English side to the foreign side. To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011). We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages. These universal POS categories not only facilitate the transfer of POS information from one language to another, but also relieve us from using controversial evaluation metrics,2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels. The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages. Second, we treat the projected labels as features in an unsupervised model (&#167;5), rather than using them directly for supervised training. We use the universal POS tagset of Petrov et al. (2011) in our experiments.10 This set C consists of the following 12 coarse-grained tags: NOUN (nouns), VERB (verbs), ADJ (adjectives), ADV (adverbs), PRON (pronouns), DET (determiners), ADP (prepositions or postpositions), NUM (numerals), CONJ (conjunctions), PRT (particles), PUNC (punctuation marks) and X (a catch-all for other categories such as abbreviations or foreign words). Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models.
