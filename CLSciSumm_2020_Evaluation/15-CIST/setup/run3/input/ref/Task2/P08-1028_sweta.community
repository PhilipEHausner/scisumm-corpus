So, if we assume that only the ith components of u and v contribute to the ith component of p, that these components are not dependent on i, and that the function is symmetric with regard to the interchange of u and v, we obtain a simpler instantiation of an additive model: Analogously, under the same assumptions, we obtain the following simpler multiplicative model: only the ith components of u and v contribute to the ith component of p. Another class of models can be derived by relaxing this constraint. An extreme form of this differential in the contribution of constituents is where one of the vectors, say u, contributes nothing at all to the combination: Admittedly the model in (8) is impoverished and rather simplistic, however it can serve as a simple baseline against which to compare more sophisticated models. This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure. Central in these models is the notion of compositionality &#8212; the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them. The resulting vector is sparser but expresses more succinctly the meaning of the predicate-argument structure, and thus allows semantic similarity to be modelled more accurately. We present a general framework for vector-based composition which allows us to consider different classes of models. Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition. Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination). We formulated composition as a function of two vectors and introduced several models based on addition and multiplication. While neural networks can readily represent single distinct objects, in the case of multiple objects there are fundamental difficulties in keeping track of which features are bound to which objects. The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors. Now, if we assume that p lies in the same space as u and v, avoiding the issues of dimensionality associated with tensor products, and that f is a linear function, for simplicity, of the cartesian product of u and v, then we generate a class of additive models: where A and B are matrices which determine the contributions made by u and v to the product p. In contrast, if we assume that f is a linear function of the tensor product of u and v, then we obtain multiplicative models: where C is a tensor of rank 3, which projects the tensor product of u and v onto the space of p. Further constraints can be introduced to reduce the free parameters in these models. We expect better models to yield a pattern of similarity scores like those observed in the human ratings (see Figure 2). The multiplicative model yields a better fit with the experimental data, &#961; = 0.17.
