A new language model , based on probabilistic top-down parsing , will be outlined and compared with the previous literature , and extensive empirical results will be presented which demonstrate its utility . The perplexity improvement was achieved by simply taking the existing parsing model and applying it , with no extra training beyond that done for parsing . A candidate analysis C = ( D , 8 , PD , F , wn consists of a derivation D , a stack S. a derivation probability PD , a figure of merit F , and a string w7 remaining to be parsed . The basic idea is that we want the beam to be very wide if there are few analyses that have been advanced , but relatively narrow if many analyses have been advanced . In addition , we show the average number of rule expansions considered per word , that is , the number of rule expansions for which a probability was calculated ( see Roark and Charniak [ 2000 ] ) , and the average number of analyses advanced to the next priority queue per word . The first set of results looks at the performance of the parser on the standard corpora for statistical parsing trials : Sections 2-21 ( 989,860 words , 39,832 sentences ) of the Penn Treebank ( Marcus , Santorini , and Marcinkiewicz 1993 ) served as the training data , Section 24 ( 34,199 words , 1,346 sentences ) as the