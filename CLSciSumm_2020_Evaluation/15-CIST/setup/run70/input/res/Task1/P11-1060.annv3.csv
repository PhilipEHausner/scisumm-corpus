Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,P11-1060,D11-1039,0,2011,0,"Clarkeet al (2010) and Liang et al (2011) describe approaches for learning semantic parsers from questions paired with database answers, while Goldwasser et al (2011) presents work on unsupervised learning","Clarke et al (2010) and Liang et al (2011) describe approaches for learning semantic parsers from questions paired with database answers, while Goldwasser et al (2011) presents work on unsupervised learning","[1, 2, 7, 9]","['<S sid=""1"" ssid=""1"">Compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.</S>', '<S sid=""2"" ssid=""2"">In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.</S>', '<S sid=""7"" ssid=""3"">Supervised semantic parsers (Zelle and Mooney, 1996; Tang and Mooney, 2001; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010) rely on manual annotation of logical forms, which is expensive.</S>', '<S sid=""9"" ssid=""5"">As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S>']","Method_Citation,Results_Citation"
2,P11-1060,P13-1092,0,2011,0,"In particular, Clarke et al (2010) and Liang et al (2011) proposed methods to learn from question answer pairs alone, which represents a significant advance","In particular, Clarke et al (2010) and Liang et al (2011) proposed methods to learn from question answer pairs alone, which represents a significant advance","[2, 6, 9, 10]","['<S sid=""2"" ssid=""2"">In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.</S>', '<S sid=""6"" ssid=""2"">Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing).</S>', '<S sid=""9"" ssid=""5"">As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S>', '<S sid=""10"" ssid=""6"">However, we still model the logical form (now as a latent variable) to capture the complexities of language.</S>']",Method_Citation
3,P11-1060,P13-1092,0,2011,0,"To handle syntax-semantics mismatch, GUSP introduces a novel dependency-based meaning representation 1Clarke et al (2010) and Liang et al (2011) used the annotated logical forms to compute answers for their experiments","To handle syntax-semantics mismatch, GUSP introduces a novel dependency-based meaning representation. Clarke et al (2010) and Liang et al (2011) used the annotated logical forms to compute answers for their experiments","[2, 3, 8, 9]","['<S sid=""2"" ssid=""2"">In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.</S>', '<S sid=""3"" ssid=""3"">In tackling this challenging learning problem, we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S>', '<S sid=""8"" ssid=""4"">On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives.</S>', '<S sid=""9"" ssid=""5"">As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S>']",Method_Citation
4,P11-1060,P13-1092,0,2011,0,"More recently, Liang et al (2011 )proposedDCS for dependency-based compositional semantics, which represents a semantic parse as a tree with nodes representing database elements and operations, and edges representing relational joins","More recently, Liang et al (2011) proposed DCS for dependency-based compositional semantics, which represents a semantic parse as a tree with nodes representing database elements and operations, and edges representing relational joins","[25, 46, 48, 49]","['<S sid=""25"" ssid=""1"">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>', '<S sid=""46"" ssid=""22"">Formally: Definition 1 (DCS trees) Let Z be the set of DCS trees, where each z &#8712; Z consists of (i) a predicate for each child i, the ji-th component of v must equal the j\'i-th component of some t in the child&#8217;s denotation (t &#8712; JciKw).</S>', '<S sid=""48"" ssid=""24"">In addition, trees enable efficient computation, thereby establishing a new connection between dependency syntax and efficient semantic evaluation.</S>', '<S sid=""49"" ssid=""25"">Aggregate relation DCS trees that only use join relations can represent arbitrarily complex compositional structures, but they cannot capture higherorder phenomena in language.</S>']",Method_Citation
5,P11-1060,P13-1092,0,"Liang et al, 2011",0,"GUSP represents meaning by a semantic tree, which is similar to DCS (Liang et al, 2011)","GUSP represents meaning by a semantic tree, which is similar to DCS (Liang et al, 2011)","[12, 34, 38, 107]","['<S sid=""12"" ssid=""8"">We represent logical forms z as labeled trees, induced automatically from (x, y) pairs.</S>', '<S sid=""34"" ssid=""10"">Figure 2(a) shows an example of a DCS tree.</S>', '<S sid=""38"" ssid=""14"">Let us start by considering a DCS tree z with only join relations.</S>', '<S sid=""107"" ssid=""83"">Our model is arc-factored, so we can sum over all DCS trees in ZL(x) using dynamic programming.</S>']",Method_Citation
6,P11-1060,W12-2802,0,2011,0,"Matuszek et al [2010], Liang et al [2011] and Chen and Mooney [2011] describe models that learn compositional semantics, but word meanings are symbolic structures rather than patterns of features in the external world","Matuszek et al [2010], Liang et al [2011] and Chen and Mooney [2011] describe models that learn compositional semantics, but word meanings are symbolic structures rather than patterns of features in the external world","[7, 164, 165, 166]","['<S sid=""7"" ssid=""3"">Supervised semantic parsers (Zelle and Mooney, 1996; Tang and Mooney, 2001; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010) rely on manual annotation of logical forms, which is expensive.</S>', '<S sid=""164"" ssid=""49"">Past work has also fofor the second.</S>', '<S sid=""165"" ssid=""50"">These rules are often too stringent, cused on aligning text to a world (Liang et al., 2009), and for complex utterances, especially in free word- using text in reinforcement learning (Branavan et al., order languages, either disharmonic combinators are 2009; Branavan et al., 2010), and many others.</S>', '<S sid=""166"" ssid=""51"">Our employed (Zettlemoyer and Collins, 2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language&#8212;think grounded al., 2010). compositional semantics.</S>']",Method_Citation
7,P11-1060,P13-2009,0,"Liang et al, 2011",0,"It is well-studied in NLP, and a wide variety of methods have been proposed to tackle it ,e.g. rule-based (Popescu et al, 2003), super vised (Zelle, 1995), unsupervised (Goldwasser et al., 2011), and response-based (Liang et al, 2011)","It is well-studied in NLP, and a wide variety of methods have been proposed to tackle it ,e.g. rule-based (Popescu et al, 2003), super vised (Zelle, 1995), unsupervised (Goldwasser et al., 2011), and response-based (Liang et al, 2011)","[7, 8, 156, 165]","['<S sid=""7"" ssid=""3"">Supervised semantic parsers (Zelle and Mooney, 1996; Tang and Mooney, 2001; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010) rely on manual annotation of logical forms, which is expensive.</S>', '<S sid=""8"" ssid=""4"">On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives.</S>', '<S sid=""156"" ssid=""41"">There has been a fair amount of past work on no predicates), confusion of Washington state with this topic: Liang et al. (2010) induces combinatory Washington D.C., learning the wrong lexical asso- logic programs in a non-linguistic setting.</S>', '<S sid=""165"" ssid=""50"">These rules are often too stringent, cused on aligning text to a world (Liang et al., 2009), and for complex utterances, especially in free word- using text in reinforcement learning (Branavan et al., order languages, either disharmonic combinators are 2009; Branavan et al., 2010), and many others.</S>']","Hypothesis_Citation,Method_Citation"
8,P11-1060,D12-1069,0,Liangetal2011,0,"One line of work eliminates the need for an annotated logical form, instead using only the correct answer for a database query (Liangetal2011) or even a binary correct/incorrect signal (Clarke et al2010)","One line of work eliminates the need for an annotated logical form, instead using only the correct answer for a database query (Lianget al 2011) or even a binary correct/incorrect signal (Clarke et al2010)","[9, 10, 135, 136]","['<S sid=""9"" ssid=""5"">As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S>', '<S sid=""10"" ssid=""6"">However, we still model the logical form (now as a latent variable) to capture the complexities of language.</S>', '<S sid=""135"" ssid=""20"">SEMRESP requires a lexicon of 1.42 words per non-value predicate, WordNet features, and syntactic parse trees; DCS requires only words for the domain-independent predicates (overall, around 0.5 words per non-value predicate), POS tags, and very simple indicator features.</S>', '<S sid=""136"" ssid=""21"">In fact, DCS performs comparably to even the version of SEMRESP trained using logical forms.</S>']",Results_Citation
9,P11-1060,N12-1049,0,2011,0,"For example, Liang et al (2011) constructs a latent parse similar in structure to a dependency grammar, but representing a logical form","For example, Liang et al (2011) constructs a latent parse similar in structure to a dependency grammar, but representing a logical form","[2, 3, 5, 22]","['<S sid=""2"" ssid=""2"">In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.</S>', '<S sid=""3"" ssid=""3"">In tackling this challenging learning problem, we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S>', '<S sid=""5"" ssid=""1"">What is the total population of the ten largest capitals in the US?</S>', '<S sid=""22"" ssid=""18"">The logical forms in this framework are trees, which is desirable for two reasons: (i) they parallel syntactic dependency trees, which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.</S>']",Method_Citation
10,P11-1060,P12-1045,0,2011,0,Clarke et al (2010) and Liang et al (2011) trained systems on question and answer pairs by automatically finding semantic interpretations of the questions that would generate the correct answers,Clarke et al (2010) and Liang et al (2011) trained systems on question and answer pairs by automatically finding semantic interpretations of the questions that would generate the correct answers,"[1, 2, 9, 132]","['<S sid=""1"" ssid=""1"">Compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.</S>', '<S sid=""2"" ssid=""2"">In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.</S>', '<S sid=""9"" ssid=""5"">As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S>', '<S sid=""132"" ssid=""17"">Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.</S>']","Method_Citation,Results_Citation"
11,P11-1060,P14-1008,0,"Liang et al,2011",0,"Dependency-based Compositional Semantics (DCS) provides an intuitive way to model semantics of questions, by using simple dependency-like trees (Liang et al, 2011)","Dependency-based Compositional Semantics (DCS) provides an intuitive way to model semantics of questions, by using simple dependency-like trees (Liang et al, 2011)","[21, 22, 25, 36]","['<S sid=""21"" ssid=""17"">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>', '<S sid=""22"" ssid=""18"">The logical forms in this framework are trees, which is desirable for two reasons: (i) they parallel syntactic dependency trees, which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.</S>', '<S sid=""25"" ssid=""1"">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>', '<S sid=""36"" ssid=""12"">It is this transparency between syntax and semantics provided by DCS which leads to a simple and streamlined compositional semantics suitable for program induction.</S>']","Hypothesis_Citation,Method_Citation"
12,P11-1060,P14-1008,0,"Liang et al, 2011",0,"DCS trees has been proposed to represent natural language semantics with a structure similar to dependency trees (Liang et al, 2011) (Figure 1)","DCS trees has been proposed to represent natural language semantics with a structure similar to dependency trees (Liang et al, 2011) (Figure 1)","[47, 48, 49, 94]","['<S sid=""47"" ssid=""23"">This algorithm is linear in the number of nodes times the size of the denotations.1 Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure, which will facilitate parsing.</S>', '<S sid=""48"" ssid=""24"">In addition, trees enable efficient computation, thereby establishing a new connection between dependency syntax and efficient semantic evaluation.</S>', '<S sid=""49"" ssid=""25"">Aggregate relation DCS trees that only use join relations can represent arbitrarily complex compositional structures, but they cannot capture higherorder phenomena in language.</S>', '<S sid=""94"" ssid=""70"">We now turn to the task of mapping natural language For the example in Figure 4(b), the de- utterances to DCS trees.</S>']",Method_Citation
13,P11-1060,P14-1008,0,"Liang et al, 2011",0,"are explained in? 2.5. 5http: //nlp.stanford.edu/software/corenlp.shtml 6 In (Liang et al, 2011) DCS trees are learned from QApairs and database entries","In (Liang et al, 2011) DCS trees are learned from QA pairs and database entries","[12, 33, 166, 167]","['<S sid=""12"" ssid=""8"">We represent logical forms z as labeled trees, induced automatically from (x, y) pairs.</S>', '<S sid=""33"" ssid=""9"">As another example, w(average) = {(S, &#175;x) : We write a DCS tree z as hp; r1 : c1; ... ; rm : cmi.</S>', '<S sid=""166"" ssid=""51"">Our employed (Zettlemoyer and Collins, 2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language&#8212;think grounded al., 2010). compositional semantics.</S>', '<S sid=""167"" ssid=""52"">In DCS, we start with lexical triggers, which are 6 Conclusion more basic than CCG lexical entries.</S>']","Hypothesis_Citation,Method_Citation"
14,P11-1060,P14-1008,0,"Liang et al, 2011",0,"as in the sentence? Tropi cal storm Debby is blamed for death?, which is a tropical storm, is Debby, etc. Technically, each germ in a DCS tree indicates a variable when the DCS tree is translated to a FOL formula, and the abstract denotation of the germ corresponds to the set of consistent values (Liang et al, 2011) of that variable","Technically, each germ in a DCS tree indicates a variable when the DCS tree is translated to a FOL formula, and the abstract denotation of the germ corresponds to the set of consistent values (Liang et al, 2011) of that variable","[43, 46, 50, 86]","['<S sid=""43"" ssid=""19"">Computation We can compute the denotation JzKw of a DCS tree z by exploiting dynamic programming on trees (Dechter, 2003).</S>', '<S sid=""46"" ssid=""22"">Formally: Definition 1 (DCS trees) Let Z be the set of DCS trees, where each z &#8712; Z consists of (i) a predicate for each child i, the ji-th component of v must equal the j\'i-th component of some t in the child&#8217;s denotation (t &#8712; JciKw).</S>', '<S sid=""50"" ssid=""26"">For example, consider the phrase number of major cities, and suppose that number corresponds to the count predicate.</S>', '<S sid=""86"" ssid=""62"">Formally, extraction simply moves the i-th column to the front: Xi(d) = d[i, &#8722;(i, &#248;)]{&#945;1 = &#248;}.</S>']",Method_Citation
15,P11-1060,D11-1140,0,2011,0,Clarke et al (2010) and Liang et al (2011) replace semantic annotations in the training set with target answers which are more easily available,Clarke et al (2010) and Liang et al (2011) replace semantic annotations in the training set with target answers which are more easily available,"[1, 2, 8, 9]","['<S sid=""1"" ssid=""1"">Compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.</S>', '<S sid=""2"" ssid=""2"">In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.</S>', '<S sid=""8"" ssid=""4"">On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives.</S>', '<S sid=""9"" ssid=""5"">As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S>']","Method_Citation,Results_Citation"
16,P11-1060,D11-1140,0,"Liang et al, 2011",0,"and Collins, 2005, 2007),? -WASP (Wong and Mooney, 2007), UBL (Kwiatkowski et al, 2010) systems and DCS (Liang et al, 2011)","WASP (Wong and Mooney, 2007), UBL (Kwiatkowski et al, 2010) systems and DCS (Liang et al, 2011)","[7, 133, 140, 166]","['<S sid=""7"" ssid=""3"">Supervised semantic parsers (Zelle and Mooney, 1996; Tang and Mooney, 2001; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010) rely on manual annotation of logical forms, which is expensive.</S>', '<S sid=""133"" ssid=""18"">Table 2 shows that our system using lexical triggers L (henceforth, DCS) outperforms SEMRESP (78.9% over 73.2%).</S>', '<S sid=""140"" ssid=""25"">Table 3 shows that even DCS, which does not use prototypes, is comparable to the best previous system (Kwiatkowski et al., 2010), and by adding a few prototypes, DCS+ offers a decisive edge (91.1% over 88.9% on GEO).</S>', '<S sid=""166"" ssid=""51"">Our employed (Zettlemoyer and Collins, 2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language&#8212;think grounded al., 2010). compositional semantics.</S>']","Aim_Citation,Hypothesis_Citation,Method_Citation"
17,P11-1060,P13-1007,0,2011,0,"In general, every plural NPpotentially introduces an implicit universal, ranging 1For example, Liang et al (2011) in their state-of-the-art statistical semantic parser within the domain of natural language queries to databases, explicitly devise quantifier scoping in the semantic model","For example, Liang et al (2011) in their state-of-the-art statistical semantic parser within the domain of natural language queries to databases, explicitly devise quantifier scoping in the semantic model","[7, 9, 162, 164]","['<S sid=""7"" ssid=""3"">Supervised semantic parsers (Zelle and Mooney, 1996; Tang and Mooney, 2001; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010) rely on manual annotation of logical forms, which is expensive.</S>', '<S sid=""9"" ssid=""5"">As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S>', '<S sid=""162"" ssid=""47"">The lexicon en- tions computed against a world (grounding) is becodes information about how each word can used in coming increasingly popular.</S>', '<S sid=""164"" ssid=""49"">Past work has also fofor the second.</S>']",Method_Citation
18,P11-1060,D11-1022,0,2011,0,"DD-ADMM may be useful in other frameworks involving logical constraints, such as the models for compositional semantics presented by Liang et al (2011)","DD-ADMM may be useful in other frameworks involving logical constraints, such as the models for compositional semantics presented by Liang et al (2011)","[17, 22, 25, 36]","['<S sid=""17"" ssid=""13"">The dominant paradigm in compositional semantics is Montague semantics, which constructs lambda calculus forms in a bottom-up manner.</S>', '<S sid=""22"" ssid=""18"">The logical forms in this framework are trees, which is desirable for two reasons: (i) they parallel syntactic dependency trees, which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.</S>', '<S sid=""25"" ssid=""1"">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>', '<S sid=""36"" ssid=""12"">It is this transparency between syntax and semantics provided by DCS which leads to a simple and streamlined compositional semantics suitable for program induction.</S>']",Results_Citation
19,P11-1060,P12-1051,0,2011,0,"In fact, for any CFG G, it 1See Liang et al (2011) for work in representing lambda calculus expressions with trees","In fact, for any CFG G, it 1See Liang et al (2011) for work in representing lambda calculus expressions with trees","[17, 50, 150, 166]","['<S sid=""17"" ssid=""13"">The dominant paradigm in compositional semantics is Montague semantics, which constructs lambda calculus forms in a bottom-up manner.</S>', '<S sid=""50"" ssid=""26"">For example, consider the phrase number of major cities, and suppose that number corresponds to the count predicate.</S>', '<S sid=""150"" ssid=""35"">For example, area (by virtue which results in programs (DCS trees) which are of being a noun) triggers many predicates: city, much simpler than the logically-equivalent lambda state, area, etc.</S>', '<S sid=""166"" ssid=""51"">Our employed (Zettlemoyer and Collins, 2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language&#8212;think grounded al., 2010). compositional semantics.</S>']","Hypothesis_Citation,Method_Citation"
