Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,P11-1061,P11-1144,0,2011,0,Subramanya et al? s model was extended by Das and Petrov (2011) to induce part-of-speech dictionaries for unsupervised learning of taggers,Subramanya et al's model was extended by Das and Petrov (2011) to induce part-of-speech dictionaries for unsupervised learning of taggers,"['17', '37', '1', '26', '45']","['    <S sid=""17"" ssid=""13"">Second, we treat the projected labels as features in an unsupervised model (&#167;5), rather than using them directly for supervised training.</S>\n', '    <S sid=""37"" ssid=""3"">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S>\n', '    <S sid=""1"" ssid=""1"">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language.</S>\n', '    <S sid=""26"" ssid=""3"">As discussed in more detail in &#167;3, we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types, while the vertices on the English side are individual word types.</S>\n', '    <S sid=""45"" ssid=""11"">Furthermore, we do not connect the English vertices to each other, but only to foreign language vertices.4 The graph vertices are extracted from the different sides of a parallel corpus (De, Df) and an additional unlabeled monolingual foreign corpus Ff, which will be used later for training.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Hypothesis_Citation', 'Result_Citation']"
3,P11-1061,P14-1126,0,2011,0,"Fortunately, some recently proposed POS taggers, such as the POStagger of Das and Petrov (2011), rely only on labeled training data for English and the same kind of parallel text in our approach","Fortunately, some recently proposed POS taggers, such as the POS tagger of Das and Petrov (2011), rely only on labeled training data for English and the same kind of parallel text in our approach","['124', '42', '126', '113', '68']","['    <S sid=""124"" ssid=""24"">We tried two versions of our graph-based approach: feature after the first stage of label propagation (Eq.</S>\n', '    <S sid=""42"" ssid=""8"">The foreign language vertices (denoted by Vf) correspond to foreign trigram types, exactly as in Subramanya et al. (2010).</S>\n', '    <S sid=""126"" ssid=""26"">Because many foreign word types are not aligned to an English word (see Table 3), and we do not run label propagation on the foreign side, we expect the projected information to have less coverage.</S>\n', '    <S sid=""113"" ssid=""13"">For each language under consideration, Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S>\n', '    <S sid=""68"" ssid=""34"">In the label propagation stage, we propagate the automatic English tags to the aligned Italian trigram types, followed by further propagation solely among the Italian vertices. the Italian vertices are connected to an automatically labeled English vertex.</S>\n']","['Hypothesis_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
4,P11-1061,N12-1086,0,"Das and Petrov, 2011",0,"Applications have ranged from domain adaptation of part-of-speech (POS) taggers (Subramanya et al, 2010), unsupervised learning ofPOS taggers by using bilingual graph-based projections (Das and Petrov, 2011), and shallow semantic parsing for unknown predicates (Das and Smith,2011)","Applications have ranged from domain adaptation of part-of-speech (POS) taggers (Subramanya et al, 2010), unsupervised learning of POS taggers by using bilingual graph-based projections (Das and Petrov, 2011), and shallow semantic parsing for unknown predicates (Das and Smith,2011)","['111', '99', '113', '157', '29']","['    <S sid=""111"" ssid=""11"">We use the universal POS tagset of Petrov et al. (2011) in our experiments.10 This set C consists of the following 12 coarse-grained tags: NOUN (nouns), VERB (verbs), ADJ (adjectives), ADV (adverbs), PRON (pronouns), DET (determiners), ADP (prepositions or postpositions), NUM (numerals), CONJ (conjunctions), PRT (particles), PUNC (punctuation marks) and X (a catch-all for other categories such as abbreviations or foreign words).</S>\n', '    <S sid=""99"" ssid=""30"">It would have therefore also been possible to use the integer programming (IP) based approach of Ravi and Knight (2009) instead of the feature-HMM for POS induction on the foreign side.</S>\n', '    <S sid=""113"" ssid=""13"">For each language under consideration, Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S>\n', '    <S sid=""157"" ssid=""20"">As a result, its POS tag needs to be induced in the &#8220;No LP&#8221; case, while the 11A word level paired-t-test is significant at p &lt; 0.01 for Danish, Greek, Italian, Portuguese, Spanish and Swedish, and p &lt; 0.05 for Dutch. correct tag is available as a constraint feature in the &#8220;With LP&#8221; case.</S>\n', '    <S sid=""29"" ssid=""6"">To establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (&#167;3.3).3 Since we have no labeled foreign data, our goal is to project syntactic information from the English side to the foreign side.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
5,P11-1061,N12-1086,0,2011,0,"Following Das and Petrov (2011) and Subramanya et al (2010), a similarity score between two trigram types was computed by measuring the cosine similarity between their empirical sentential context statistics","Following Das and Petrov (2011) and Subramanya et al (2010), a similarity score between two trigram types was computed by measuring the cosine similarity between their empirical sentential context statistics","['51', '47', '52', '67', '137']","['    <S sid=""51"" ssid=""17"">For each trigram type x2 x3 x4 in a sequence x1 x2 x3 x4 x5, we count how many times that trigram type co-occurs with the different instantiations of each concept, and compute the point-wise mutual information (PMI) between the two.5 The similarity between two trigram types is given by summing over the PMI values over feature instantiations that they have in common.</S>\n', '    <S sid=""47"" ssid=""13"">Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).</S>\n', '    <S sid=""52"" ssid=""18"">This is similar to stacking the different feature instantiations into long (sparse) vectors and computing the cosine similarity between them.</S>\n', '    <S sid=""67"" ssid=""33"">It is worth noting that the middle words of the Italian trigrams are nouns too, which exhibits the fact that the similarity metric connects types having the same syntactic category.</S>\n', '    <S sid=""137"" ssid=""37"">The graph was constructed using 2 million trigrams; we chose these by truncating the parallel datasets up to the number of sentence pairs that contained 2 million trigrams.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
6,P11-1061,N12-1086,0,"Das and Petrov, 2011",0,"Sparsity is desirable in settings where labeled development data for tuning thresholds that select the most probable labels for a given type is unavailable (e.g., Das and Petrov, 2011)","Sparsity is desirable in settings where labeled development data for tuning thresholds that select the most probable labels for a given type is unavailable (e.g., Das and Petrov, 2011)","['17', '135', '107', '157', '134']","['    <S sid=""17"" ssid=""13"">Second, we treat the projected labels as features in an unsupervised model (&#167;5), rather than using them directly for supervised training.</S>\n', '    <S sid=""135"" ssid=""35"">For seven out of eight languages a threshold of 0.2 gave the best results for our final model, which indicates that for languages without any validation set, r = 0.2 can be used.</S>\n', '    <S sid=""107"" ssid=""7"">Of course, we are primarily interested in applying our techniques to languages for which no labeled resources are available.</S>\n', '    <S sid=""157"" ssid=""20"">As a result, its POS tag needs to be induced in the &#8220;No LP&#8221; case, while the 11A word level paired-t-test is significant at p &lt; 0.01 for Danish, Greek, Italian, Portuguese, Spanish and Swedish, and p &lt; 0.05 for Dutch. correct tag is available as a constraint feature in the &#8220;With LP&#8221; case.</S>\n', '    <S sid=""134"" ssid=""34"">Because we don&#8217;t have a separate development set, we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
7,P11-1061,N12-1052,0,2011,0,"Specifically, by replacing fine-grained language specific part-of-speech tags with universal part-of-speech tags, generated with the method described by Das and Petrov (2011), a universal parser is achieved that can be applied to any language for which universal part-of-speech tags are available. Below, we extend this approach to universal parsing by adding cross-lingual word cluster features","Specifically, by replacing fine-grained language specific part-of-speech tags with universal part-of-speech tags, generated with the method described by Das and Petrov (2011), a universal parser is achieved that can be applied to any language for which universal part-of-speech tags are available. Below, we extend this approach to universal parsing by adding cross-lingual word cluster features","['1', '45', '65', '72', '157']","['    <S sid=""1"" ssid=""1"">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language.</S>\n', '    <S sid=""45"" ssid=""11"">Furthermore, we do not connect the English vertices to each other, but only to foreign language vertices.4 The graph vertices are extracted from the different sides of a parallel corpus (De, Df) and an additional unlabeled monolingual foreign corpus Ff, which will be used later for training.</S>\n', '    <S sid=""65"" ssid=""31"">In this particular case, all English vertices are labeled as nouns by the supervised tagger.</S>\n', '    <S sid=""72"" ssid=""3"">In the first stage, we run a single step of label propagation, which transfers the label distributions from the English vertices to the connected foreign language vertices (say, Vf&#65533;) at the periphery of the graph.</S>\n', '    <S sid=""157"" ssid=""20"">As a result, its POS tag needs to be induced in the &#8220;No LP&#8221; case, while the 11A word level paired-t-test is significant at p &lt; 0.01 for Danish, Greek, Italian, Portuguese, Spanish and Swedish, and p &lt; 0.05 for Dutch. correct tag is available as a constraint feature in the &#8220;With LP&#8221; case.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
8,P11-1061,N12-1052,0,2011,0,"We study the impact of using cross-lingual cluster features by comparing the strong delexicalized baseline model of McDonald et al (2011), which only has features derived from universal part-of-speech tags, projected from English with the method of Das and Petrov (2011), to the same model when adding features derived from cross-lingual clusters","We study the impact of using cross-lingual cluster features by comparing the strong delexicalized baseline model of McDonald et al (2011), which only has features derived from universal part-of-speech tags, projected from English with the method of Das and Petrov (2011), to the same model when adding features derived from cross-lingual clusters","['45', '37', '147', '17', '38']","['    <S sid=""45"" ssid=""11"">Furthermore, we do not connect the English vertices to each other, but only to foreign language vertices.4 The graph vertices are extracted from the different sides of a parallel corpus (De, Df) and an additional unlabeled monolingual foreign corpus Ff, which will be used later for training.</S>\n', '    <S sid=""37"" ssid=""3"">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S>\n', '    <S sid=""147"" ssid=""10"">As indicated by bolding, for seven out of eight languages the improvements of the &#8220;With LP&#8221; setting are statistically significant with respect to the other models, including the &#8220;No LP&#8221; setting.11 Overall, it performs 10.4% better than the hitherto state-of-the-art feature-HMM baseline, and 4.6% better than direct projection, when we macro-average the accuracy over all languages.</S>\n', '    <S sid=""17"" ssid=""13"">Second, we treat the projected labels as features in an unsupervised model (&#167;5), rather than using them directly for supervised training.</S>\n', '    <S sid=""38"" ssid=""4"">More recently, Subramanya et al. (2010) defined a graph over the cliques in an underlying structured prediction model.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
9,P11-1061,N12-1090,0,"Das and Petrov, (2011)",0,"MT-based projection has been applied to various NLP tasks, such as part of-speech tagging (e.g., Das and Petrov (2011)), mention detection (e.g., Zitouni and Florian (2008)), and sentiment analysis (e.g., Mihalcea et al (2007)) .There have been two initial attempts to apply projection to create co reference-annotated data for aresource-poor language, both of which involve projecting hand-annotated co reference data from English to Romanian via a parallel corpus","MT-based projection has been applied to various NLP tasks, such as part of-speech tagging (e.g., Das and Petrov (2011)), mention detection (e.g., Zitouni and Florian (2008)), and sentiment analysis (e.g., Mihalcea et al (2007))","['57', '4', '147', '113', '134']","['    <S sid=""57"" ssid=""23"">Since our graph is built from a parallel corpus, we can use standard word alignment techniques to align the English sentences De 5Note that many combinations are impossible giving a PMI value of 0; e.g., when the trigram type and the feature instantiation don&#8217;t have words in common. and their foreign language translations Df.6 Label propagation in the graph will provide coverage and high recall, and we therefore extract only intersected high-confidence (&gt; 0.9) alignments De&#65533;f.</S>\n', '    <S sid=""4"" ssid=""4"">Across eight European languages, our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline, and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.</S>\n', '    <S sid=""147"" ssid=""10"">As indicated by bolding, for seven out of eight languages the improvements of the &#8220;With LP&#8221; setting are statistically significant with respect to the other models, including the &#8220;No LP&#8221; setting.11 Overall, it performs 10.4% better than the hitherto state-of-the-art feature-HMM baseline, and 4.6% better than direct projection, when we macro-average the accuracy over all languages.</S>\n', '    <S sid=""113"" ssid=""13"">For each language under consideration, Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S>\n', '    <S sid=""134"" ssid=""34"">Because we don&#8217;t have a separate development set, we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
10,P11-1061,W11-2205,0,2011,0,"For example, the multilingual PoS induction approach of Das and Petrov (2011) assumes no supervision for the language whose PoS tags are being 35 induced, but it assumes access to a labeled dataset of a different language. We begin by surveying recent work on unsupervised PoS tagging, focusing on the issue of evaluation (Section 2)","For example, the multilingual PoS induction approach of Das and Petrov (2011) assumes no supervision for the language whose PoS tags are being 35 induced, but it assumes access to a labeled dataset of a different language. We begin by surveying recent work on unsupervised PoS tagging, focusing on the issue of evaluation (Section 2)","['161', '29', '113', '111', '57']","['    <S sid=""161"" ssid=""4"">Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models.</S>\n', '    <S sid=""29"" ssid=""6"">To establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (&#167;3.3).3 Since we have no labeled foreign data, our goal is to project syntactic information from the English side to the foreign side.</S>\n', '    <S sid=""113"" ssid=""13"">For each language under consideration, Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S>\n', '    <S sid=""111"" ssid=""11"">We use the universal POS tagset of Petrov et al. (2011) in our experiments.10 This set C consists of the following 12 coarse-grained tags: NOUN (nouns), VERB (verbs), ADJ (adjectives), ADV (adverbs), PRON (pronouns), DET (determiners), ADP (prepositions or postpositions), NUM (numerals), CONJ (conjunctions), PRT (particles), PUNC (punctuation marks) and X (a catch-all for other categories such as abbreviations or foreign words).</S>\n', '    <S sid=""57"" ssid=""23"">Since our graph is built from a parallel corpus, we can use standard word alignment techniques to align the English sentences De 5Note that many combinations are impossible giving a PMI value of 0; e.g., when the trigram type and the feature instantiation don&#8217;t have words in common. and their foreign language translations Df.6 Label propagation in the graph will provide coverage and high recall, and we therefore extract only intersected high-confidence (&gt; 0.9) alignments De&#65533;f.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
11,P11-1061,P13-1155,0,"Das and Petrov, 2011",0,"(Das and Petrov, 2011) used graph-based label propagation for cross-lingual knowledge transfers to induce POS tags between two languages","(Das and Petrov, 2011) used graph-based label propagation for cross-lingual knowledge transfers to induce POS tags between two languages","['113', '3', '157', '24', '99']","['    <S sid=""113"" ssid=""13"">For each language under consideration, Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S>\n', '    <S sid=""3"" ssid=""3"">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al., 2010).</S>\n', '    <S sid=""157"" ssid=""20"">As a result, its POS tag needs to be induced in the &#8220;No LP&#8221; case, while the 11A word level paired-t-test is significant at p &lt; 0.01 for Danish, Greek, Italian, Portuguese, Spanish and Swedish, and p &lt; 0.05 for Dutch. correct tag is available as a constraint feature in the &#8220;With LP&#8221; case.</S>\n', '    <S sid=""24"" ssid=""1"">The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.</S>\n', '    <S sid=""99"" ssid=""30"">It would have therefore also been possible to use the integer programming (IP) based approach of Ravi and Knight (2009) instead of the feature-HMM for POS induction on the foreign side.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
12,P11-1061,D12-1127,0,2011,0,Recent work by Das and Petrov (2011 )buildsa dictionary for a particular language by transfer ring annotated data from a resource-rich language through the use of word alignments in parallel text,Recent work by Das and Petrov (2011 ) builds a dictionary for a particular language by transferring annotated data from a resource-rich language through the use of word alignments in parallel text,"['45', '57', '58', '36', '1']","['    <S sid=""45"" ssid=""11"">Furthermore, we do not connect the English vertices to each other, but only to foreign language vertices.4 The graph vertices are extracted from the different sides of a parallel corpus (De, Df) and an additional unlabeled monolingual foreign corpus Ff, which will be used later for training.</S>\n', '    <S sid=""57"" ssid=""23"">Since our graph is built from a parallel corpus, we can use standard word alignment techniques to align the English sentences De 5Note that many combinations are impossible giving a PMI value of 0; e.g., when the trigram type and the feature instantiation don&#8217;t have words in common. and their foreign language translations Df.6 Label propagation in the graph will provide coverage and high recall, and we therefore extract only intersected high-confidence (&gt; 0.9) alignments De&#65533;f.</S>\n', '    <S sid=""58"" ssid=""24"">Based on these high-confidence alignments we can extract tuples of the form [u H v], where u is a foreign trigram type, whose middle word aligns to an English word type v. Our bilingual similarity function then sets the edge weights in proportion to these tuple counts.</S>\n', '    <S sid=""36"" ssid=""2"">Graph construction for structured prediction problems such as POS tagging is non-trivial: on the one hand, using individual words as the vertices throws away the context necessary for disambiguation; on the other hand, it is unclear how to define (sequence) similarity if the vertices correspond to entire sentences.</S>\n', '    <S sid=""1"" ssid=""1"">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
13,P11-1061,D12-1127,0,"Das and Petrov, 2011",0,"Theseapproaches build a dictionary by transferring labeled data from a resource rich language (English) to a re source poor language (Das and Petrov, 2011)","These approaches build a dictionary by transferring labeled data from a resource rich language (English) to a resource poor language (Das and Petrov, 2011)","['1', '10', '45', '57', '2']","['    <S sid=""1"" ssid=""1"">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language.</S>\n', '    <S sid=""10"" ssid=""6"">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>\n', '    <S sid=""45"" ssid=""11"">Furthermore, we do not connect the English vertices to each other, but only to foreign language vertices.4 The graph vertices are extracted from the different sides of a parallel corpus (De, Df) and an additional unlabeled monolingual foreign corpus Ff, which will be used later for training.</S>\n', '    <S sid=""57"" ssid=""23"">Since our graph is built from a parallel corpus, we can use standard word alignment techniques to align the English sentences De 5Note that many combinations are impossible giving a PMI value of 0; e.g., when the trigram type and the feature instantiation don&#8217;t have words in common. and their foreign language translations Df.6 Label propagation in the graph will provide coverage and high recall, and we therefore extract only intersected high-confidence (&gt; 0.9) alignments De&#65533;f.</S>\n', '    <S sid=""2"" ssid=""2"">Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
14,P11-1061,P12-3012,0,"Das and Petrov, 2011",0,"In recent years research in Natural Language Processing (NLP) has been steadily moving towards multilingual processing: the availability of ever growing amounts of text in different languages ,infact, has been a major driving force behind research on multilingual approaches, from morphosyntactic (Das and Petrov, 2011) and syntactico semantic (Peirsman and Pado?, 2010) phenomena to high-end tasks like textual entailment (Mehdad et al., 2011) and sentiment analysis (Lu et al, 2011)","In recent years research in Natural Language Processing (NLP) has been steadily moving towards multilingual processing: the availability of ever growing amounts of text in different languages, in fact, has been a major driving force behind research on multilingual approaches, from morphosyntactic (Das and Petrov, 2011) and syntactico semantic (Peirsman and Pado?, 2010) phenomena to high-end tasks like textual entailment (Mehdad et al., 2011) and sentiment analysis (Lu et al, 2011)","['57', '58', '147', '4', '9']","['    <S sid=""57"" ssid=""23"">Since our graph is built from a parallel corpus, we can use standard word alignment techniques to align the English sentences De 5Note that many combinations are impossible giving a PMI value of 0; e.g., when the trigram type and the feature instantiation don&#8217;t have words in common. and their foreign language translations Df.6 Label propagation in the graph will provide coverage and high recall, and we therefore extract only intersected high-confidence (&gt; 0.9) alignments De&#65533;f.</S>\n', '    <S sid=""58"" ssid=""24"">Based on these high-confidence alignments we can extract tuples of the form [u H v], where u is a foreign trigram type, whose middle word aligns to an English word type v. Our bilingual similarity function then sets the edge weights in proportion to these tuple counts.</S>\n', '    <S sid=""147"" ssid=""10"">As indicated by bolding, for seven out of eight languages the improvements of the &#8220;With LP&#8221; setting are statistically significant with respect to the other models, including the &#8220;No LP&#8221; setting.11 Overall, it performs 10.4% better than the hitherto state-of-the-art feature-HMM baseline, and 4.6% better than direct projection, when we macro-average the accuracy over all languages.</S>\n', '    <S sid=""4"" ssid=""4"">Across eight European languages, our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline, and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.</S>\n', '    <S sid=""9"" ssid=""5"">Unfortunately, the best completely unsupervised English POS tagger (that does not make use of a tagging dictionary) reaches only 76.1% accuracy (Christodoulopoulos et al., 2010), making its practical usability questionable at best.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
15,P11-1061,D11-1006,0,2011,0,"Furthermore, we evaluate with both gold-standard part-of-speech tags, as well as predicted part-of speech tags from the projected part-of-speech tagger of Das and Petrov (2011) .2 This tagger relies only onlabeled training data for English, and achieves accuracies around 85% on the languages that we con sider","Furthermore, we evaluate with both gold-standard part-of-speech tags, as well as predicted part-of speech tags from the projected part-of-speech tagger of Das and Petrov (2011). This tagger relies only on labeled training data for English, and achieves accuracies around 85% on the languages that we consider","['1', '45', '157', '147', '6']","['    <S sid=""1"" ssid=""1"">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language.</S>\n', '    <S sid=""45"" ssid=""11"">Furthermore, we do not connect the English vertices to each other, but only to foreign language vertices.4 The graph vertices are extracted from the different sides of a parallel corpus (De, Df) and an additional unlabeled monolingual foreign corpus Ff, which will be used later for training.</S>\n', '    <S sid=""157"" ssid=""20"">As a result, its POS tag needs to be induced in the &#8220;No LP&#8221; case, while the 11A word level paired-t-test is significant at p &lt; 0.01 for Danish, Greek, Italian, Portuguese, Spanish and Swedish, and p &lt; 0.05 for Dutch. correct tag is available as a constraint feature in the &#8220;With LP&#8221; case.</S>\n', '    <S sid=""147"" ssid=""10"">As indicated by bolding, for seven out of eight languages the improvements of the &#8220;With LP&#8221; setting are statistically significant with respect to the other models, including the &#8220;No LP&#8221; setting.11 Overall, it performs 10.4% better than the hitherto state-of-the-art feature-HMM baseline, and 4.6% better than direct projection, when we macro-average the accuracy over all languages.</S>\n', '    <S sid=""6"" ssid=""2"">Supervised part-of-speech (POS) taggers, for example, approach the level of inter-annotator agreement (Shen et al., 2007, 97.3% accuracy for English).</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
16,P11-1061,D11-1006,0,2011,0,"In the first, we assumed that the test set for each target language had gold part-of-speech tags, and in the second we used predicted part-of-speech tags from the projection tagger of Das and Petrov (2011), which also uses English as the source language","In the first, we assumed that the test set for each target language had gold part-of-speech tags, and in the second we used predicted part-of-speech tags from the projection tagger of Das and Petrov (2011), which also uses English as the source language","['114', '71', '160', '1', '153']","['    <S sid=""114"" ssid=""14"">The supervised POS tagging accuracies (on this tagset) are shown in the last row of Table 2.</S>\n', '    <S sid=""71"" ssid=""2"">We use label propagation in two stages to generate soft labels on all the vertices in the graph.</S>\n', '    <S sid=""160"" ssid=""3"">Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data, but have translations into a resource-rich language.</S>\n', '    <S sid=""1"" ssid=""1"">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language.</S>\n', '    <S sid=""153"" ssid=""16"">Figure 2 shows an excerpt of a sentence from the Italian test set and the tags assigned by four different models, as well as the gold tags.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
17,P11-1061,P13-2112,0,2011,0,"This parallel data can be exploited to bridge languages, and in particular, transfer information from a highly-resourced language to a lesser-resourced language, to build unsupervised POS taggers. In this paper, we propose an unsupervised approach to POS tagging in a similar vein to the work of Das and Petrov (2011)","This parallel data can be exploited to bridge languages, and in particular, transfer information from a highly-resourced language to a lesser-resourced language, to build unsupervised POS taggers. In this paper, we propose an unsupervised approach to POS tagging in a similar vein to the work of Das and Petrov (2011)","['161', '10', '113', '1', '70']","['    <S sid=""161"" ssid=""4"">Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models.</S>\n', '    <S sid=""10"" ssid=""6"">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>\n', '    <S sid=""113"" ssid=""13"">For each language under consideration, Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S>\n', '    <S sid=""1"" ssid=""1"">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language.</S>\n', '    <S sid=""70"" ssid=""1"">Given the bilingual graph described in the previous section, we can use label propagation to project the English POS labels to the foreign language.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
18,P11-1061,P13-2112,0,2011,0,Das and Petrov (2011) achieved the current state-of-the-art for unsupervised tagging by exploiting high confidence alignments to copy tags from the source language to the target language,Das and Petrov (2011) achieved the current state-of-the-art for unsupervised tagging by exploiting high confidence alignments to copy tags from the source language to the target language,"['147', '1', '57', '5', '112']","['    <S sid=""147"" ssid=""10"">As indicated by bolding, for seven out of eight languages the improvements of the &#8220;With LP&#8221; setting are statistically significant with respect to the other models, including the &#8220;No LP&#8221; setting.11 Overall, it performs 10.4% better than the hitherto state-of-the-art feature-HMM baseline, and 4.6% better than direct projection, when we macro-average the accuracy over all languages.</S>\n', '    <S sid=""1"" ssid=""1"">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language.</S>\n', '    <S sid=""57"" ssid=""23"">Since our graph is built from a parallel corpus, we can use standard word alignment techniques to align the English sentences De 5Note that many combinations are impossible giving a PMI value of 0; e.g., when the trigram type and the feature instantiation don&#8217;t have words in common. and their foreign language translations Df.6 Label propagation in the graph will provide coverage and high recall, and we therefore extract only intersected high-confidence (&gt; 0.9) alignments De&#65533;f.</S>\n', '    <S sid=""5"" ssid=""1"">Supervised learning approaches have advanced the state-of-the-art on a variety of tasks in natural language processing, resulting in highly accurate systems.</S>\n', '    <S sid=""112"" ssid=""12"">While there might be some controversy about the exact definition of such a tagset, these 12 categories cover the most frequent part-of-speech and exist in one form or another in all of the languages that we studied.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
19,P11-1061,P13-2112,0,"Das and Petrov, 2011",0,"We have proposed a method for unsupervised POStagging that performs on par with the current state of-the-art (Das and Petrov, 2011), but is subs tan tially less-sophisticated (specifically not requiring convex optimization or a feature-based HMM)","We have proposed a method for unsupervised POS tagging that performs on par with the current state of-the-art (Das and Petrov, 2011), but is substantially less-sophisticated (specifically not requiring convex optimization or a feature-based HMM)","['111', '58', '99', '36', '4']","['    <S sid=""111"" ssid=""11"">We use the universal POS tagset of Petrov et al. (2011) in our experiments.10 This set C consists of the following 12 coarse-grained tags: NOUN (nouns), VERB (verbs), ADJ (adjectives), ADV (adverbs), PRON (pronouns), DET (determiners), ADP (prepositions or postpositions), NUM (numerals), CONJ (conjunctions), PRT (particles), PUNC (punctuation marks) and X (a catch-all for other categories such as abbreviations or foreign words).</S>\n', '    <S sid=""58"" ssid=""24"">Based on these high-confidence alignments we can extract tuples of the form [u H v], where u is a foreign trigram type, whose middle word aligns to an English word type v. Our bilingual similarity function then sets the edge weights in proportion to these tuple counts.</S>\n', '    <S sid=""99"" ssid=""30"">It would have therefore also been possible to use the integer programming (IP) based approach of Ravi and Knight (2009) instead of the feature-HMM for POS induction on the foreign side.</S>\n', '    <S sid=""36"" ssid=""2"">Graph construction for structured prediction problems such as POS tagging is non-trivial: on the one hand, using individual words as the vertices throws away the context necessary for disambiguation; on the other hand, it is unclear how to define (sequence) similarity if the vertices correspond to entire sentences.</S>\n', '    <S sid=""4"" ssid=""4"">Across eight European languages, our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline, and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
