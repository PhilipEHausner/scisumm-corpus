Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,D09-1092,P14-1004,0,"Mimno et al, 2009",0,"This configuration is similar to PolyLDA (Mimno et al, 2009) or LinkLDA (Yano et al, 2009), such that utterances from different parties are treated as different languages or blog-post and comments pairs","This configuration is similar to PolyLDA (Mimno et al, 2009) or LinkLDA (Yano et al, 2009), such that utterances from different parties are treated as different languages or blog-post and comments pairs","['6', '150', '40', '153', '128']","['    <S sid=""6"" ssid=""2"">Topic models have been used for analyzing topic trends in research literature (Mann et al., 2006; Hall et al., 2008), inferring captions for images (Blei and Jordan, 2003), social network analysis in email (McCallum et al., 2005), and expanding queries with topically related words in information retrieval (Wei and Croft, 2006).</S>\n', '    <S sid=""150"" ssid=""99"">In finding a lowdimensional semantic representation, topic models deliberately smooth over much of the variation present in language.</S>\n', '    <S sid=""40"" ssid=""6"">Anew document tuple w = (w1, ... , wL) is generated by first drawing a tuple-specific topic distribution from an asymmetric Dirichlet prior with concentration parameter &#945; and base measure m: Then, for each language l, a latent topic assignment is drawn for each token in that language: Finally, the observed tokens are themselves drawn using the language-specific topic parameters: wl &#8764; P(wl  |zl,&#934;l) = 11n &#966;lwl |zl .</S>\n', '    <S sid=""153"" ssid=""102"">In order to make the task more difficult, we train a relatively coarse-grained PLTM with 50 topics on the training set.</S>\n', '    <S sid=""128"" ssid=""77"">Although the PLTM is clearly not a substitute for a machine translation system&#8212;it has no way to represent syntax or even multi-word phrases&#8212;it is clear from the examples in figure 2 that the sets of high probability words in different languages for a given topic are likely to include translations.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
2,D09-1092,P10-1044,0,"Mimno et al, 2009",0,"Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al, 2009), modeling topic-aligned articles in different languages (Mimno et al, 2009), and word sense induction (Brody and Lapata, 2009)","Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al, 2009), modeling topic-aligned articles in different languages (Mimno et al, 2009), and word sense induction (Brody and Lapata, 2009)","['15', '196', '195', '25', '32']","['    <S sid=""15"" ssid=""11"">Previous work on bilingual topic modeling has focused on machine translation applications, which rely on sentence-aligned parallel translations.</S>\n', '    <S sid=""196"" ssid=""5"">When applied to comparable document collections such as Wikipedia, PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.</S>\n', '    <S sid=""195"" ssid=""4"">Additionally, PLTM can support the creation of bilingual lexica for low resource language pairs, providing candidate translations for more computationally intense alignment processes without the sentence-aligned translations typically used in such tasks.</S>\n', '    <S sid=""25"" ssid=""1"">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing, 2007).</S>\n', '    <S sid=""32"" ssid=""8"">Outside of the field of topic modeling, Kawaba et al. (Kawaba et al., 2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
3,D09-1092,P11-2084,0,"Mimno et al, 2009",0,"(Mimno et al, 2009) retrieve a list of potential translations simply by selecting a small number N of the most probable words in both languages and then add the Cartesian product of these sets for every topic to a set of candidate translations","(Mimno et al, 2009) retrieve a list of potential translations simply by selecting a small number N of the most probable words in both languages and then add the Cartesian product of these sets for every topic to a set of candidate translations","['148', '54', '128', '28', '29']","['    <S sid=""148"" ssid=""97"">To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.</S>\n', '    <S sid=""54"" ssid=""3"">Although direct translations in multiple languages are relatively rare (in contrast with comparable documents), we use direct translations to explore the characteristics of the model.</S>\n', '    <S sid=""128"" ssid=""77"">Although the PLTM is clearly not a substitute for a machine translation system&#8212;it has no way to represent syntax or even multi-word phrases&#8212;it is clear from the examples in figure 2 that the sets of high probability words in different languages for a given topic are likely to include translations.</S>\n', '    <S sid=""28"" ssid=""4"">We take a simpler approach that is more suitable for topically similar document tuples (where documents are not direct translations of one another) in more than two languages.</S>\n', '    <S sid=""29"" ssid=""5"">A recent extended abstract, developed concurrently by Ni et al. (Ni et al., 2009), discusses a multilingual topic model similar to the one presented here.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
4,D09-1092,E12-1014,0,"Mimno et al, 2009",0,"Our Wikipedia-based topic similarity feature, w (f, e), is similar in spirit to polylingualtopic models (Mimno et al 2009), but it is scalable to full bilingual lexicon induction","Our Wikipedia-based topic similarity feature, w (f, e), is similar in spirit to polylingual topic models (Mimno et al 2009), but it is scalable to full bilingual lexicon induction","['131', '15', '25', '77', '105']","['    <S sid=""131"" ssid=""80"">We evaluate sets of high-probability words in each topic and multilingual &#8220;synsets&#8221; by comparing them to entries in human-constructed bilingual dictionaries, as done by Haghighi et al. (2008).</S>\n', '    <S sid=""15"" ssid=""11"">Previous work on bilingual topic modeling has focused on machine translation applications, which rely on sentence-aligned parallel translations.</S>\n', '    <S sid=""25"" ssid=""1"">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing, 2007).</S>\n', '    <S sid=""77"" ssid=""26"">Maximum topic probability in document Although the posterior distribution over topics for each tuple is not concentrated on one topic, it is worth checking that this is not simply because the model is assigning a single topic to the 1We use the R density function. tokens in each of the languages.</S>\n', '    <S sid=""105"" ssid=""54"">An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct, non-comparable documents in multiple languages.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
5,D09-1092,D11-1086,0,"Mimno et al, 2009",0,"of English document and the second half of its aligned foreign language document (Mimno et al,2009)","For Europarl data sets, we artificially make them comparable by considering the first half of English document and the second half of its aligned foreign language document (Mimno et al,2009)","['114', '17', '69', '186', '40']","['    <S sid=""114"" ssid=""63"">Ideally, the &#8220;glue&#8221; documents in g will be sufficient to align the topics across languages, and will cause comparable documents in S to have similar distributions over topics even though they are modeled independently.</S>\n', '    <S sid=""17"" ssid=""13"">We argue that topic modeling is both a useful and appropriate tool for leveraging correspondences between semantically comparable documents in multiple different languages.</S>\n', '    <S sid=""69"" ssid=""18"">English and the Romance languages use only singular and plural versions of &#8220;objective.&#8221; The other Germanic languages include compound words, while Greek and Finnish are dominated by inflected variants of the same lexical item.</S>\n', '    <S sid=""186"" ssid=""20"">Although we find that if Wikipedia contains an article on a particular subject in some language, the article will tend to be topically similar to the articles about that subject in other languages, we also find that across the whole collection different languages emphasize topics to different extents.</S>\n', '    <S sid=""40"" ssid=""6"">Anew document tuple w = (w1, ... , wL) is generated by first drawing a tuple-specific topic distribution from an asymmetric Dirichlet prior with concentration parameter &#945; and base measure m: Then, for each language l, a latent topic assignment is drawn for each token in that language: Finally, the observed tokens are themselves drawn using the language-specific topic parameters: wl &#8764; P(wl  |zl,&#934;l) = 11n &#966;lwl |zl .</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
6,D09-1092,N12-1007,0,"Mimno et al, 2009",0,"Since the PLTM is not a contribution of this paper, we refer the interested reader to (Mimno et al, 2009) for more details","Since the PLTM is not a contribution of this paper, we refer the interested reader to (Mimno et al, 2009) for more details","['29', '138', '170', '111', '127']","['    <S sid=""29"" ssid=""5"">A recent extended abstract, developed concurrently by Ni et al. (Ni et al., 2009), discusses a multilingual topic model similar to the one presented here.</S>\n', '    <S sid=""138"" ssid=""87"">We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.</S>\n', '    <S sid=""170"" ssid=""4"">First, we explore whether comparable document tuples support the alignment of fine-grained topics, as demonstrated earlier using parallel documents.</S>\n', '    <S sid=""111"" ssid=""60"">In order to simulate this scenario we create a set of variations of the EuroParl corpus by treating some documents as if they have no parallel/comparable texts &#8211; i.e., we put each of these documents in a single-document tuple.</S>\n', '    <S sid=""127"" ssid=""76"">One area for future work is to explore whether initialization techniques or better representations of topic co-occurrence might result in alignment of topics with a smaller proportion of comparable texts.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
7,D09-1092,N12-1007,0,2009,0,"Evaluation Corpus The automatic evaluation of cross-lingual co reference systems requires annotated 10Mimno et al (2009) showed that so long as the proportion of topically-aligned to non-aligned documents exceeded 0.25, the topic distributions (as measured by mean Jensen-Shannon Divergence between distributions) did not degrade significantly","Mimno et al (2009) showed that so long as the proportion of topically-aligned to non-aligned documents exceeded 0.25, the topic distributions (as measured by mean Jensen-Shannon Divergence between distributions) did not degrade significantly","['150', '114', '27', '196', '85']","['    <S sid=""150"" ssid=""99"">In finding a lowdimensional semantic representation, topic models deliberately smooth over much of the variation present in language.</S>\n', '    <S sid=""114"" ssid=""63"">Ideally, the &#8220;glue&#8221; documents in g will be sufficient to align the topics across languages, and will cause comparable documents in S to have similar distributions over topics even though they are modeled independently.</S>\n', '    <S sid=""27"" ssid=""3"">Both of these translation-focused topic models infer word-to-word alignments as part of their inference procedures, which would become exponentially more complex if additional languages were added.</S>\n', '    <S sid=""196"" ssid=""5"">When applied to comparable document collections such as Wikipedia, PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.</S>\n', '    <S sid=""85"" ssid=""34"">Smoothed histograms of inter&#8722;language JS divergence A topic model specifies a probability distribution over documents, or in the case of PLTM, document tuples.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
8,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"Similarly, Polylingual Topic Models (PLTM) (Mimno et al, 2009) generalized LDA to tuples of documents from multiple languages","Similarly, Polylingual Topic Models (PLTM) (Mimno et al, 2009) generalized LDA to tuples of documents from multiple languages","['35', '1', '105', '25', '192']","['    <S sid=""35"" ssid=""1"">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.</S>\n', '    <S sid=""1"" ssid=""1"">Topic models are a useful tool for analyzing large text collections, but have previously been applied in only monolingual, or at most bilingual, contexts.</S>\n', '    <S sid=""105"" ssid=""54"">An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct, non-comparable documents in multiple languages.</S>\n', '    <S sid=""25"" ssid=""1"">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing, 2007).</S>\n', '    <S sid=""192"" ssid=""1"">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
9,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"Our baseline joint PLSA model (JPLSA) is closely related to the poly-lingual LDA model of (Mimno et al, 2009)","Our baseline joint PLSA model (JPLSA) is closely related to the poly-lingual LDA model of (Mimno et al, 2009)","['35', '31', '194', '32', '25']","['    <S sid=""35"" ssid=""1"">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.</S>\n', '    <S sid=""31"" ssid=""7"">They also provide little analysis of the differences between polylingual and single-language topic models.</S>\n', '    <S sid=""194"" ssid=""3"">We also demonstrated that relatively small numbers of topically comparable document tuples are sufficient to align topics between languages in non-comparable corpora.</S>\n', '    <S sid=""32"" ssid=""8"">Outside of the field of topic modeling, Kawaba et al. (Kawaba et al., 2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S>\n', '    <S sid=""25"" ssid=""1"">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing, 2007).</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
10,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"We describe the model for two languages, but it is straightforward to generalize to more than two languages, as in (Mimno et al, 2009)","We describe the model for two languages, but it is straightforward to generalize to more than two languages, as in (Mimno et al, 2009)","['170', '6', '40', '17', '171']","['    <S sid=""170"" ssid=""4"">First, we explore whether comparable document tuples support the alignment of fine-grained topics, as demonstrated earlier using parallel documents.</S>\n', '    <S sid=""6"" ssid=""2"">Topic models have been used for analyzing topic trends in research literature (Mann et al., 2006; Hall et al., 2008), inferring captions for images (Blei and Jordan, 2003), social network analysis in email (McCallum et al., 2005), and expanding queries with topically related words in information retrieval (Wei and Croft, 2006).</S>\n', '    <S sid=""40"" ssid=""6"">Anew document tuple w = (w1, ... , wL) is generated by first drawing a tuple-specific topic distribution from an asymmetric Dirichlet prior with concentration parameter &#945; and base measure m: Then, for each language l, a latent topic assignment is drawn for each token in that language: Finally, the observed tokens are themselves drawn using the language-specific topic parameters: wl &#8764; P(wl  |zl,&#934;l) = 11n &#966;lwl |zl .</S>\n', '    <S sid=""17"" ssid=""13"">We argue that topic modeling is both a useful and appropriate tool for leveraging correspondences between semantically comparable documents in multiple different languages.</S>\n', '    <S sid=""171"" ssid=""5"">This property is useful for building machine translation systems as well as for human readers who are either learning new languages or analyzing texts in languages they do not know.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
11,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"j=1 P (z2j|?) P (w 2 j| ?z2j) The difference between the JPLSA model and the poly-lingual topic model of (Mimno et al, 2009) is that we merge the vocabularies in the two languages and learn topic-specific word distributions over these merged vocabularies, instead of having pairs of topic-specific word distributions, one for each language, like in (Mimno et al, 2009)","The difference between the JPLSA model and the poly-lingual topic model of (Mimno et al, 2009) is that we merge the vocabularies in the two languages and learn topic-specific word distributions over these merged vocabularies, instead of having pairs of topic-specific word distributions, one for each language, like in (Mimno et al, 2009)","['85', '15', '196', '109', '150']","['    <S sid=""85"" ssid=""34"">Smoothed histograms of inter&#8722;language JS divergence A topic model specifies a probability distribution over documents, or in the case of PLTM, document tuples.</S>\n', '    <S sid=""15"" ssid=""11"">Previous work on bilingual topic modeling has focused on machine translation applications, which rely on sentence-aligned parallel translations.</S>\n', '    <S sid=""196"" ssid=""5"">When applied to comparable document collections such as Wikipedia, PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.</S>\n', '    <S sid=""109"" ssid=""58"">One simple way to achieve this topic alignment is to add a small set of comparable document tuples that provide sufficient &#8220;glue&#8221; to bind the topics together.</S>\n', '    <S sid=""150"" ssid=""99"">In finding a lowdimensional semantic representation, topic models deliberately smooth over much of the variation present in language.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
12,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"Another difference between our model and the poly-lingual LDA model of (Mimno et al, 2009) is that we use maximum aposteriori (MAP) instead of Bayesian inference","Another difference between our model and the poly-lingual LDA model of (Mimno et al, 2009) is that we use maximum aposteriori (MAP) instead of Bayesian inference","['29', '150', '27', '170', '17']","['    <S sid=""29"" ssid=""5"">A recent extended abstract, developed concurrently by Ni et al. (Ni et al., 2009), discusses a multilingual topic model similar to the one presented here.</S>\n', '    <S sid=""150"" ssid=""99"">In finding a lowdimensional semantic representation, topic models deliberately smooth over much of the variation present in language.</S>\n', '    <S sid=""27"" ssid=""3"">Both of these translation-focused topic models infer word-to-word alignments as part of their inference procedures, which would become exponentially more complex if additional languages were added.</S>\n', '    <S sid=""170"" ssid=""4"">First, we explore whether comparable document tuples support the alignment of fine-grained topics, as demonstrated earlier using parallel documents.</S>\n', '    <S sid=""17"" ssid=""13"">We argue that topic modeling is both a useful and appropriate tool for leveraging correspondences between semantically comparable documents in multiple different languages.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
13,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"For computing distance we used the L1-norm of the difference, which worked a bit better than the Jensen Shannon divergence between the topic vectors used in (Mimno et al, 2009)","For computing distance we used the L1-norm of the difference, which worked a bit better than the Jensen Shannon divergence between the topic vectors used in (Mimno et al, 2009)","['29', '182', '109', '163', '120']","['    <S sid=""29"" ssid=""5"">A recent extended abstract, developed concurrently by Ni et al. (Ni et al., 2009), discusses a multilingual topic model similar to the one presented here.</S>\n', '    <S sid=""182"" ssid=""16"">As with EuroParl, we can calculate the JensenShannon divergence between pairs of documents within a comparable document tuple.</S>\n', '    <S sid=""109"" ssid=""58"">One simple way to achieve this topic alignment is to add a small set of comparable document tuples that provide sufficient &#8220;glue&#8221; to bind the topics together.</S>\n', '    <S sid=""163"" ssid=""112"">Performance continues to improve with longer documents, most likely due to better topic inference.</S>\n', '    <S sid=""120"" ssid=""69"">From the results in figure 4, we know that leaving all document tuples intact should result in a mean JS divergence of less than 0.1.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
15,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"In previously reported work, (Mimno et al, 2009) evaluate parallel document retrieval using PLTM on Europarl speeches in English and Spanish, using training and test sets of size similar to ours","In previously reported work, (Mimno et al, 2009) evaluate parallel document retrieval using PLTM on Europarl speeches in English and Spanish, using training and test sets of size similar to ours","['129', '196', '10', '128', '51']","['    <S sid=""129"" ssid=""78"">We therefore evaluate the ability of the PLTM to generate bilingual lexica, similar to other work in unsupervised translation modeling (Haghighi et al., 2008).</S>\n', '    <S sid=""196"" ssid=""5"">When applied to comparable document collections such as Wikipedia, PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.</S>\n', '    <S sid=""10"" ssid=""6"">We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).</S>\n', '    <S sid=""128"" ssid=""77"">Although the PLTM is clearly not a substitute for a machine translation system&#8212;it has no way to represent syntax or even multi-word phrases&#8212;it is clear from the examples in figure 2 that the sets of high probability words in different languages for a given topic are likely to include translations.</S>\n', '    <S sid=""51"" ssid=""17"">Gibbs sampling involves sequentially resampling each zln from its conditional posterior: where z\\l,n is the current set of topic assignments for all other tokens in the tuple, while (Nt)\\l,n is the number of occurrences of topic t in the tuple, excluding zln, the variable being resampled.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
16,D09-1092,W12-3117,0,"Mimno et al, 2009",0,"We assume that a higher dimensionality can lead to a better repartitioning of the vocabulary over the topics. Multilingual LDA has been used before in natural language processing ,e.g .polylingual topic models (Mimno et al, 2009) or multilingual topic models for unaligned text (Boyd-Graber and Blei, 2009)","Multilingual LDA has been used before in natural language processing, e.g. polylingual topic models (Mimno et al, 2009) or multilingual topic models for unaligned text (Boyd-Graber and Blei, 2009)","['35', '25', '1', '192', '3']","['    <S sid=""35"" ssid=""1"">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.</S>\n', '    <S sid=""25"" ssid=""1"">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing, 2007).</S>\n', '    <S sid=""1"" ssid=""1"">Topic models are a useful tool for analyzing large text collections, but have previously been applied in only monolingual, or at most bilingual, contexts.</S>\n', '    <S sid=""192"" ssid=""1"">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>\n', '    <S sid=""3"" ssid=""3"">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
17,D09-1092,W11-2133,0,2009,0,"ji =wjk? M j i? m k=1w j k, (1) where M j is the topic distribution of document j and wk is the number of occurrences of phrase pair Xk in document j. Mimno et al (2009) extend the original concept of LDA to support polylingual topic models (PLTM), both on parallel (such as EuroParl) and partly comparable documents (such as Wikipediaarticles)","Mimno et al (2009) extend the original concept of LDA to support polylingual topic models (PLTM), both on parallel (such as EuroParl) and partly comparable documents (such as Wikipedia articles)","['105', '35', '73', '9', '194']","['    <S sid=""105"" ssid=""54"">An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct, non-comparable documents in multiple languages.</S>\n', '    <S sid=""35"" ssid=""1"">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.</S>\n', '    <S sid=""73"" ssid=""22"">If the model assigns all tokens in a tuple to a single topic, the maximum posterior topic probability for that tuple will be near to 1.0.</S>\n', '    <S sid=""9"" ssid=""5"">In this paper, we present the polylingual topic model (PLTM).</S>\n', '    <S sid=""194"" ssid=""3"">We also demonstrated that relatively small numbers of topically comparable document tuples are sufficient to align topics between languages in non-comparable corpora.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
18,D09-1092,W11-2133,0,2009,0,Tuple-specific topic distributions arelearned using LDA with distinct topic-word concentration parameters? l. Mimno et al (2009) show that PLTM sufficiently aligns topics in parallel corpora,Mimno et al (2009) show that PLTM sufficiently aligns topics in parallel corpora,"['194', '105', '99', '192', '6']","['    <S sid=""194"" ssid=""3"">We also demonstrated that relatively small numbers of topically comparable document tuples are sufficient to align topics between languages in non-comparable corpora.</S>\n', '    <S sid=""105"" ssid=""54"">An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct, non-comparable documents in multiple languages.</S>\n', '    <S sid=""99"" ssid=""48"">Figure 5 shows the proportion of all tokens in English and Finnish assigned to each topic under LDA and PLTM with 800 topics.</S>\n', '    <S sid=""192"" ssid=""1"">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>\n', '    <S sid=""6"" ssid=""2"">Topic models have been used for analyzing topic trends in research literature (Mann et al., 2006; Hall et al., 2008), inferring captions for images (Blei and Jordan, 2003), social network analysis in email (McCallum et al., 2005), and expanding queries with topically related words in information retrieval (Wei and Croft, 2006).</S>\n']","['Result_Citation', 'Result_Citation', 'Hypothesis_Citation', 'Result_Citation', 'Result_Citation']"
19,D09-1092,P14-2110,0,"Mimno et al, 2009",0,"A good candidate for multilingual topic analyses are polylin gual topic models (Mimno et al, 2009), which learn topics for multiple languages, creating tuples of language specific distributions over monolingual vocabularies for each topic","A good candidate for multilingual topic analyses are polylingual topic models (Mimno et al, 2009), which learn topics for multiple languages, creating tuples of language specific distributions over monolingual vocabularies for each topic","['192', '3', '105', '1', '131']","['    <S sid=""192"" ssid=""1"">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>\n', '    <S sid=""3"" ssid=""3"">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S>\n', '    <S sid=""105"" ssid=""54"">An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct, non-comparable documents in multiple languages.</S>\n', '    <S sid=""1"" ssid=""1"">Topic models are a useful tool for analyzing large text collections, but have previously been applied in only monolingual, or at most bilingual, contexts.</S>\n', '    <S sid=""131"" ssid=""80"">We evaluate sets of high-probability words in each topic and multilingual &#8220;synsets&#8221; by comparing them to entries in human-constructed bilingual dictionaries, as done by Haghighi et al. (2008).</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
20,D09-1092,P14-2110,0,2009,0,"3csLDATo train a polylingual topic model on social media, we make two modifications to the model of Mimno et al (2009): add a token specific language variable, and a process for identifying aligned top ics. First ,polylingual topic models require parallel or comparable corpora in which each document has an assigned language","To train a polylingual topic model on social media, we make two modifications to the model of Mimno et al (2009): add a token specific language variable, and a process for identifying aligned top ics","['65', '114', '193', '15', '77']","['    <S sid=""65"" ssid=""14"">This topic provides an illustration of the variation in technical terminology captured by PLTM, including the wide array of acronyms used by different languages.</S>\n', '    <S sid=""114"" ssid=""63"">Ideally, the &#8220;glue&#8221; documents in g will be sufficient to align the topics across languages, and will cause comparable documents in S to have similar distributions over topics even though they are modeled independently.</S>\n', '    <S sid=""193"" ssid=""2"">We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.</S>\n', '    <S sid=""15"" ssid=""11"">Previous work on bilingual topic modeling has focused on machine translation applications, which rely on sentence-aligned parallel translations.</S>\n', '    <S sid=""77"" ssid=""26"">Maximum topic probability in document Although the posterior distribution over topics for each tuple is not concentrated on one topic, it is worth checking that this is not simply because the model is assigning a single topic to the 1We use the R density function. tokens in each of the languages.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
