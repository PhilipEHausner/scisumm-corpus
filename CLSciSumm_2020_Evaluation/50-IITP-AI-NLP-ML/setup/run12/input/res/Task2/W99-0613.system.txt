adaboost finds a weighted combination of simple weak classifiers where the weights are chosen to minimize a function that bounds the classification error on a set of training examples 
adaboost is given access to a weak learning algorithm which accepts as input the training examples along with a distribution over the instances 
the approach builds from an initial seed set for a category and is quite similar to the decision list approach described in yarowsky 
in addition to a heuristic based on decision list learning we also presented a boosting like framework that builds on ideas from blum and mitchell 
the task can be considered to be one component of the muc muc named entity task the other task is that of segmentation i e pulling possible people places and locations from text before sending them to the classifier 
formally let el be the number of classification errors of the first second learner on the training data and let eco be the number of unlabeled examples on which the two classifiers disagree 
before describing the unsupervised case we first describe the supervised version of the algorithm input to the learning algorithm n labeled examples of the form xi y y is the label of the ith example given that there are k possible labels y is a member of y xi is a set of mi features x xi 
several extensions of adaboost for multiclass problems have been suggested freund and schapire schapire and singer 
 sentences of new york times text were parsed using the parser of collins word sequences that met the following criteria were then extracted as named entity examples whose head is a singular noun tagged nn 
