in the previous sections we have concentrated on the relation of the parser to a maximumentropy approach the aspect of the parser that is most novel 
but let us look at how it works for a particular case in our parsing scheme 
that the previous three best parsers on this test all perform within a percentage point of each other despite quite different basic mechanisms led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training and to conjecture that perhaps we were at it 
for example in the penn treebank a vp with both main and auxiliary verbs has the structure shown in figure 
in particular we measure labeled precision lp and recall lr average number of crossbrackets per sentence cb percentage of sentences with zero cross brackets ocb and percentage of sentences with cross brackets cb 
we created a parser based upon the maximumentropy inspired model of the last section smoothed using standard deleted interpolation 
when we do so using our maximum entropy inspired conditioning we get another improvement in average precision recall as indicated in figure on the line labeled quot maxent inspired 
thus an expansion e c looks like the expansion is generated by guessing first m then in order l through l a and similarly for ri through in a pure markov pcfg we are given the left hand side label and then probabilistically generate the right hand side conditioning on no information other than and possibly previously generated pieces of the right hand side itself 
