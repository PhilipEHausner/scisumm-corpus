we proceedings of the th conference on computational natural language learning conll x pages new york city june c association for computational linguistics assume that all dependency graphs are trees but may be non projective both of which are true in the data sets we use 
we use the mira online learner to set the weights crammer and singer mcdonald et al a since we found it trained quickly and provide good performance 
the second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph 
for instance the system of mcdonald et al a incorporates features over the part of speech of words occurring between and around a possible head dependent relation 
these results show that the discriminative spanning tree parsing framework mcdonald et al b mcdonald and pereira is easily adapted across all these languages 
this interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non projectivity in freer word order languages 
furthermore for arabic and spanish we used lemmas instead of inflected word forms again based on performance on held out data 
however recently their has been a revived interest in parsing models that produce dependency graph representations of sentences which model words and their arguments through directed edges hudson mel cuk 
for instance if we consider a head xi with dependents xj xjm it is often the case that many of these dependencies will have correlated labels 
