also the parser returns a set of candidate parses from which we have been choosing the top ranked if we use an oracle to choose the parse with the highest accuracy from among the candidates which averaged in number per sentence we find an average labeled precision recall of for sentences of length 
in addition we show the average number of rule expansions considered per word that is the number of rule expansions for which a probability was calculated see roark and charniak and the average number of analyses advanced to the next priority queue per word 
a lexicalized probabilistic topdown parser is then presented which performs very well in terms of both the accuracy of returned parses and the efficiency with which they are found relative to the best broad coverage statistical parsers 
the parser error parser coverage and the uninterpolated model perplexity a all suffered substantially from a narrower search but the interpolated perplexity remained quite good even at the extremes 
this contrasts with our perplexity results reported above as well as with the recognition experiments in chelba where the best results resulted from interpolated models 
he was able to reduce both sentence and word error rates on the atis corpus using this method 
a left toright parser whose derivations are not rooted i e with derivations that can consist of disconnected tree fragments such as an lr or shift reduce parser cannot incrementally calculate the probability of each prefix string being generated by the probabilistic grammar because their derivations include probability mass from unrooted structures 
