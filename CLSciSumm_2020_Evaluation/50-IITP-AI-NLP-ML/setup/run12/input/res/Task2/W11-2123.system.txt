for the perplexity and translation tasks we used srilm to build a gram english language model on million tokens from europarl v koehn and the workshop on machine translation news crawl corpus with duplicate lines removed 
for even larger models storing counts talbot and osborne pauls and klein guthrie and hepple is a possibility 
it is generally considered to be fast pauls probabilities and non zero backoffs and klein with a default implementation based on hash tables within each trie node 
performance improvements transfer to the moses koehn et al cdec dyer et al and joshua li et al translation systems where our code has been integrated 
lossy compressed models randlm talbot and osborne and sheffield guthrie and hepple offer better memory consumption at the expense of cpu and accuracy 
time for moses itself to load including loading the language model and phrase table is included 
the probing model was designed to improve upon srilm by using linear probing hash tables though not arranged in a trie allocating memory all at once eliminating the need for full pointers and being easy to compile 
for example syntactic decoders koehn et al dyer et al li et al perform dynamic programming parametrized by both backward and forward looking state 
irstlm federico et al is a sorted trie implementation designed for lower memory consumption 
sheffield guthrie and hepple explore several randomized compression techniques but did not release code 
with a good hash function collisions of the full bit hash are exceedingly rare one in billion queries for our baseline model will falsely find a key not present 
