through parser combination we have reduced the precision error rate by and the recall error rate by compared to the best previously published result 
the bayes models were able to achieve significantly higher precision than their non parametric counterparts 
furthermore we know one of the original parses will be the hypothesized parse so the direct method of determining which one is best is to compute the probability of each of the candidate parses using the probabilistic model we developed in section 
it is the performance we could achieve if an omniscient observer told us which parser to pick for each of the sentences 
the probabilistic version of this procedure is straightforward we once again assume independence among our various member parsers 
the precision and recall measures described in more detail in section used in evaluating treebank parsing treat each constituent as a separate entity a minimal unit of correctness 
the maximum precision oracle is an upper bound on the possible gain we can achieve by parse hybridization 
the combining algorithm is presented with the candidate parses and asked to choose which one is best 
the development of a na ve bayes classifier involves learning how much each parser should be trusted for the decisions it makes 
in each figure the upper graph shows the isolated constituent precision and the bottom graph shows the corresponding number of hypothesized constituents 
their theoretical finding is simply stated classification error rate decreases toward the noise rate exponentially in the number of independent accurate classifiers 
