the bayes models were able to achieve significantly higher precision than their non-parametric counterparts.
these two principles guide experimentation in this framework, and together with the evaluation measures help us decide which specific type of substructure to combine.
for example, one parser could be more accurate at predicting noun phrases than the other parsers.
we used these three parsers to explore parser combination techniques.
we then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.
the standard measures for evaluating penn treebank parsing performance are precision and recall of the predicted constituents.
finally we show the combining techniques degrade very little when a poor parser is added to the set.
the counts represent portions of the approximately 44000 constituents hypothesized by the parsers in the development set.
we plan to explore more powerful techniques for exploiting the diversity of parsing methods.
in each figure the upper graph shows the isolated constituent precision and the bottom graph shows the corresponding number of hypothesized constituents.
our original hope in combining these parsers is that their errors are independently distributed.
we have presented two general approaches to studying parser combination: parser switching and parse hybridization.
in the cases where isolated constituent precision is larger than 0.5 the affected portion of the hypotheses is negligible.
in equations 1 through 3 we develop the model for constructing our parse using na√Øve bayes classification.
we show the results of three of the experiments we conducted to measure isolated constituent precision under various partitioning schemes.
when this metric is less than 0.5, we expect to incur more errors' than we will remove by adding those constituents to the parse.
