we employed leave-one-out resampling (weiss and kulikowski, 1991), by correlating the data obtained from each participant with the ratings obtained from all other participants.
in this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.
computational models of semantics which use symbolic logic representations (montague, 1974) can account naturally for the meaning of phrases or sentences.
our results show that the multiplicative models are superior and correlate significantly with behavioral data.
the appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar (harris, 1968).
experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments.
moreover, the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (mcdonald, 2000) and word association norms (denhire and lemaire, 2004).
despite the popularity of additive models, our experimental results showed the superiority of models utilizing multiplicative combinations, at least for the sentence similarity task attempted here.
kintsch (2001) proposes a variation on the vector addition theme in an attempt to model how the meaning of a predicate (e.g., run) varies depending on the arguments it operates upon (e.g, the horse ran vs. the color ran).
although we have presented multiplicative and additive models separately, there is nothing inherent in our formulation that disallows their combination.
to give a concrete example, circular convolution is an instance of the general multiplicative model which breaks this constraint by allowing uj to contribute to pi: for example, according to (5), the addition of the two vectors representing horse and run in figure 1 would yield horse + run = [1 14 6 14 4].
