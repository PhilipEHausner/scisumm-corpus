performance improvements transfer to the moses (koehn et al., 2007), cdec (dyer et al., 2010), and joshua (li et al., 2009) translation systems where our code has been integrated.
the cost of storing these averages, in bits, is because there are comparatively few unigrams, we elected to store them byte-aligned and unquantized, making every query faster.
unlike germann et al. (2009), we chose a model size so that all benchmarks fit comfortably in main memory.
this is most severe with randlm in the multi-threaded case, where each thread keeps a separate cache, exceeding the original model size.
time for moses itself to load, including loading the language model and phrase table, is included.
this has the effect of randomly permuting vocabulary identifiers, meeting the requirements of interpolation search when vocabulary identifiers are used as keys.
irstlm 5.60.02 (federico et al., 2008) is a sorted trie implementation designed for lower memory consumption.
while sorted arrays could be used to implement the same data structure as probing, effectively making m = 1, we abandoned this implementation because it is slower and larger than a trie implementation.
compared with the widely- srilm, our is 2.4 times as fast while using 57% of the mem- the structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less cpu than the baseline.
for speed, we plan to implement the direct-mapped cache from berkeleylm.
for randlm and irstlm, the effect of caching can be seen on speed and memory usage.
