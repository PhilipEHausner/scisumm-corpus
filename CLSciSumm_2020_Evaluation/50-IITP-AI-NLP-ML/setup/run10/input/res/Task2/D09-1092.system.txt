topic models have been used for analyzing topic trends in research literature (mann et al., 2006; hall et al., 2008), inferring captions for images (blei and jordan, 2003), social network analysis in email (mccallum et al., 2005), and expanding queries with topically related words in information retrieval (wei and croft, 2006).
in finding a lowdimensional semantic representation, topic models deliberately smooth over much of the variation present in language.
we take a simpler approach that is more suitable for topically similar document tuples (where documents are not direct translations of one another) in more than two languages.
outside of the field of topic modeling, kawaba et al. (kawaba et al., 2008) use a wikipedia-based model to perform sentiment analysis of blog posts.
we analyzed the characteristics of pltm in comparison to monolingual lda, and demonstrated that it is possible to discover aligned topics.
we also demonstrated that relatively small numbers of topically comparable document tuples are sufficient to align topics between languages in non-comparable corpora.
a recent extended abstract, developed concurrently by ni et al. (ni et al., 2009), discusses a multilingual topic model similar to the one presented here.
in this paper, we present the polylingual topic model (pltm).
an important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct, non-comparable documents in multiple languages.
if the model assigns all tokens in a tuple to a single topic, the maximum posterior topic probability for that tuple will be near to 1.0.
we argue that topic modeling is both a useful and appropriate tool for leveraging correspondences between semantically comparable documents in multiple different languages.
