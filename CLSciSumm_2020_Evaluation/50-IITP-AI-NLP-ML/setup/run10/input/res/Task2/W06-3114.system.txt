following this method, we repeatedly — say, 1000 times — sample sets of sentences from the output of each system, measure their bleu score, and use these 1000 bleu scores as basis for estimating a confidence interval.
let say, if we find one system doing better on 20 of the blocks, and worse on 80 of the blocks, is it significantly worse?
however, a recent study (callison-burch et al., 2006), pointed out that this correlation may not always be strong.
sentences and systems were randomly selected and randomly shuffled for presentation.
see figure 3 for a screenshot of the evaluation tool.
in addition to the europarl test set, we also collected 29 editorials from the project syndicate website2, which are published in all the four languages of the shared task.
due to many similarly performing systems, we are not able to draw strong conclusions on the question of correlation of manual and automatic evaluation metrics.
we confirm the finding by callison-burch et al. (2006) that the rule-based system of systran is not adequately appreciated by bleu.
also, the argument has been made that machine translation performance should be evaluated via task-based evaluation metrics, i.e. how much it assists performing a useful task, such as supporting human translators or aiding the analysis of texts.
however, ince we extracted the test corpus automatically from web sources, the reference translation was not always accurate — due to sentence alignment errors, or because translators did not adhere to a strict sentence-by-sentence translation (say, using pronouns when referring to entities mentioned in the previous sentence).
