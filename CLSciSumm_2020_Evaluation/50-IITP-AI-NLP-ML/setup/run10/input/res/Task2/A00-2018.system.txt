for example, in the penn treebank a vp with both main and auxiliary verbs has the structure shown in figure 3.
that the previous three best parsers on this test [5,9,17] all perform within a percentage point of each other, despite quite different basic mechanisms, led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training, and to conjecture that perhaps we were at it.
in particular, we measure labeled precision (lp) and recall (lr), average number of crossbrackets per sentence (cb), percentage of sentences with zero cross brackets (ocb), and percentage of sentences with < 2 cross brackets (2cb).
so, for example, in a second-order markov pcfg, l2 would be conditioned on l1 and m. in our complete model, of course, the probability of each label in the expansions is also conditioned on other material as specified in equation 1, e.g., p(e t, h, h).
the results of [13] achieved by combining the aforementioned three-best parsers also suggest that the limit on tree-bank trained parsers is much higher than previously thought.
looking in particular at the precision and recall figures, the new parser's give us a 13% error reduction over the best of the previous work, co1199 [9].
now consider computing a conditional probability p(a h) with a set of features h that connect a to the history h. in a log-linear model the probability function takes the following form: here the ai are weights between negative and positive infinity that indicate the relative importance of a feature: the more relevant the feature to the value of the probability, the higher the absolute value of the associated a.
