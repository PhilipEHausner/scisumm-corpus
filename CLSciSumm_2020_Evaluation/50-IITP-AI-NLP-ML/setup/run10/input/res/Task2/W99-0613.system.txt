adaboost finds a weighted combination of simple (weak) classifiers, where the weights are chosen to minimize a function that bounds the classification error on a set of training examples.
recent results (e.g., (yarowsky 95; brill 95; blum and mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.
the following features were used: full-string=x the full string (e.g., for maury cooper, full- s tring=maury_cooper). contains(x) if the spelling contains more than one word, this feature applies for any words that the string contains (e.g., maury cooper contributes two such features, contains (maury) and contains (cooper) . allcapl this feature appears if the spelling is a single word which is all capitals (e.g., ibm would contribute this feature). allcap2 this feature appears if the spelling is a single word which is all capitals or full periods, and contains at least one period.
in addition to a heuristic based on decision list learning, we also presented a boosting-like framework that builds on ideas from (blum and mitchell 98).
the first method builds on results from (yarowsky 95) and (blum and mitchell 98).
a spelling rule might be a simple look-up for the string (e.g., a rule that honduras is a location) or a rule that looks at words within a string (e.g., a rule that any string containing mr. is a person).
adaboost is given access to a weak learning algorithm, which accepts as input the training examples, along with a distribution over the instances.
the 2(yarowsky 95) describes the use of more sophisticated smoothing methods.
