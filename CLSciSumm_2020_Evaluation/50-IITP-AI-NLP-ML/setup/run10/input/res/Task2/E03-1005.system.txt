in bod (2000b), an alternative notion for the best parse tree was proposed based on a simplicity criterion: instead of producing the most probable tree, this model produced the tree generated by the shortest derivation with the fewest training subtrees.
compared to the reranking technique in collins (2000), who obtained an lp of 89.9% and an lr of 89.6%, our results show a 9% relative error rate reduction.
as to the processing time, the pcfg reduction parses each sentence 100 words) in 3.6 seconds average, while the parser in bod (2001, 2003), which uses over 5 million subtrees, is reported to take about 220 seconds per sentence.
recall that for n=1, sl-dop is equal to the pcfg-reduction of bod (2001) (which we also called likelihood-dop) while ls-dop is equal to simplicity-dop.
in the following section first results of sl-dop and ls-dop with a compact pcfg-reduction. we will see that our new definition of best parse tree also outperforms the best results obtained in bod (2001).
collins & duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with dop1's subtrees, reporting a 5.1% relative reduction in error rate over the model in collins (1999) on the wsj.
for example, bod (2001) samples a fixed number of subtrees of each depth, which has the effect of assigning roughly equal weight to each node in the training data, and roughly exponentially less probability for larger trees (see goodman 2002: 12).
together with a pcfgreduction of dop we obtain improved accuracy and efficiency on the wall street journal treebank our results show an 11% relative reduction in error rate over previous models, and an average processing time of 3.6 seconds per wsj sentence.
