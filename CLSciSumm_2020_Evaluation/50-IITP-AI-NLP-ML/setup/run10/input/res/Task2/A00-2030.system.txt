in this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (lpcfg-hr) to information extraction.
we can think of this prior probability as an estimate of the probability of generating a subtree with the constituent category, starting at the topmost node.
the probability of generating a constituent of the specified category, starting at the topmost node.
since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the upenn treebank as a gold standard.
instead, our parsing algorithm, trained on the upenn treebank, was run on the new york times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.
our system for muc-7 consisted of the sentential model described in this paper, coupled with a simple probability model for cross-sentence merging.
instead, we applied an information retrieval system to select a large number of articles from the desired sources, yielding a corpus rich in the desired types of events.
the detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.
currently, the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: since we were interested in exploiting recent advances in parsing, replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.
we were already using a generative statistical model for part-of-speech tagging (weischedel et al. 1993), and more recently, had begun using a generative statistical model for name finding (bikel et al.
