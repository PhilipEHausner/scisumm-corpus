on the penn chinese treebank 5.0, we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline.
as all the sub-models, including the perceptron, are regarded as separate features of the outside-layer linear model, we can train them respectively with special algorithms.
another widely used discriminative method is the perceptron algorithm (collins, 2002), which achieves comparable performance to crfs with much faster training, so we base this work on the perceptron.
besides this perceptron, other sub-models are trained and used as additional features of the outside-layer linear model.
we used sri language modelling toolkit (stolcke and andreas, 2002) to train a 3gram word lm with modified kneser-ney smoothing (chen and goodman, 1998), and a 4-gram pos lm with witten-bell smoothing, and we trained a word-pos co-occurrence model simply by mle without smoothing.
as shown in figure 1, the character-based perceptron is used as the inside-layer linear model and sends its output to the outside-layer.
according to ng and low (2004), the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: we can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.
test results listed in table 2 shows that this model obtains higher accuracy than the best of sighan bakeoff 2 in three corpora (as, cityu and msr).
figure 3 shows their learning curves depicting the f-measure on the development set after 1 to 10 training iterations.
