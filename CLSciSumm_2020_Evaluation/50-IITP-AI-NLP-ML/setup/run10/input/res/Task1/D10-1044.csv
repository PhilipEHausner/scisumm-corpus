Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,D10-1044,P11-2074,0,"Foster et al, 2010",0,"Another popular task in SMT is domain adaptation (Foster et al, 2010)","Another popular task in SMT is domain adaptation (Foster et al, 2010)","['10', '122', '136', '141', '111']","['    <S sid=""10"" ssid=""7"">This is a standard adaptation problem for SMT.</S>\n', '    <S sid=""122"" ssid=""26"">The linear LM (lin lm), TM (lin tm) and MAP TM (map tm) used with non-adapted counterparts perform in all cases slightly worse than the log-linear combination, which adapts both LM and TM components.</S>\n', '    <S sid=""136"" ssid=""5"">It is also worth pointing out a connection with Daum&#180;e&#8217;s (2007) work that splits each feature into domain-specific and general copies.</S>\n', '    <S sid=""141"" ssid=""10"">Moving beyond directly related work, major themes in SMT adaptation include the IR (Hildebrand et al., 2005; L&#168;u et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L&#168;u et al., 2007) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (Bertoldi and Federico, 2009; Ueffing et al., 2007; Schwenk and Senellart, 2009).</S>\n', '    <S sid=""111"" ssid=""15"">We used a standard one-pass phrase-based system (Koehn et al., 2003), with the following features: relative-frequency TM probabilities in both directions; a 4-gram LM with Kneser-Ney smoothing; word-displacement distortion model; and word count.</S>\n']","['Method_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
2,D10-1044,P12-1048,0,"Foster et al, 2010",0,"In addition, discriminative weighting methods were proposed to assign appropriate weights to the sentences from training corpus (Matsoukas et al, 2009) or the phrase pairs of phrase table (Foster et al, 2010)","In addition, discriminative weighting methods were proposed to assign appropriate weights to the sentences from training corpus (Matsoukas et al, 2009) or the phrase pairs of phrase table (Foster et al, 2010)","['149', '80', '151', '95', '98']","['    <S sid=""149"" ssid=""6"">We obtained positive results using a very simple phrase-based system in two different adaptation settings: using English/French Europarl to improve a performance on a small, specialized medical domain; and using non-news portions of the NIST09 training material to improve performance on the news-related corpora.</S>\n', '    <S sid=""80"" ssid=""17"">We model po(s|t) using a MAP criterion over weighted phrase-pair counts: and from the similarity to (5), assuming y = 0, we see that w&#955;(s, t) can be interpreted as approximating pf(s, t)/po(s, t).</S>\n', '    <S sid=""151"" ssid=""8"">In future work we plan to try this approach with more competitive SMT systems, and to extend instance weighting to other standard SMT components such as the LM, lexical phrase weights, and lexicalized distortion.</S>\n', '    <S sid=""95"" ssid=""32"">Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models), and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples, with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.</S>\n', '    <S sid=""98"" ssid=""2"">The first setting uses the European Medicines Agency (EMEA) corpus (Tiedemann, 2009) as IN, and the Europarl (EP) corpus (www.statmt.org/europarl) as OUT, for English/French translation in both directions.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
3,D10-1044,D12-1129,0,"Foster et al., 2010",0,"Domain knowledge also has the potential to improve open-text applications such as summarization (Ceylan et al 2010) and machine translation (Foster et al., 2010) .Research in Word Sense Disambiguation (Navigli, 2009, WSD), the task aimed at the automatic labeling of text with word senses, has been oriented towards domain text understanding for several years now","Domain knowledge also has the potential to improve open-text applications such as summarization (Ceylan et al 2010) and machine translation (Foster et al., 2010)","['138', '141', '151', '136', '7']","['    <S sid=""138"" ssid=""7"">However, for multinomial models like our LMs and TMs, there is a one to one correspondence between instances and features, eg the correspondence between a phrase pair (s, t) and its conditional multinomial probability p(s1t).</S>\n', '    <S sid=""141"" ssid=""10"">Moving beyond directly related work, major themes in SMT adaptation include the IR (Hildebrand et al., 2005; L&#168;u et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L&#168;u et al., 2007) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (Bertoldi and Federico, 2009; Ueffing et al., 2007; Schwenk and Senellart, 2009).</S>\n', '    <S sid=""151"" ssid=""8"">In future work we plan to try this approach with more competitive SMT systems, and to extend instance weighting to other standard SMT components such as the LM, lexical phrase weights, and lexicalized distortion.</S>\n', '    <S sid=""136"" ssid=""5"">It is also worth pointing out a connection with Daum&#180;e&#8217;s (2007) work that splits each feature into domain-specific and general copies.</S>\n', '    <S sid=""7"" ssid=""4"">For developers of Statistical Machine Translation (SMT) systems, an additional complication is the heterogeneous nature of SMT components (word-alignment model, language model, translation model, etc.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
4,D10-1044,P14-2093,0,2010,0,"Yasuda et al (2008) and Foster et al (2010) ranked the sentence pairs in the general-domain corpus according to the perplexity scores of sentences, which are computed with respect to in-domain language models","Yasuda et al (2008) and Foster et al (2010) ranked the sentence pairs in the general-domain corpus according to the perplexity scores of sentences, which are computed with respect to in-domain language models","['98', '119', '31', '20', '111']","['    <S sid=""98"" ssid=""2"">The first setting uses the European Medicines Agency (EMEA) corpus (Tiedemann, 2009) as IN, and the Europarl (EP) corpus (www.statmt.org/europarl) as OUT, for English/French translation in both directions.</S>\n', '    <S sid=""119"" ssid=""23"">The 2nd block contains the IR system, which was tuned by selecting text in multiples of the size of the EMEA training corpus, according to dev set performance.</S>\n', '    <S sid=""31"" ssid=""28"">For comparison to information-retrieval inspired baselines, eg (L&#168;u et al., 2007), we select sentences from OUT using language model perplexities from IN.</S>\n', '    <S sid=""20"" ssid=""17"">Daum&#180;e (2007) applies a related idea in a simpler way, by splitting features into general and domain-specific versions.</S>\n', '    <S sid=""111"" ssid=""15"">We used a standard one-pass phrase-based system (Koehn et al., 2003), with the following features: relative-frequency TM probabilities in both directions; a 4-gram LM with Kneser-Ney smoothing; word-displacement distortion model; and word count.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
5,D10-1044,E12-1055,0,2010,0,"However, such confounding factors do not affect the optimization algorithm, which works with a fixed set of phrase pairs, and merely varies? .Our main technical contributions are as fol lows: Additionally to perplexity optimization for linear interpolation, which was first applied by Foster et al (2010), we propose perplexity optimization for weighted counts (equation 3), and a modified implementation of linear interpolation. Also, we independently perform perplexity minimization for all four features of the standard SMTtranslation model: the phrase translation probabilities p (t|s) and p (s|t), and the lexical weights lex (t|s) and lex (s|t)","Our main technical contributions are as follows: Additionally to perplexity optimization for linear interpolation, which was first applied by Foster et al (2010), we propose perplexity optimization for weighted counts (equation 3), and a modified implementation of linear interpolation","['71', '54', '28', '50', '44']","['    <S sid=""71"" ssid=""8"">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where c&#955;(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>\n', '    <S sid=""54"" ssid=""18"">However, we note that the final conditional estimates p(s|t) from a given phrase table maximize the likelihood of joint empirical phrase pair counts over a word-aligned corpus.</S>\n', '    <S sid=""28"" ssid=""25"">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S>\n', '    <S sid=""50"" ssid=""14"">Linear weights are difficult to incorporate into the standard MERT procedure because they are &#8220;hidden&#8221; within a top-level probability that represents the linear combination.1 Following previous work (Foster and Kuhn, 2007), we circumvent this problem by choosing weights to optimize corpus loglikelihood, which is roughly speaking the training criterion used by the LM and TM themselves.</S>\n', '    <S sid=""44"" ssid=""8"">When OUT is large and distinct, its contribution can be controlled by training separate IN and OUT models, and weighting their combination.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
6,D10-1044,E12-1055,0,2010,0,"Matsoukas et al (2009) propose an approach where each sentence is weighted according to a classifier, and Foster et al (2010) ex tend this approach by weighting individual phrase pairs","Matsoukas et al (2009) propose an approach where each sentence is weighted according to a classifier, and Foster et al (2010) extend this approach by weighting individual phrase pairs","['136', '98', '21', '149', '59']","['    <S sid=""136"" ssid=""5"">It is also worth pointing out a connection with Daum&#180;e&#8217;s (2007) work that splits each feature into domain-specific and general copies.</S>\n', '    <S sid=""98"" ssid=""2"">The first setting uses the European Medicines Agency (EMEA) corpus (Tiedemann, 2009) as IN, and the Europarl (EP) corpus (www.statmt.org/europarl) as OUT, for English/French translation in both directions.</S>\n', '    <S sid=""21"" ssid=""18"">This highly effective approach is not directly applicable to the multinomial models used for core SMT components, which have no natural method for combining split features, so we rely on an instance-weighting approach (Jiang and Zhai, 2007) to downweight domain-specific examples in OUT.</S>\n', '    <S sid=""149"" ssid=""6"">We obtained positive results using a very simple phrase-based system in two different adaptation settings: using English/French Europarl to improve a performance on a small, specialized medical domain; and using non-news portions of the NIST09 training material to improve performance on the news-related corpora.</S>\n', '    <S sid=""59"" ssid=""23"">To set &#946;, we used the same criterion as for &#945;, over a dev corpus: The MAP combination was used for TM probabilities only, in part due to a technical difficulty in formulating coherent counts when using standard LM smoothing techniques (Kneser and Ney, 1995).3 Motivated by information retrieval, a number of approaches choose &#8220;relevant&#8221; sentence pairs from OUT by matching individual source sentences from IN (Hildebrand et al., 2005; L&#168;u et al., 2007), or individual target hypotheses (Zhao et al., 2004).</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
7,D10-1044,E12-1055,0,2010,0,"These more fine-grained methods need not be seen as alternatives to coarse-grained ones. Foster et al (2010) combine the two, applying linear interpolation to combine the instance 542 weighted out-of-domain model with an in-domain model","Foster et al (2010) combine the two, applying linear interpolation to combine the instance weighted out-of-domain model with an in-domain model","['5', '49', '71', '7', '136']","['    <S sid=""5"" ssid=""2"">Even when there is training data available in the domain of interest, there is often additional data from other domains that could in principle be used to improve performance.</S>\n', '    <S sid=""49"" ssid=""13"">This leads to a linear combination of domain-specific probabilities, with weights in [0, 1], normalized to sum to 1.</S>\n', '    <S sid=""71"" ssid=""8"">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where c&#955;(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>\n', '    <S sid=""7"" ssid=""4"">For developers of Statistical Machine Translation (SMT) systems, an additional complication is the heterogeneous nature of SMT components (word-alignment model, language model, translation model, etc.</S>\n', '    <S sid=""136"" ssid=""5"">It is also worth pointing out a connection with Daum&#180;e&#8217;s (2007) work that splits each feature into domain-specific and general copies.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
8,D10-1044,E12-1055,0,Foster et al 2010,0,Note that both data sets have a relatively high ratio of in-domain to out-of-domain parallel training data (1:20 for DE? EN and 1:5 for HT? EN) Previous research has been performed with ratios of 1:100 (Foster et al 2010) or 1:400 (Axelrod et al 2011),Note that both data sets have a relatively high ratio of in-domain to out-of-domain parallel training data (1:20 for DE? EN and 1:5 for HT? EN); Previous research has been performed with ratios of 1:100 (Foster et al 2010) or 1:400 (Axelrod et al 2011),"['141', '142', '59', '111', '53']","['    <S sid=""141"" ssid=""10"">Moving beyond directly related work, major themes in SMT adaptation include the IR (Hildebrand et al., 2005; L&#168;u et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L&#168;u et al., 2007) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (Bertoldi and Federico, 2009; Ueffing et al., 2007; Schwenk and Senellart, 2009).</S>\n', '    <S sid=""142"" ssid=""11"">There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan, 2007; Wu et al., 2005), and on dynamically choosing a dev set (Xu et al., 2007).</S>\n', '    <S sid=""59"" ssid=""23"">To set &#946;, we used the same criterion as for &#945;, over a dev corpus: The MAP combination was used for TM probabilities only, in part due to a technical difficulty in formulating coherent counts when using standard LM smoothing techniques (Kneser and Ney, 1995).3 Motivated by information retrieval, a number of approaches choose &#8220;relevant&#8221; sentence pairs from OUT by matching individual source sentences from IN (Hildebrand et al., 2005; L&#168;u et al., 2007), or individual target hypotheses (Zhao et al., 2004).</S>\n', '    <S sid=""111"" ssid=""15"">We used a standard one-pass phrase-based system (Koehn et al., 2003), with the following features: relative-frequency TM probabilities in both directions; a 4-gram LM with Kneser-Ney smoothing; word-displacement distortion model; and word count.</S>\n', '    <S sid=""53"" ssid=""17"">This has led previous workers to adopt ad hoc linear weighting schemes (Finch and Sumita, 2008; Foster and Kuhn, 2007; L&#168;u et al., 2007).</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
9,D10-1044,E12-1055,0,Foster et al 2010,0,"We expand on work by (Foster et al 2010) in establishing translation model perplexity minimization as a robust baseline for a weighted combination of translationmodels.15 We demonstrate perplexity optimization for weighted counts, which are a natural extension of unadapted MLE training, but are of little prominence in domain adaptation research",We expand on work by (Foster et al 2010) in establishing translation model perplexity minimization as a robust baseline for a weighted combination of translation models,"['126', '54', '48', '151', '111']","['    <S sid=""126"" ssid=""30"">The 4th block contains instance-weighting models trained on all features, used within a MAP TM combination, and with a linear LM mixture.</S>\n', '    <S sid=""54"" ssid=""18"">However, we note that the final conditional estimates p(s|t) from a given phrase table maximize the likelihood of joint empirical phrase pair counts over a word-aligned corpus.</S>\n', '    <S sid=""48"" ssid=""12"">This is appropriate in cases where it is sanctioned by Bayes&#8217; law, such as multiplying LM and TM probabilities, but for adaptation a more suitable framework is often a mixture model in which each event may be generated from some domain.</S>\n', '    <S sid=""151"" ssid=""8"">In future work we plan to try this approach with more competitive SMT systems, and to extend instance weighting to other standard SMT components such as the LM, lexical phrase weights, and lexicalized distortion.</S>\n', '    <S sid=""111"" ssid=""15"">We used a standard one-pass phrase-based system (Koehn et al., 2003), with the following features: relative-frequency TM probabilities in both directions; a 4-gram LM with Kneser-Ney smoothing; word-displacement distortion model; and word count.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
10,D10-1044,P12-1099,0,"Foster et al, 2010",0,"In addition to the basic approach of concatenation of in-domain and out-of-domain data, we also trained a log-linear mixture model (Foster and Kuhn, 2007) 940 as well as the linear mixture model of (Foster et al, 2010) for conditional phrase-pair probabilities over IN and OUT","In addition to the basic approach of concatenation of in-domain and out-of-domain data, we also trained a log-linear mixture model (Foster and Kuhn, 2007) as well as the linear mixture model of (Foster et al, 2010) for conditional phrase-pair probabilities over IN and OUT","['55', '50', '136', '71', '87']","['    <S sid=""55"" ssid=""19"">This suggests a direct parallel to (1): where &#732;p(s, t) is a joint empirical distribution extracted from the IN dev set using the standard procedure.2 An alternative form of linear combination is a maximum a posteriori (MAP) combination (Bacchiani et al., 2004).</S>\n', '    <S sid=""50"" ssid=""14"">Linear weights are difficult to incorporate into the standard MERT procedure because they are &#8220;hidden&#8221; within a top-level probability that represents the linear combination.1 Following previous work (Foster and Kuhn, 2007), we circumvent this problem by choosing weights to optimize corpus loglikelihood, which is roughly speaking the training criterion used by the LM and TM themselves.</S>\n', '    <S sid=""136"" ssid=""5"">It is also worth pointing out a connection with Daum&#180;e&#8217;s (2007) work that splits each feature into domain-specific and general copies.</S>\n', '    <S sid=""71"" ssid=""8"">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where c&#955;(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>\n', '    <S sid=""87"" ssid=""24"">A final alternate approach would be to combine weighted joint frequencies rather than conditional estimates, ie: cI(s, t) + w,\\(s, t)co(, s, t), suitably normalized.5 Such an approach could be simulated by a MAP-style combination in which separate 0(t) values were maintained for each t. This would make the model more powerful, but at the cost of having to learn to downweight OUT separately for each t, which we suspect would require more training data for reliable performance.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
11,D10-1044,P12-1099,0,2010,0,m ?mpm (e? |f?) Our technique for setting? m is similar to that outlined in Foster et al (2010),Our technique for setting ? m is similar to that outlined in Foster et al (2010),"['143', '129', '114', '47', '59']","['    <S sid=""143"" ssid=""12"">Other work includes transferring latent topic distributions from source to target language for LM adaptation, (Tam et al., 2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita, 2008).</S>\n', '    <S sid=""129"" ssid=""33"">This best instance-weighting model beats the equivalant model without instance weights by between 0.6 BLEU and 1.8 BLEU, and beats the log-linear baseline by a large margin.</S>\n', '    <S sid=""114"" ssid=""18"">It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model.</S>\n', '    <S sid=""47"" ssid=""11"">Apart from MERT difficulties, a conceptual problem with log-linear combination is that it multiplies feature probabilities, essentially forcing different features to agree on high-scoring candidates.</S>\n', '    <S sid=""59"" ssid=""23"">To set &#946;, we used the same criterion as for &#945;, over a dev corpus: The MAP combination was used for TM probabilities only, in part due to a technical difficulty in formulating coherent counts when using standard LM smoothing techniques (Kneser and Ney, 1995).3 Motivated by information retrieval, a number of approaches choose &#8220;relevant&#8221; sentence pairs from OUT by matching individual source sentences from IN (Hildebrand et al., 2005; L&#168;u et al., 2007), or individual target hypotheses (Zhao et al., 2004).</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
12,D10-1044,P12-1099,0,"Foster et al., 2010",0,"m ?mpm (e? |f?) For efficiency and stability, we use the EMalgorithm to find??, rather than L-BFGS as in (Foster et al., 2010)","For efficiency and stability, we use the EM algorithm to find ?, rather than L-BFGS as in (Foster et al., 2010)","['47', '143', '59', '112', '129']","['    <S sid=""47"" ssid=""11"">Apart from MERT difficulties, a conceptual problem with log-linear combination is that it multiplies feature probabilities, essentially forcing different features to agree on high-scoring candidates.</S>\n', '    <S sid=""143"" ssid=""12"">Other work includes transferring latent topic distributions from source to target language for LM adaptation, (Tam et al., 2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita, 2008).</S>\n', '    <S sid=""59"" ssid=""23"">To set &#946;, we used the same criterion as for &#945;, over a dev corpus: The MAP combination was used for TM probabilities only, in part due to a technical difficulty in formulating coherent counts when using standard LM smoothing techniques (Kneser and Ney, 1995).3 Motivated by information retrieval, a number of approaches choose &#8220;relevant&#8221; sentence pairs from OUT by matching individual source sentences from IN (Hildebrand et al., 2005; L&#168;u et al., 2007), or individual target hypotheses (Zhao et al., 2004).</S>\n', '    <S sid=""112"" ssid=""16"">Feature weights were set using Och&#8217;s MERT algorithm (Och, 2003).</S>\n', '    <S sid=""129"" ssid=""33"">This best instance-weighting model beats the equivalant model without instance weights by between 0.6 BLEU and 1.8 BLEU, and beats the log-linear baseline by a large margin.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
13,D10-1044,P12-1099,0,2010,0,"Foster et al (2010), however, uses a different approach to select related sentences from OUT","Foster et al (2010), however, uses a different approach to select related sentences from OUT","['145', '66', '137', '95', '25']","['    <S sid=""145"" ssid=""2"">Each out-of-domain phrase pair is characterized by a set of simple features intended to reflect how useful it will be.</S>\n', '    <S sid=""66"" ssid=""3"">The weight on each sentence is a value in [0, 1] computed by a perceptron with Boolean features that indicate collection and genre membership.</S>\n', '    <S sid=""137"" ssid=""6"">At first glance, this seems only peripherally related to our work, since the specific/general distinction is made for features rather than instances.</S>\n', '    <S sid=""95"" ssid=""32"">Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models), and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples, with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.</S>\n', '    <S sid=""25"" ssid=""22"">For instance, the sentence Similar improvements in haemoglobin levels were reported in the scientific literature for other epoetins would likely be considered domain-specific despite the presence of general phrases like were reported in.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
14,D10-1044,P12-1099,0,2010,0,Foster et al (2010) propose asimilar method for machine translation that uses features to capture degrees of generality,Foster et al (2010) propose a similar method for machine translation that uses features to capture degrees of generality,"['37', '98', '152', '114', '65']","['    <S sid=""37"" ssid=""1"">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S>\n', '    <S sid=""98"" ssid=""2"">The first setting uses the European Medicines Agency (EMEA) corpus (Tiedemann, 2009) as IN, and the Europarl (EP) corpus (www.statmt.org/europarl) as OUT, for English/French translation in both directions.</S>\n', '    <S sid=""152"" ssid=""9"">We will also directly compare with a baseline similar to the Matsoukas et al approach in order to measure the benefit from weighting phrase pairs (or ngrams) rather than full sentences.</S>\n', '    <S sid=""114"" ssid=""18"">It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model.</S>\n', '    <S sid=""65"" ssid=""2"">Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
15,D10-1044,P13-1126,0,"Foster et al, 2010",0,"As in (Foster et al, 2010), this approach works at the level of phrase pairs","As in (Foster et al, 2010), this approach works at the level of phrase pairs","['26', '53', '33', '67', '27']","['    <S sid=""26"" ssid=""23"">Phrase-level granularity distinguishes our work from previous work by Matsoukas et al (2009), who weight sentences according to sub-corpus and genre membership.</S>\n', '    <S sid=""53"" ssid=""17"">This has led previous workers to adopt ad hoc linear weighting schemes (Finch and Sumita, 2008; Foster and Kuhn, 2007; L&#168;u et al., 2007).</S>\n', '    <S sid=""33"" ssid=""30"">The paper is structured as follows.</S>\n', '    <S sid=""67"" ssid=""4"">We extend the Matsoukas et al approach in several ways.</S>\n', '    <S sid=""27"" ssid=""24"">Finally, we make some improvements to baseline approaches.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Hypothesis_Citation', 'Hypothesis_Citation']"
16,D10-1044,D11-1033,0,2010,0,"The ranking of the sentences in a general-domain corpus according to in-domain perplexity has also been applied to machine translation by both Yasuda et al (2008), and Foster et al (2010)","The ranking of the sentences in a general-domain corpus according to in-domain perplexity has also been applied to machine translation by both Yasuda et al (2008), and Foster et al (2010)","['136', '135', '138', '59', '142']","['    <S sid=""136"" ssid=""5"">It is also worth pointing out a connection with Daum&#180;e&#8217;s (2007) work that splits each feature into domain-specific and general copies.</S>\n', '    <S sid=""135"" ssid=""4"">Finally, we note that Jiang&#8217;s instance-weighting framework is broader than we have presented above, encompassing among other possibilities the use of unlabelled IN data, which is applicable to SMT settings where source-only IN corpora are available.</S>\n', '    <S sid=""138"" ssid=""7"">However, for multinomial models like our LMs and TMs, there is a one to one correspondence between instances and features, eg the correspondence between a phrase pair (s, t) and its conditional multinomial probability p(s1t).</S>\n', '    <S sid=""59"" ssid=""23"">To set &#946;, we used the same criterion as for &#945;, over a dev corpus: The MAP combination was used for TM probabilities only, in part due to a technical difficulty in formulating coherent counts when using standard LM smoothing techniques (Kneser and Ney, 1995).3 Motivated by information retrieval, a number of approaches choose &#8220;relevant&#8221; sentence pairs from OUT by matching individual source sentences from IN (Hildebrand et al., 2005; L&#168;u et al., 2007), or individual target hypotheses (Zhao et al., 2004).</S>\n', '    <S sid=""142"" ssid=""11"">There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan, 2007; Wu et al., 2005), and on dynamically choosing a dev set (Xu et al., 2007).</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
17,D10-1044,D11-1033,0,2010,0,"Foster et al (2010) do not mention what percentage of the corpus they select for their IR-baseline, but they concatenate the data to their in-domain corpus and re port a decrease in performance","Foster et al (2010) do not mention what percentage of the corpus they select for their IR-baseline, but they concatenate the data to their in-domain corpus and report a decrease in performance","['66', '98', '137', '25', '114']","['    <S sid=""66"" ssid=""3"">The weight on each sentence is a value in [0, 1] computed by a perceptron with Boolean features that indicate collection and genre membership.</S>\n', '    <S sid=""98"" ssid=""2"">The first setting uses the European Medicines Agency (EMEA) corpus (Tiedemann, 2009) as IN, and the Europarl (EP) corpus (www.statmt.org/europarl) as OUT, for English/French translation in both directions.</S>\n', '    <S sid=""137"" ssid=""6"">At first glance, this seems only peripherally related to our work, since the specific/general distinction is made for features rather than instances.</S>\n', '    <S sid=""25"" ssid=""22"">For instance, the sentence Similar improvements in haemoglobin levels were reported in the scientific literature for other epoetins would likely be considered domain-specific despite the presence of general phrases like were reported in.</S>\n', '    <S sid=""114"" ssid=""18"">It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
18,D10-1044,D11-1033,0,2010,0,"Foster et al (2010) further perform this on extracted phrase pairs, not just sentences","Foster et al (2010) further perform this on extracted phrase pairs, not just sentences","['35', '145', '137', '37', '98']","['    <S sid=""35"" ssid=""32"">Experiments are presented in section 4.</S>\n', '    <S sid=""145"" ssid=""2"">Each out-of-domain phrase pair is characterized by a set of simple features intended to reflect how useful it will be.</S>\n', '    <S sid=""137"" ssid=""6"">At first glance, this seems only peripherally related to our work, since the specific/general distinction is made for features rather than instances.</S>\n', '    <S sid=""37"" ssid=""1"">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S>\n', '    <S sid=""98"" ssid=""2"">The first setting uses the European Medicines Agency (EMEA) corpus (Tiedemann, 2009) as IN, and the Europarl (EP) corpus (www.statmt.org/europarl) as OUT, for English/French translation in both directions.</S>\n']","['Hypothesis_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
19,D10-1044,P14-1012,0,"Foster et al, 2010",0,"To address the first shortcoming, we adapt and extend some simple but effective phrase features as the input features for new DNN feature learning, and these features have been shown significant improvement for SMT, such as, phrase pair similarity (Zhao et al, 2004), phrase frequency, phrase length (Hopkins and May, 2011), and phrase generative probability (Foster et al, 2010), which also show further improvement for new phrase feature learning in our experiments","To address the first shortcoming, we adapt and extend some simple but effective phrase features as the input features for new DNN feature learning, and these features have been shown significant improvement for SMT, such as, phrase pair similarity (Zhao et al, 2004), phrase frequency, phrase length (Hopkins and May, 2011), and phrase generative probability (Foster et al, 2010), which also show further improvement for new phrase feature learning in our experiments","['98', '65', '152', '149', '95']","['    <S sid=""98"" ssid=""2"">The first setting uses the European Medicines Agency (EMEA) corpus (Tiedemann, 2009) as IN, and the Europarl (EP) corpus (www.statmt.org/europarl) as OUT, for English/French translation in both directions.</S>\n', '    <S sid=""65"" ssid=""2"">Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.</S>\n', '    <S sid=""152"" ssid=""9"">We will also directly compare with a baseline similar to the Matsoukas et al approach in order to measure the benefit from weighting phrase pairs (or ngrams) rather than full sentences.</S>\n', '    <S sid=""149"" ssid=""6"">We obtained positive results using a very simple phrase-based system in two different adaptation settings: using English/French Europarl to improve a performance on a small, specialized medical domain; and using non-news portions of the NIST09 training material to improve performance on the news-related corpora.</S>\n', '    <S sid=""95"" ssid=""32"">Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models), and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples, with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
