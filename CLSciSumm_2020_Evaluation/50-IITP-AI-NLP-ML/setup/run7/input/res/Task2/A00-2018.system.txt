now consider computing a conditional probability p(a h) with a set of features h that connect a to the history h. in a log-linear model the probability function takes the following form: here the ai are weights between negative and positive infinity that indicate the relative importance of a feature: the more relevant the feature to the value of the probability, the higher the absolute value of the associated a.
that the previous three best parsers on this test [5,9,17] all perform within a percentage point of each other, despite quite different basic mechanisms, led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training, and to conjecture that perhaps we were at it.
we present a new parser for parsing down to penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40, and 89.5% for sentences of length < 100, when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the wall street journal tree-bank.
thus an expansion e(c) looks like: the expansion is generated by guessing first m, then in order l1 through l,â€ž.+1 (= a), and similarly for ri through in a pure markov pcfg we are given the left-hand side label 1 and then probabilistically generate the right-hand side conditioning on no information other than 1 and (possibly) previously generated pieces of the right-hand side itself.
the results of [13] achieved by combining the aforementioned three-best parsers also suggest that the limit on tree-bank trained parsers is much higher than previously thought.
