now, if we assume that p lies in the same space as u and v, avoiding the issues of dimensionality associated with tensor products, and that f is a linear function, for simplicity, of the cartesian product of u and v, then we generate a class of additive models: where a and b are matrices which determine the contributions made by u and v to the product p. in contrast, if we assume that f is a linear function of the tensor product of u and v, then we obtain multiplicative models: where c is a tensor of rank 3, which projects the tensor product of u and v onto the space of p. further constraints can be introduced to reduce the free parameters in these models.
experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments.
so, if we assume that only the ith components of u and v contribute to the ith component of p, that these components are not dependent on i, and that the function is symmetric with regard to the interchange of u and v, we obtain a simpler instantiation of an additive model: analogously, under the same assumptions, we obtain the following simpler multiplicative model: only the ith components of u and v contribute to the ith component of p. another class of models can be derived by relaxing this constraint.
in this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.
