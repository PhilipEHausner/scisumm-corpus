in order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named non-lex using only nonlexical-target features and another named lex using both the two kinds of features.
as all the sub-models, including the perceptron, are regarded as separate features of the outside-layer linear model, we can train them respectively with special algorithms.
according to ng and low (2004), the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: we can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.
as shown in figure 1, the character-based perceptron is used as the inside-layer linear model and sends its output to the outside-layer.
the first was conducted to test the performance of the perceptron on segmentation on the corpus from sighan bakeoff 2, including the academia sinica corpus (as), the hong kong city university corpus (cityu), the peking university corpus (pku) and the microsoft research corpus (msr).
besides this perceptron, other sub-models are trained and used as additional features of the outside-layer linear model.
since the typical approach of discriminative models treats segmentation as a labelling problem by assigning each character a boundary tag (xue and shen, 2003), joint s&t can be conducted in a labelling fashion by expanding boundary tags to include pos information (ng and low, 2004).
