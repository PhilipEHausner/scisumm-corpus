maximum topic probability in document although the posterior distribution over topics for each tuple is not concentrated on one topic, it is worth checking that this is not simply because the model is assigning a single topic to the 1we use the r density function. tokens in each of the languages.
topic models have been used for analyzing topic trends in research literature (mann et al., 2006; hall et al., 2008), inferring captions for images (blei and jordan, 2003), social network analysis in email (mccallum et al., 2005), and expanding queries with topically related words in information retrieval (wei and croft, 2006).
an important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct, non-comparable documents in multiple languages.
additionally, pltm can support the creation of bilingual lexica for low resource language pairs, providing candidate translations for more computationally intense alignment processes without the sentence-aligned translations typically used in such tasks.
a recent extended abstract, developed concurrently by ni et al. (ni et al., 2009), discusses a multilingual topic model similar to the one presented here.
we take a simpler approach that is more suitable for topically similar document tuples (where documents are not direct translations of one another) in more than two languages.
to evaluate this scenario, we train pltm on a set of document tuples from europarl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.
