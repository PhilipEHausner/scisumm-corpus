sentences and systems were randomly selected and randomly shuffled for presentation.
by taking the ratio of matching n-grams to the total number of n-grams in the system output, we obtain the precision pn for each n-gram order n. these values for n-gram precision are combined into a bleu score: the formula for the bleu metric also includes a brevity penalty for too short output, which is based on the total number of words in the system output c and in the reference r. bleu is sensitive to tokenization.
also, the argument has been made that machine translation performance should be evaluated via task-based evaluation metrics, i.e. how much it assists performing a useful task, such as supporting human translators or aiding the analysis of texts.
the bootstrap method has been critized by riezler and maxwell (2005) and collins et al. (2005), as being too optimistic in deciding for statistical significant difference between systems.
we check, how likely only up to k = 20 better scores out of n = 100 would have been generated by two equal systems, using the binomial distribution: if p(0..k; n, p) < 0.05, or p(0..k; n, p) > 0.95 then we have a statistically significant difference between the systems.
however, ince we extracted the test corpus automatically from web sources, the reference translation was not always accurate â€” due to sentence alignment errors, or because translators did not adhere to a strict sentence-by-sentence translation (say, using pronouns when referring to entities mentioned in the previous sentence).
the predominate focus of building systems that translate into english has ignored so far the difficult issues of generating rich morphology which may not be determined solely by local context.
