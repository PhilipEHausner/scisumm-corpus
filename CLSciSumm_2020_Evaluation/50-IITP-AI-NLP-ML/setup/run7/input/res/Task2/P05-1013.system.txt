the second experiment is limited to data from pdt.5 the training part of the treebank was projectivized under different encoding schemes and used to train memory-based dependency parsers, which were run on the test part of the treebank, consisting of 7,507 sentences and 125,713 tokens.6 the inverse transformation was applied to the output of the parsers and the result compared to the gold standard test set.
even this may be nondeterministic, in case the graph contains several non-projective arcs whose lifts interact, but we use the following algorithm to construct a minimal projective transformation d0 = (w, a0) of a (nonprojective) dependency graph d = (w, a): the function smallest-nonp-arc returns the non-projective arc with the shortest distance from head to dependent (breaking ties from left to right).
compared to related work on the recovery of long-distance dependencies in constituency-based parsing, our approach is similar to that of dienes and dubey (2003) in that the processing of non-local dependencies is partly integrated in the parsing process, via an extension of the set of syntactic categories, whereas most other approaches rely on postprocessing only.
by applying an inverse transformation to the output of the parser, arcs with non-standard labels can be lowered to their proper place in the dependency graph, giving rise 1the dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.
by contrast, when we turn to a comparison of the three encoding schemes it is hard to find any significant differences, and the overall impression is that it makes little or no difference which encoding scheme is used, as long as there is some indication of which words are assigned their linear head instead of their syntactic head by the projective parser.
