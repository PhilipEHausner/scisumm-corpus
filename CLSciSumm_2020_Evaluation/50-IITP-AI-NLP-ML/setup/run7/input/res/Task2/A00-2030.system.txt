because generative statistical models had already proven successful for each of the first three stages, we were optimistic that some of their properties — especially their ability to learn from large amounts of data, and their robustness when presented with unexpected inputs — would also benefit semantic analysis.
currently, the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: since we were interested in exploiting recent advances in parsing, replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.
instead, our parsing algorithm, trained on the upenn treebank, was run on the new york times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.
thus, we did not consider simply adding semantic labels to the existing penn treebank, which is drawn from a single source — the wall street journal — and is impoverished in articles about rocket launches.
we were already using a generative statistical model for part-of-speech tagging (weischedel et al. 1993), and more recently, had begun using a generative statistical model for name finding (bikel et al.
in this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machinegenerated syntactic parse trees.
the detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.
at each step in the process, a choice is made from a statistical distribution, with the probability of each possible selection dependent on particular features of previously generated elements.
