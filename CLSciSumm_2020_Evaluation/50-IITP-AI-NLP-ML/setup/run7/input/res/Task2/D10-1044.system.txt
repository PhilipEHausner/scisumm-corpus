moving beyond directly related work, major themes in smt adaptation include the ir (hildebrand et al., 2005; l¨u et al., 2007; zhao et al., 2004) and mixture (finch and sumita, 2008; foster and kuhn, 2007; koehn and schroeder, 2007; l¨u et al., 2007) approaches for lms and tms described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (bertoldi and federico, 2009; ueffing et al., 2007; schwenk and senellart, 2009).
to set β, we used the same criterion as for α, over a dev corpus: the map combination was used for tm probabilities only, in part due to a technical difficulty in formulating coherent counts when using standard lm smoothing techniques (kneser and ney, 1995).3 motivated by information retrieval, a number of approaches choose “relevant” sentence pairs from out by matching individual source sentences from in (hildebrand et al., 2005; l¨u et al., 2007), or individual target hypotheses (zhao et al., 2004).
we used a standard one-pass phrase-based system (koehn et al., 2003), with the following features: relative-frequency tm probabilities in both directions; a 4-gram lm with kneser-ney smoothing; word-displacement distortion model; and word count.
a final alternate approach would be to combine weighted joint frequencies rather than conditional estimates, ie: ci(s, t) + w,\(s, t)co(, s, t), suitably normalized.5 such an approach could be simulated by a map-style combination in which separate 0(t) values were maintained for each t. this would make the model more powerful, but at the cost of having to learn to downweight out separately for each t, which we suspect would require more training data for reliable performance.
