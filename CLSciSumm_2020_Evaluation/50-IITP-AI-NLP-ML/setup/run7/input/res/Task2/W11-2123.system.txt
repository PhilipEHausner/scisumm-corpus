compared with the widely- srilm, our is 2.4 times as fast while using 57% of the mem- the structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less cpu than the baseline.
while sorted arrays could be used to implement the same data structure as probing, effectively making m = 1, we abandoned this implementation because it is slower and larger than a trie implementation.
performance improvements transfer to the moses (koehn et al., 2007), cdec (dyer et al., 2010), and joshua (li et al., 2009) translation systems where our code has been integrated.
the cost of storing these averages, in bits, is because there are comparatively few unigrams, we elected to store them byte-aligned and unquantized, making every query faster.
memory usage is likely much lower than ours. fthe original paper (germann et al., 2009) provided only 2s of query timing and compared with sri when it exceeded available ram.
this has the effect of randomly permuting vocabulary identifiers, meeting the requirements of interpolation search when vocabulary identifiers are used as keys.
for example, syntactic decoders (koehn et al., 2007; dyer et al., 2010; li et al., 2009) perform dynamic programming parametrized by both backward- and forward-looking state.
this is most severe with randlm in the multi-threaded case, where each thread keeps a separate cache, exceeding the original model size.
irstlm 5.60.02 (federico et al., 2008) is a sorted trie implementation designed for lower memory consumption.
the binary language model from section 5.2 and text phrase table were forced into disk cache before each run.
