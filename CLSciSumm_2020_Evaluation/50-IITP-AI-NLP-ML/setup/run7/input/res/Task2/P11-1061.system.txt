since our graph is built from a parallel corpus, we can use standard word alignment techniques to align the english sentences de 5note that many combinations are impossible giving a pmi value of 0; e.g., when the trigram type and the feature instantiation don’t have words in common. and their foreign language translations df.6 label propagation in the graph will provide coverage and high recall, and we therefore extract only intersected high-confidence (> 0.9) alignments de�f.
we use the universal pos tagset of petrov et al. (2011) in our experiments.10 this set c consists of the following 12 coarse-grained tags: noun (nouns), verb (verbs), adj (adjectives), adv (adverbs), pron (pronouns), det (determiners), adp (prepositions or postpositions), num (numerals), conj (conjunctions), prt (particles), punc (punctuation marks) and x (a catch-all for other categories such as abbreviations or foreign words).
to bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like english) when building tools for resource-poor foreign languages.1 we assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.
as indicated by bolding, for seven out of eight languages the improvements of the “with lp” setting are statistically significant with respect to the other models, including the “no lp” setting.11 overall, it performs 10.4% better than the hitherto state-of-the-art feature-hmm baseline, and 4.6% better than direct projection, when we macro-average the accuracy over all languages.
