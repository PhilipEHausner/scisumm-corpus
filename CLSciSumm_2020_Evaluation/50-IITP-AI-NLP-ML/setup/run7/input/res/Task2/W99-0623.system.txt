the precision and recall measures (described in more detail in section 3) used in evaluating treebank parsing treat each constituent as a separate entity, a minimal unit of correctness.
these two principles guide experimentation in this framework, and together with the evaluation measures help us decide which specific type of substructure to combine.
in each figure the upper graph shows the isolated constituent precision and the bottom graph shows the corresponding number of hypothesized constituents.
we show the results of three of the experiments we conducted to measure isolated constituent precision under various partitioning schemes.
the probabilistic version of this procedure is straightforward: we once again assume independence among our various member parsers.
when this metric is less than 0.5, we expect to incur more errors' than we will remove by adding those constituents to the parse.
the development of a na√Øve bayes classifier involves learning how much each parser should be trusted for the decisions it makes.
the standard measures for evaluating penn treebank parsing performance are precision and recall of the predicted constituents.
there are simply not enough votes remaining to allow any of the crossing structures to enter the hypothesized constituent set.
we then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.
in the cases where isolated constituent precision is larger than 0.5 the affected portion of the hypotheses is negligible.
the counts represent portions of the approximately 44000 constituents hypothesized by the parsers in the development set.
we have presented two general approaches to studying parser combination: parser switching and parse hybridization.
