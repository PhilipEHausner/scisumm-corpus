Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
2,W06-2932,W06-2920,,"McDonald et al, 2006",0,"Introduce through post-processing ,e.g. through reattachment rules (Bick, 2006) or if the change increases overall parse tree probability (McDonald et al, 2006)","Introduce through post-processing ,e.g. through reattachment rules (Bick, 2006) or if the change increases overall parse tree probability (McDonald et al, 2006)","['18', '25', '58', '44', '3']","['    <S sid=""18"" ssid=""14"">We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X), pages 216&#8211;220, New York City, June 2006. c&#65533;2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective, both of which are true in the data sets we use.</S>\n', '    <S sid=""25"" ssid=""7"">For instance, the system of McDonald et al. (2005a) incorporates features over the part of speech of words occurring between and around a possible head-dependent relation.</S>\n', '    <S sid=""58"" ssid=""6"">These results show that the discriminative spanning tree parsing framework (McDonald et al., 2005b; McDonald and Pereira, 2006) is easily adapted across all these languages.</S>\n', '    <S sid=""44"" ssid=""13"">We use the MIRA online learner to set the weights (Crammer and Singer, 2003; McDonald et al., 2005a) since we found it trained quickly and provide good performance.</S>\n', '    <S sid=""3"" ssid=""3"">The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
3,W06-2932,W06-2920,0,2006,0,Table 5 shows the official results for submitted parser outputs.31 The two participant groups with the highest total score are McDonald et al (2006) and Nivre et al (2006),Table 5 shows the official results for submitted parser outputs. The two participant groups with the highest total score are McDonald et al (2006) and Nivre et al (2006),"['81', '58', '18', '11', '43']","['    <S sid=""81"" ssid=""3"">Other high-frequency word classes with relatively low attachment accuracy are prepositions (80%), adverbs (82%) and subordinating conjunctions (80%), for a total of another 23% of the test corpus.</S>\n', '    <S sid=""58"" ssid=""6"">These results show that the discriminative spanning tree parsing framework (McDonald et al., 2005b; McDonald and Pereira, 2006) is easily adapted across all these languages.</S>\n', '    <S sid=""18"" ssid=""14"">We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X), pages 216&#8211;220, New York City, June 2006. c&#65533;2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective, both of which are true in the data sets we use.</S>\n', '    <S sid=""11"" ssid=""7"">This has been shown through their successful use in many standard natural language processing tasks, including machine translation (Ding and Palmer, 2005), sentence compression (McDonald, 2006), and textual inference (Haghighi et al., 2005).</S>\n', '    <S sid=""43"" ssid=""12"">For score functions, we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation, we can find the highest scoring label sequence with Viterbi&#8217;s algorithm.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
4,W06-2932,W06-2920,0,2006,0,"Even though McDonald et al (2006) and Nivre et al (2006) obtained very similar overall scores, a more detailed look at their performance shows clear differences","Even though McDonald et al (2006) and Nivre et al (2006) obtained very similar overall scores, a more detailed look at their performance shows clear differences","['44', '104', '58', '34', '61']","['    <S sid=""44"" ssid=""13"">We use the MIRA online learner to set the weights (Crammer and Singer, 2003; McDonald et al., 2005a) since we found it trained quickly and provide good performance.</S>\n', '    <S sid=""104"" ssid=""1"">We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al., 2005b; McDonald and Pereira, 2006) generalizes well to languages other than English.</S>\n', '    <S sid=""58"" ssid=""6"">These results show that the discriminative spanning tree parsing framework (McDonald et al., 2005b; McDonald and Pereira, 2006) is easily adapted across all these languages.</S>\n', '    <S sid=""34"" ssid=""3"">However, the parser is fundamentally limited by the scope of local factorizations that make inference tractable.</S>\n', '    <S sid=""61"" ssid=""9"">In fact, for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
5,W06-2932,W08-1007,0,2006,0,"The high est score on parsing German in the CoNLL-X shared task was obtained by the system of McDonald et al (2006) with a LAS of 87.34 based on the TIGER tree bank, but we want to stress that these results are not comparable due to different data sets (anda different policy regarding the inclusion of punctuation) .The constituency versions were evaluated according to the labeled recall (LR), labeled precision (LP) and labeled F-score (LF)","The highest score on parsing German in the CoNLL-X shared task was obtained by the system of McDonald et al (2006) with a LAS of 87.34 based on the TIGER tree bank, but we want to stress that these results are not comparable due to different data sets (and a different policy regarding the inclusion of punctuation). The constituency versions were evaluated according to the labeled recall (LR), labeled precision (LP) and labeled F-score (LF)","['25', '30', '3', '8', '64']","['    <S sid=""25"" ssid=""7"">For instance, the system of McDonald et al. (2005a) incorporates features over the part of speech of words occurring between and around a possible head-dependent relation.</S>\n', '    <S sid=""30"" ssid=""12"">These features play the obvious role of explicitly modeling consistencies and commonalities between a head and its dependents in terms of attributes like gender, case, or number.</S>\n', '    <S sid=""3"" ssid=""3"">The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph.</S>\n', '    <S sid=""8"" ssid=""4"">This interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages.</S>\n', '    <S sid=""64"" ssid=""2"">N/P: Allow non-projective/Force projective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
6,W06-2932,W09-1210,0,2006,,McDonald et al (2006) use an additional algorithm,McDonald et al (2006) use an additional algorithm,"['53', '11', '44', '58', '52']","['    <S sid=""53"" ssid=""1"">We trained models for all 13 languages provided by the CoNLL organizers (Buchholz et al., 2006).</S>\n', '    <S sid=""11"" ssid=""7"">This has been shown through their successful use in many standard natural language processing tasks, including machine translation (Ding and Palmer, 2005), sentence compression (McDonald, 2006), and textual inference (Haghighi et al., 2005).</S>\n', '    <S sid=""44"" ssid=""13"">We use the MIRA online learner to set the weights (Crammer and Singer, 2003; McDonald et al., 2005a) since we found it trained quickly and provide good performance.</S>\n', '    <S sid=""58"" ssid=""6"">These results show that the discriminative spanning tree parsing framework (McDonald et al., 2005b; McDonald and Pereira, 2006) is easily adapted across all these languages.</S>\n', '    <S sid=""52"" ssid=""21"">Thus a joint model of parsing and labeling could not easily include them without some form of re-ranking or approximate parameter estimation.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
7,W06-2932,W12-3407,0,"McDonald et al, 2006",0,"Regarding the data-driven parsers, we have made use of MaltParser (Nivre et al, 2007b) and MST Parser (McDonald et al, 2006), two state of the art dependency parsers representing two dominant approaches in data-driven dependency parsing, and that have been successfully applied to typologically different languages and tree banks (McDonald and Nivre, 2007)","Regarding the data-driven parsers, we have made use of MaltParser (Nivre et al, 2007b) and MST Parser (McDonald et al, 2006), two state of the art dependency parsers representing two dominant approaches in data-driven dependency parsing, and that have been successfully applied to typologically different languages and tree banks (McDonald and Nivre, 2007)","['25', '21', '58', '104', '7']","['    <S sid=""25"" ssid=""7"">For instance, the system of McDonald et al. (2005a) incorporates features over the part of speech of words occurring between and around a possible head-dependent relation.</S>\n', '    <S sid=""21"" ssid=""3"">That work extends the maximum spanning tree dependency parsing framework (McDonald et al., 2005a; McDonald et al., 2005b) to incorporate features over multiple edges in the dependency graph.</S>\n', '    <S sid=""58"" ssid=""6"">These results show that the discriminative spanning tree parsing framework (McDonald et al., 2005b; McDonald and Pereira, 2006) is easily adapted across all these languages.</S>\n', '    <S sid=""104"" ssid=""1"">We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al., 2005b; McDonald and Pereira, 2006) generalizes well to languages other than English.</S>\n', '    <S sid=""7"" ssid=""3"">However, recently their has been a revived interest in parsing models that produce dependency graph representations of sentences, which model words and their arguments through directed edges (Hudson, 1984; Mel\'&#711;cuk, 1988).</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
8,W06-2932,I08-1012,0,"McDonald et al, 2006",0,"In fact, our approach can also be applied to other parsers, such as (Yamada and Matsumoto, 2003)? s parser, (McDonald et al., 2006)? s parser, and so on","In fact, our approach can also be applied to other parsers, such as (Yamada and Matsumoto, 2003)'s parser, (McDonald et al., 2006)'s parser, and so on","['58', '20', '25', '4', '45']","['    <S sid=""58"" ssid=""6"">These results show that the discriminative spanning tree parsing framework (McDonald et al., 2005b; McDonald and Pereira, 2006) is easily adapted across all these languages.</S>\n', '    <S sid=""20"" ssid=""2"">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S>\n', '    <S sid=""25"" ssid=""7"">For instance, the system of McDonald et al. (2005a) incorporates features over the part of speech of words occurring between and around a possible head-dependent relation.</S>\n', '    <S sid=""4"" ssid=""4"">We report results on the CoNLL-X shared task (Buchholz et al., 2006) data sets and present an error analysis.</S>\n', '    <S sid=""45"" ssid=""14"">Furthermore, it made the system homogeneous in terms of learning algorithms since that is what is used to train our unlabeled parser (McDonald and Pereira, 2006).</S>\n']","['Result_Citation', 'Hypothesis_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
11,W06-2932,N07-1050,0,"McDonald et al, 2006",0,"We have shown that, for languages with a7McDonald et al (2006) use post-processing for non projective dependencies and for labeling",McDonald et al (2006) use post-processing for non-projective dependencies and for labeling,"['10', '104', '25', '22', '18']","['    <S sid=""10"" ssid=""6"">Dependency graphs also encode much of the deep syntactic information needed for further processing.</S>\n', '    <S sid=""104"" ssid=""1"">We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al., 2005b; McDonald and Pereira, 2006) generalizes well to languages other than English.</S>\n', '    <S sid=""25"" ssid=""7"">For instance, the system of McDonald et al. (2005a) incorporates features over the part of speech of words occurring between and around a possible head-dependent relation.</S>\n', '    <S sid=""22"" ssid=""4"">An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge.</S>\n', '    <S sid=""18"" ssid=""14"">We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X), pages 216&#8211;220, New York City, June 2006. c&#65533;2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective, both of which are true in the data sets we use.</S>\n']","['Hypothesis_Citation', 'Result_Citation', 'Result_Citation', 'Hypothesis_Citation', 'Result_Citation']"
12,W06-2932,D07-1122,0,"McDonald et al, 2006",0,"As described in (McDonald et al, 2006), we treat the labeling of dependencies as a sequence labeling problem","As described in (McDonald et al, 2006), we treat the labeling of dependencies as a sequence labeling problem","['2', '104', '40', '21', '75']","['    <S sid=""2"" ssid=""2"">The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.</S>\n', '    <S sid=""104"" ssid=""1"">We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al., 2005b; McDonald and Pereira, 2006) generalizes well to languages other than English.</S>\n', '    <S sid=""40"" ssid=""9"">For instance, if we consider a head xi with dependents xj1, ... , xjM, it is often the case that many of these dependencies will have correlated labels.</S>\n', '    <S sid=""21"" ssid=""3"">That work extends the maximum spanning tree dependency parsing framework (McDonald et al., 2005a; McDonald et al., 2005b) to incorporate features over multiple edges in the dependency graph.</S>\n', '    <S sid=""75"" ssid=""13"">This is not surprising since these edge labels typically are the most correlated (i.e., if you already know which noun dependent is the subject, then it should be easy to find the object).</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
14,W06-2932,D07-1015,0,2006,0,5It should be noted that McDonald et al (2006) use a richer feature set that is incomparable to our features,It should be noted that McDonald et al (2006) use a richer feature set that is incomparable to our features,"['45', '25', '2', '55', '29']","['    <S sid=""45"" ssid=""14"">Furthermore, it made the system homogeneous in terms of learning algorithms since that is what is used to train our unlabeled parser (McDonald and Pereira, 2006).</S>\n', '    <S sid=""25"" ssid=""7"">For instance, the system of McDonald et al. (2005a) incorporates features over the part of speech of words occurring between and around a possible head-dependent relation.</S>\n', '    <S sid=""2"" ssid=""2"">The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.</S>\n', '    <S sid=""55"" ssid=""3"">Furthermore, for Arabic and Spanish, we used lemmas instead of inflected word forms, again based on performance on held-out data1.</S>\n', '    <S sid=""29"" ssid=""11"">We then add to the representation of the edge: Mi as head features, Mj as dependent features, and also each conjunction of a feature from both sets.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
18,W06-2932,D10-1004,0,2006,0,"Entries marked with? are the highest reported in the literature, to the best of our knowledge, beating (sometimes slightly) McDonald et al (2006), Martins et al (2008), Martins et al (2009), and, in the case of English Proj., also the third-order parser of Koo and Collins (2010), which achieves 93.04% on that dataset (their experiments in Czech are not comparable, since the datasets are different)","Entries marked with are the highest reported in the literature, to the best of our knowledge, beating (sometimes slightly) McDonald et al (2006), Martins et al (2008), Martins et al (2009), and, in the case of English Proj., also the third-order parser of Koo and Collins (2010), which achieves 93.04% on that dataset (their experiments in Czech are not comparable, since the datasets are different)","['58', '45', '44', '30', '11']","['    <S sid=""58"" ssid=""6"">These results show that the discriminative spanning tree parsing framework (McDonald et al., 2005b; McDonald and Pereira, 2006) is easily adapted across all these languages.</S>\n', '    <S sid=""45"" ssid=""14"">Furthermore, it made the system homogeneous in terms of learning algorithms since that is what is used to train our unlabeled parser (McDonald and Pereira, 2006).</S>\n', '    <S sid=""44"" ssid=""13"">We use the MIRA online learner to set the weights (Crammer and Singer, 2003; McDonald et al., 2005a) since we found it trained quickly and provide good performance.</S>\n', '    <S sid=""30"" ssid=""12"">These features play the obvious role of explicitly modeling consistencies and commonalities between a head and its dependents in terms of attributes like gender, case, or number.</S>\n', '    <S sid=""11"" ssid=""7"">This has been shown through their successful use in many standard natural language processing tasks, including machine translation (Ding and Palmer, 2005), sentence compression (McDonald, 2006), and textual inference (Haghighi et al., 2005).</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
19,W06-2932,P08-1108,0,"McDonald et al, 2006",0,"The specific graph-based model studied in this work is that presented by McDonald et al (2006), which factors scores over pairs of arcs (instead of just single arcs) and uses near exhaustive search for unlabeled parsing coupled with a separate classifier to label each arc. We call this system MSTParser, or simply MST for short, which is also the name of the freely available implementation.2 2.3 Transition-Based Models","The specific graph-based model studied in this work is that presented by McDonald et al (2006), which factors scores over pairs of arcs (instead of just single arcs) and uses near exhaustive search for unlabeled parsing coupled with a separate classifier to label each arc. We call this system MSTParser, or simply MST for short, which is also the name of the freely available implementation.","['31', '25', '30', '3', '82']","['    <S sid=""31"" ssid=""13"">Not all data sets in our experiments include morphological features, so we use them only when available.</S>\n', '    <S sid=""25"" ssid=""7"">For instance, the system of McDonald et al. (2005a) incorporates features over the part of speech of words occurring between and around a possible head-dependent relation.</S>\n', '    <S sid=""30"" ssid=""12"">These features play the obvious role of explicitly modeling consistencies and commonalities between a head and its dependents in terms of attributes like gender, case, or number.</S>\n', '    <S sid=""3"" ssid=""3"">The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph.</S>\n', '    <S sid=""82"" ssid=""4"">These weaknesses are not surprising, since these decisions encode the more global aspects of sentence structure: arrangement of clauses and adverbial dependents in multi-clause sentences, and prepositional phrase attachment.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
20,W06-2932,P08-1108,0,"McDonald et al, 2006",0,"More precisely, dependency arcs (or pairs of arcs) are first represented by a high dimensional feature vector f (i, j, l)? Rk, where f is typically a bi nary feature vector over properties of the arc as well as the surrounding input (McDonald et al, 2005a; McDonald et al, 2006)","More precisely, dependency arcs (or pairs of arcs) are first represented by a high dimensional feature vector f (i, j, l) Rk, where f is typically a binary feature vector over properties of the arc as well as the surrounding input (McDonald et al, 2005a; McDonald et al, 2006)","['25', '44', '58', '8', '18']","['    <S sid=""25"" ssid=""7"">For instance, the system of McDonald et al. (2005a) incorporates features over the part of speech of words occurring between and around a possible head-dependent relation.</S>\n', '    <S sid=""44"" ssid=""13"">We use the MIRA online learner to set the weights (Crammer and Singer, 2003; McDonald et al., 2005a) since we found it trained quickly and provide good performance.</S>\n', '    <S sid=""58"" ssid=""6"">These results show that the discriminative spanning tree parsing framework (McDonald et al., 2005b; McDonald and Pereira, 2006) is easily adapted across all these languages.</S>\n', '    <S sid=""8"" ssid=""4"">This interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages.</S>\n', '    <S sid=""18"" ssid=""14"">We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X), pages 216&#8211;220, New York City, June 2006. c&#65533;2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective, both of which are true in the data sets we use.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
