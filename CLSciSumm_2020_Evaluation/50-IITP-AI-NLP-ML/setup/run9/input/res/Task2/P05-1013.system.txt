the second experiment is limited to data from pdt the training part of the treebank was projectivized under different encoding schemes and used to train memory based dependency parsers which were run on the test part of the treebank consisting of sentences and tokens the inverse transformation was applied to the output of the parsers and the result compared to the gold standard test set 
the last four columns in table show the distribution of nonprojective arcs with respect to the number of lifts required 
however since we want to preserve as much of the original structure as possible we are interested in finding a transformation that involves a minimal number of lifts 
this may seem surprising given the experiments reported in section but the explanation is probably that the non projective dependencies that can be recovered at all are of the simple kind that only requires a single lift where the encoding of path information is often redundant 
compared to related work on the recovery of long distance dependencies in constituency based parsing our approach is similar to that of dienes and dubey in that the processing of non local dependencies is partly integrated in the parsing process via an extension of the set of syntactic categories whereas most other approaches rely on postprocessing only 
even this may be nondeterministic in case the graph contains several non projective arcs whose lifts interact but we use the following algorithm to construct a minimal projective transformation d w a of a nonprojective dependency graph d w a the function smallest nonp arc returns the non projective arc with the shortest distance from head to dependent breaking ties from left to right 
