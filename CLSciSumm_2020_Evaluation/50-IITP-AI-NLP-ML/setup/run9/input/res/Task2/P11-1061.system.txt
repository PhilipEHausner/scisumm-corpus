as indicated by bolding for seven out of eight languages the improvements of the with lp setting are statistically significant with respect to the other models including the no lp setting overall it performs better than the hitherto state of the art feature hmm baseline and better than direct projection when we macro average the accuracy over all languages 
we use the universal pos tagset of petrov et al in our experiments this set c consists of the following coarse grained tags noun nouns verb verbs adj adjectives adv adverbs pron pronouns det determiners adp prepositions or postpositions num numerals conj conjunctions prt particles punc punctuation marks and x a catch all for other categories such as abbreviations or foreign words 
to bridge this gap we consider a practically motivated scenario in which we want to leverage existing resources from a resource rich language like english when building tools for resource poor foreign languages we assume that absolutely no labeled training data is available for the foreign language of interest but that we have access to parallel data with a resource rich language 
for each trigram type x x x in a sequence x x x x x we count how many times that trigram type co occurs with the different instantiations of each concept and compute the point wise mutual information pmi between the two the similarity between two trigram types is given by summing over the pmi values over feature instantiations that they have in common 
since our graph is built from a parallel corpus we can use standard word alignment techniques to align the english sentences de note that many combinations are impossible giving a pmi value of e g when the trigram type and the feature instantiation don t have words in common and their foreign language translations df label propagation in the graph will provide coverage and high recall and we therefore extract only intersected high confidence alignments de f 
