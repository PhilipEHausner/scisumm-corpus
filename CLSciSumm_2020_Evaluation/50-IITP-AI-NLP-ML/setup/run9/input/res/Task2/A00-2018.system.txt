for runs with the generative model based upon markov grammar statistics the first pass uses the same statistics but conditioned only on standard pcfg information 
from our perspective perhaps the two most important numbers to come out of this research are the overall error reduction of over the results in and the intermediateresult improvement of nearly on labeled precision recall due to the simple idea of guessing the head s pre terminal before guessing the head 
a vp coordinate structure is defined here as a constituent with two or more vp children one or more of the constituents comma cc conjp conjunctive phrase and nothing else coordinate np phrases are defined similarly 
we have presented a lexicalized markov grammar parsing model that achieves using the now standard training testing development sections of the penn treebank an average precision recall of on sentences of length and on sentences of length 
now consider computing a conditional probability p a h with a set of features h that connect a to the history h in a log linear model the probability function takes the following form here the ai are weights between negative and positive infinity that indicate the relative importance of a feature the more relevant the feature to the value of the probability the higher the absolute value of the associated a 
in the previous sections we have concentrated on the relation of the parser to a maximumentropy approach the aspect of the parser that is most novel 
that is for all sentences s and all parses r the parser assigns a probability p s r p r the equality holding when we restrict consideration to r whose yield this research was supported in part by nsf grant lis sbr 
