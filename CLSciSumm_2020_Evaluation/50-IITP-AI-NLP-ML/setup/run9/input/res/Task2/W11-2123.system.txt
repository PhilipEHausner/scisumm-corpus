for the perplexity and translation tasks we used srilm to build a gram english language model on million tokens from europarl v koehn and the workshop on machine translation news crawl corpus with duplicate lines removed 
this would result in better rest cost estimation and better pruning in general tighter but well factored integration between the decoder and language model should produce a significant speed improvement 
compared with the widely srilm our is times as fast while using of the mem the structure is a trie with bit level packing sorted records interpolation search and optional quantization aimed lower memory consumption simultaneously uses less memory than the smallest lossless baseline and less cpu than the baseline 
performance improvements transfer to the moses koehn et al cdec dyer et al and joshua li et al translation systems where our code has been integrated 
the probing model was designed to improve upon srilm by using linear probing hash tables though not arranged in a trie allocating memory all at once eliminating the need for full pointers and being easy to compile 
all language model queries issued by machine translation decoders follow a left to right pattern starting with either the begin of sentence token or null context for mid sentence fragments 
while sorted arrays could be used to implement the same data structure as probing effectively making m we abandoned this implementation because it is slower and larger than a trie implementation 
for example syntactic decoders koehn et al dyer et al li et al perform dynamic programming parametrized by both backward and forward looking state 
