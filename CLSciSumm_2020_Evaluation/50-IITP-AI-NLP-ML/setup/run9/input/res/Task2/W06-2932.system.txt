we proceedings of the th conference on computational natural language learning conll x pages new york city june c association for computational linguistics assume that all dependency graphs are trees but may be non projective both of which are true in the data sets we use 
n p allow non projective force projective s a sequential labeling atomic labeling m b include morphology features no morphology features assignment of edge labels instead of individual assignment and a rich feature set that incorporates morphological properties when available 
an exact projective and an approximate non projective parsing algorithm are presented since it is shown that nonprojective dependency parsing becomes np hard when features are extended beyond a single edge 
these weaknesses are not surprising since these decisions encode the more global aspects of sentence structure arrangement of clauses and adverbial dependents in multi clause sentences and prepositional phrase attachment 
the second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph 
this interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non projectivity in freer word order languages 
for score functions we use simple dot products between high dimensional feature representations and a weight vector assuming we have an appropriate feature representation we can find the highest scoring label sequence with viterbi s algorithm 
we have presented results showing that the spanning tree dependency parsing framework of mcdonald et al mcdonald et al b mcdonald and pereira generalizes well to languages other than english 
