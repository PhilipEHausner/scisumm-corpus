formally let el be the number of classification errors of the first second learner on the training data and let eco be the number of unlabeled examples on which the two classifiers disagree 
it may be more realistic to replace the second criteria with a softer one for example blum and mitchell suggest the alternative alternatively if ii and are probabilistic learners it might make sense to encode the second constraint as one of minimizing some measure of the distance between the distributions given by the two learners 
before describing the unsupervised case we first describe the supervised version of the algorithm input to the learning algorithm n labeled examples of the form xi y y is the label of the ith example given that there are k possible labels y is a member of y xi is a set of mi features x xi 
otherwise label the training data with the combined spelling contextual decision list then induce a final decision list from the labeled examples where all rules regardless of strength are added to the decision list 
the following features were used full string x the full string e g for maury cooper full s tring maury cooper contains x if the spelling contains more than one word this feature applies for any words that the string contains e g maury cooper contributes two such features contains maury and contains cooper allcapl this feature appears if the spelling is a single word which is all capitals e g ibm would contribute this feature allcap this feature appears if the spelling is a single word which is all capitals or full periods and contains at least one period 
