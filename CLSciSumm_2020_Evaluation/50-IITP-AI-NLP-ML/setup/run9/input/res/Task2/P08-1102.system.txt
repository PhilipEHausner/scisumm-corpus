at each position i we enumerate all possible word pos pairs by assigning each pos to each possible word formed from the character subsequence spanning length l l min i k k is assigned in all our experiments and ending at position i then we derive all candidate results by attaching each word pos pair p of length l to the tail of each candidate result at the prior position of p position i l and select for position i a n best list of candidate results from all these candidates 
suppose we have n features gj j n coupled with n corresponding weights wj j n each feature gj gives a score gj r to a candidate r then the total score of r is given by the decoding procedure aims to find the candidate r with the highest score while the mission of the training procedure is to tune the weights wj j n to guarantee that the candidate r with the highest score happens to be the best result with a high probability 
on the penn chinese treebank we obtain an error reduction of segmentation and joint segmentation and part of speech tagging over the perceptron only baseline 
if this cascaded linear model were chosen could more accurate generative models lms word pos co occurrence model be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely or by self training on raw corpus in a similar approach to that of mcclosky 
