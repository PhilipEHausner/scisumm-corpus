a lexicalized probabilistic topdown parser is then presented which performs very well in terms of both the accuracy of returned parses and the efficiency with which they are found relative to the best broad coverage statistical parsers 
these context free rules can be interpreted as saying that a nonterminal symbol a expands into one or more either nonterminal or terminal symbols a x xk a sequence of context free rule expansions can be represented in a tree with parents expanding into one or more children below them in the tree 
table reports the word and sentence error rates for five different models i the trigram model that comes with the lattices trained on approximately m words with a vocabulary of ii the best performing model from chelba which was interpolated with the lattice trigram at a iii our parsing model with the same training and vocabulary as the perplexity trials above iv a trigram model with the same training and vocabulary as the parsing model and v no language model at all 
these results achieved using very straightforward conditioning events and considering only the left context are within one to four points of the best published observed running time on section of the penn treebank with the full conditional probability model and beam of using one mhz ultrasparc processor and mb of ram of a sun enterprise accuracies cited above 
also the parser returns a set of candidate parses from which we have been choosing the top ranked if we use an oracle to choose the parse with the highest accuracy from among the candidates which averaged in number per sentence we find an average labeled precision recall of for sentences of length 
