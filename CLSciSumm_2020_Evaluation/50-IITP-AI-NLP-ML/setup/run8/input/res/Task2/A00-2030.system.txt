instead, our parsing algorithm, trained on the upenn treebank, was run on the new york times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.
because generative statistical models had already proven successful for each of the first three stages, we were optimistic that some of their properties — especially their ability to learn from large amounts of data, and their robustness when presented with unexpected inputs — would also benefit semantic analysis.
if we generalize the tree components (constituent labels, words, tags, etc.) and treat them all as simply elements, e, and treat all the conditioning factors as the history, h, we can write:
the detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.
the categories for head constituents, cl„ are predicted based solely on the category of the parent node, cp: modifier constituent categories, cm, are predicted based on their parent node, cp, the head constituent of their parent node, chp, the previously generated modifier, c„,_1, and the head word of their parent, wp.
thus, we did not consider simply adding semantic labels to the existing penn treebank, which is drawn from a single source — the wall street journal — and is impoverished in articles about rocket launches.
currently, the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: since we were interested in exploiting recent advances in parsing, replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.
