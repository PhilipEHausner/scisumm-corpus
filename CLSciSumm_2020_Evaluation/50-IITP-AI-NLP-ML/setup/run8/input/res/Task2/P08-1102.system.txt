at each position i, we enumerate all possible word-pos pairs by assigning each pos to each possible word formed from the character subsequence spanning length l = l. min(i, k) (k is assigned 20 in all our experiments) and ending at position i, then we derive all candidate results by attaching each word-pos pair p (of length l) to the tail of each candidate result at the prior position of p (position iâˆ’l), and select for position i a n-best list of candidate results from all these candidates.
in order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named non-lex using only nonlexical-target features and another named lex using both the two kinds of features.
suppose we have n features gj (j = 1..n) coupled with n corresponding weights wj (j = 1..n), each feature gj gives a score gj(r) to a candidate r, then the total score of r is given by: the decoding procedure aims to find the candidate r* with the highest score: while the mission of the training procedure is to tune the weights wj(j = 1..n) to guarantee that the candidate r with the highest score happens to be the best result with a high probability.
2 segmentation and pos tagging given a chinese character sequence: while the segmentation and pos tagging result can be depicted as: here, ci (i = l.n) denotes chinese character, ti (i = l.m) denotes pos tag, and cl:r (l < r) denotes character sequence ranges from cl to cr.
