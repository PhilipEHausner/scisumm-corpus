anew document tuple w = (w1, ... , wl) is generated by first drawing a tuple-specific topic distribution from an asymmetric dirichlet prior with concentration parameter α and base measure m: then, for each language l, a latent topic assignment is drawn for each token in that language: finally, the observed tokens are themselves drawn using the language-specific topic parameters: wl ∼ p(wl  |zl,φl) = 11n φlwl |zl .
topic models have been used for analyzing topic trends in research literature (mann et al., 2006; hall et al., 2008), inferring captions for images (blei and jordan, 2003), social network analysis in email (mccallum et al., 2005), and expanding queries with topically related words in information retrieval (wei and croft, 2006).
although the pltm is clearly not a substitute for a machine translation system—it has no way to represent syntax or even multi-word phrases—it is clear from the examples in figure 2 that the sets of high probability words in different languages for a given topic are likely to include translations.
although we find that if wikipedia contains an article on a particular subject in some language, the article will tend to be topically similar to the articles about that subject in other languages, we also find that across the whole collection different languages emphasize topics to different extents.
we demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the european parliament (in eleven languages) and a collection of wikipedia articles (in twelve languages).
a recent extended abstract, developed concurrently by ni et al. (ni et al., 2009), discusses a multilingual topic model similar to the one presented here.
