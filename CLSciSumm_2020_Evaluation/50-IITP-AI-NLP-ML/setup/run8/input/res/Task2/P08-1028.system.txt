our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).
now, if we assume that p lies in the same space as u and v, avoiding the issues of dimensionality associated with tensor products, and that f is a linear function, for simplicity, of the cartesian product of u and v, then we generate a class of additive models: where a and b are matrices which determine the contributions made by u and v to the product p. in contrast, if we assume that f is a linear function of the tensor product of u and v, then we obtain multiplicative models: where c is a tensor of rank 3, which projects the tensor product of u and v onto the space of p. further constraints can be introduced to reduce the free parameters in these models.
examples include automatic thesaurus extraction (grefenstette, 1994), word sense discrimination (sch¨utze, 1998) and disambiguation (mccarthy et al., 2004), collocation extraction (schone and jurafsky, 2001), text segmentation (choi et al., 2001) , and notably information retrieval (salton et al., 1975).
the merits of different approaches are illustrated with a few hand picked examples and parameter values and large scale evaluations are uniformly absent (see frank et al. (2007) for a criticism of kintsch’s (2001) evaluation standards).
it was not the sales manager who hit the bottle that day, but the office worker with the serious drinking problem. b.
the projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.
