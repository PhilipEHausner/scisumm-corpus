to set β, we used the same criterion as for α, over a dev corpus: the map combination was used for tm probabilities only, in part due to a technical difficulty in formulating coherent counts when using standard lm smoothing techniques (kneser and ney, 1995).3 motivated by information retrieval, a number of approaches choose “relevant” sentence pairs from out by matching individual source sentences from in (hildebrand et al., 2005; l¨u et al., 2007), or individual target hypotheses (zhao et al., 2004).
moving beyond directly related work, major themes in smt adaptation include the ir (hildebrand et al., 2005; l¨u et al., 2007; zhao et al., 2004) and mixture (finch and sumita, 2008; foster and kuhn, 2007; koehn and schroeder, 2007; l¨u et al., 2007) approaches for lms and tms described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (bertoldi and federico, 2009; ueffing et al., 2007; schwenk and senellart, 2009).
experiments are presented in section 4.
this highly effective approach is not directly applicable to the multinomial models used for core smt components, which have no natural method for combining split features, so we rely on an instance-weighting approach (jiang and zhai, 2007) to downweight domain-specific examples in out.
a final alternate approach would be to combine weighted joint frequencies rather than conditional estimates, ie: ci(s, t) + w,\(s, t)co(, s, t), suitably normalized.5 such an approach could be simulated by a map-style combination in which separate 0(t) values were maintained for each t. this would make the model more powerful, but at the cost of having to learn to downweight out separately for each t, which we suspect would require more training data for reliable performance.
