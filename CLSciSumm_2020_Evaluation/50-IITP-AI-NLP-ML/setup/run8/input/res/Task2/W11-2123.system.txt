for example, syntactic decoders (koehn et al., 2007; dyer et al., 2010; li et al., 2009) perform dynamic programming parametrized by both backward- and forward-looking state.
for the perplexity and translation tasks, we used srilm to build a 5-gram english language model on 834 million tokens from europarl v6 (koehn, 2005) and the 2011 workshop on machine translation news crawl corpus with duplicate lines removed.
this would result in better rest cost estimation and better pruning.10 in general, tighter, but well factored, integration between the decoder and language model should produce a significant speed improvement.
compared with the widely- srilm, our is 2.4 times as fast while using 57% of the mem- the structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less cpu than the baseline.
the probing model was designed to improve upon srilm by using linear probing hash tables (though not arranged in a trie), allocating memory all at once (eliminating the need for full pointers), and being easy to compile.
lossy compressed models randlm (talbot and osborne, 2007) and sheffield (guthrie and hepple, 2010) offer better memory consumption at the expense of cpu and accuracy.
all language model queries issued by machine translation decoders follow a left-to-right pattern, starting with either the begin of sentence token or null context for mid-sentence fragments.
performance improvements transfer to the moses (koehn et al., 2007), cdec (dyer et al., 2010), and joshua (li et al., 2009) translation systems where our code has been integrated.
