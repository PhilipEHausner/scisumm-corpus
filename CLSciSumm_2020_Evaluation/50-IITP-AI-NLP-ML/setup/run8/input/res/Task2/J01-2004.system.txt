these results, achieved using very straightforward conditioning events and considering only the left context, are within one to four points of the best published observed running time on section 23 of the penn treebank, with the full conditional probability model and beam of 10-11, using one 300 mhz ultrasparc processor and 256mb of ram of a sun enterprise 450. accuracies cited above.'
evaluation is carried out on a hand-parsed test corpus, and the manual parses are treated as correct.
if the left-hand side of the production is a pos, then the algorithm takes the right branch of the decision tree, and returns (at level 4) the pos of the closest c-commanding lexical head to a, which it finds by walking the parse tree; if the left-hand side of the rule is not a pos, then the algorithm returns (at level 4) the closest sibling to the left of the parent of constituent (a).
a parser that is not left to right, but which has rooted derivations, e.g., a headfirst parser, will be able to calculate generative joint probabilities for entire strings; however, it will not be able to calculate probabilities for each word conditioned on previously generated words, unless each derivation generates the words in the string in exactly the same order.
table 5 reports the word and sentence error rates for five different models: (i) the trigram model that comes with the lattices, trained on approximately 40m words, with a vocabulary of 20,000; (ii) the best-performing model from chelba (2000), which was interpolated with the lattice trigram at a -= 0.4; (iii) our parsing model, with the same training and vocabulary as the perplexity trials above; (iv) a trigram model with the same training and vocabulary as the parsing model; and (v) no language model at all.
