we propose a cascaded linear model for joint chinese word segmentation and partof-speech tagging.
as shown in figure 1, the character-based perceptron is used as the inside-layer linear model and sends its output to the outside-layer.
the perceptron has been used in many nlp tasks, such as pos tagging (collins, 2002), chinese word segmentation (ng and low, 2004; zhang and clark, 2007) and so on.
as all the sub-models, including the perceptron, are regarded as separate features of the outside-layer linear model, we can train them respectively with special algorithms.
the second was conducted on the penn chinese treebank 5.0 (ctb5.0) to test the performance of the cascaded model on segmentation and joint s&t.
besides this perceptron, other sub-models are trained and used as additional features of the outside-layer linear model.
on the penn chinese treebank 5.0, we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline.
we find that the cascaded model achieves a f-measure increment of about 0.5 points on segmentation and about 0.9 points on joint s&t, over the perceptron-only model pos+.
finally, the word count penalty gives improvement to the cascaded model, 0.13 points on segmentation and 0.16 points on joint s&t.
as the next step, a group of experiments were conducted to investigate how well the cascaded linear model performs.
to cope with this problem, we propose a cascaded linear model inspired by the log-linear model (och and ney, 2004) widely used in statistical machine translation to incorporate different kinds of knowledge sources.
according to ng and low (2004), the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: we can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.
