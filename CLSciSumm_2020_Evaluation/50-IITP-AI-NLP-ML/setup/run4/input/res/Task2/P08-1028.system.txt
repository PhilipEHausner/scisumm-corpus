importantly, additive models capture composition by considering all vector components representing the meaning of the verb and its subject, whereas multiplicative models consider a subset, namely non-zero components.
in this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.
following previous work (bullinaria and levy, 2007), we optimized its parameters on a word-based semantic similarity task.
our results show that the multiplicative models are superior and correlate significantly with behavioral data.
computational models of semantics which use symbolic logic representations (montague, 1974) can account naturally for the meaning of phrases or sentences.
experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments.
although we have presented multiplicative and additive models separately, there is nothing inherent in our formulation that disallows their combination.
the appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar (harris, 1968).
also note that in contrast to the combined model, the multiplicative model does not have any free parameters and hence does not require optimization for this particular task.
the resulting vector is sparser but expresses more succinctly the meaning of the predicate-argument structure, and thus allows semantic similarity to be modelled more accurately.
relaxing the assumption of symmetry in the case of the simple additive model produces a model which weighs the contribution of the two components differently: this allows additive models to become more syntax aware, since semantically important constituents can participate more actively in the composition.
