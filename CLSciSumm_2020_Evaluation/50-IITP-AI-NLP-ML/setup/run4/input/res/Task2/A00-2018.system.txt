we created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.
the results of [13] achieved by combining the aforementioned three-best parsers also suggest that the limit on tree-bank trained parsers is much higher than previously thought.
following [5,10], our parser is based upon a probabilistic generative model.
for runs with the generative model based upon markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard pcfg information.
that the previous three best parsers on this test [5,9,17] all perform within a percentage point of each other, despite quite different basic mechanisms, led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training, and to conjecture that perhaps we were at it.
this corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of collins [9].
so, for example, in a second-order markov pcfg, l2 would be conditioned on l1 and m. in our complete model, of course, the probability of each label in the expansions is also conditioned on other material as specified in equation 1, e.g., p(e t, h, h).
however, collins in [10] does not stress the decision to guess the head's pre-terminal first, and it might be lost on the casual reader.
we present a new parser for parsing down to penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less, and for of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the wall street journal treebank.
