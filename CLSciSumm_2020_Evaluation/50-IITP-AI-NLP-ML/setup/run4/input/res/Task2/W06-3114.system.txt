sentences and systems were randomly selected and randomly shuffled for presentation.
following this method, we repeatedly — say, 1000 times — sample sets of sentences from the output of each system, measure their bleu score, and use these 1000 bleu scores as basis for estimating a confidence interval.
see figure 3 for a screenshot of the evaluation tool.
however, a recent study (callison-burch et al., 2006), pointed out that this correlation may not always be strong.
let say, if we find one system doing better on 20 of the blocks, and worse on 80 of the blocks, is it significantly worse?
in addition to the europarl test set, we also collected 29 editorials from the project syndicate website2, which are published in all the four languages of the shared task.
also, the argument has been made that machine translation performance should be evaluated via task-based evaluation metrics, i.e. how much it assists performing a useful task, such as supporting human translators or aiding the analysis of texts.
we confirm the finding by callison-burch et al. (2006) that the rule-based system of systran is not adequately appreciated by bleu.
given the closeness of most systems and the wide over-lapping confidence intervals it is hard to make strong statements about the correlation between human judgements and automatic scoring methods such as bleu.
however, ince we extracted the test corpus automatically from web sources, the reference translation was not always accurate — due to sentence alignment errors, or because translators did not adhere to a strict sentence-by-sentence translation (say, using pronouns when referring to entities mentioned in the previous sentence).
