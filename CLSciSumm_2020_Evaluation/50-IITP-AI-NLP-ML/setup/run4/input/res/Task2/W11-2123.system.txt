time for moses itself to load, including loading the language model and phrase table, is included.
this is most severe with randlm in the multi-threaded case, where each thread keeps a separate cache, exceeding the original model size.
irstlm (federico et al., 2008) is an open-source toolkit for building and querying language models.
while sorted arrays could be used to implement the same data structure as probing, effectively making m = 1, we abandoned this implementation because it is slower and larger than a trie implementation.
the binary language model from section 5.2 and text phrase table were forced into disk cache before each run.
the cost of storing these averages, in bits, is because there are comparatively few unigrams, we elected to store them byte-aligned and unquantized, making every query faster.
time starts when moses is launched and therefore includes model loading time.
this has the effect of randomly permuting vocabulary identifiers, meeting the requirements of interpolation search when vocabulary identifiers are used as keys.
unlike germann et al. (2009), we chose a model size so that all benchmarks fit comfortably in main memory.
memory usage is likely much lower than ours. fthe original paper (germann et al., 2009) provided only 2s of query timing and compared with sri when it exceeded available ram.
later, berkeleylm (pauls and klein, 2011) described ideas similar to ours.
compared with the widely- srilm, our is 2.4 times as fast while using 57% of the mem- the structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less cpu than the baseline.
