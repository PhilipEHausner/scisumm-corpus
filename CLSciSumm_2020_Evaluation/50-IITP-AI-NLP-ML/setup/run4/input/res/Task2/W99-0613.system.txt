adaboost finds a weighted combination of simple (weak) classifiers, where the weights are chosen to minimize a function that bounds the classification error on a set of training examples.
a spelling rule might be a simple look-up for the string (e.g., a rule that honduras is a location) or a rule that looks at words within a string (e.g., a rule that any string containing mr. is a person).
the first method builds on results from (yarowsky 95) and (blum and mitchell 98).
the second algorithm extends ideas from boosting algorithms, designed for supervised learning tasks, to the framework suggested by (blum and mitchell 98).
the following features were used: full-string=x the full string (e.g., for maury cooper, full- s tring=maury_cooper). contains(x) if the spelling contains more than one word, this feature applies for any words that the string contains (e.g., maury cooper contributes two such features, contains (maury) and contains (cooper) . allcapl this feature appears if the spelling is a single word which is all capitals (e.g., ibm would contribute this feature). allcap2 this feature appears if the spelling is a single word which is all capitals or full periods, and contains at least one period.
recent results (e.g., (yarowsky 95; brill 95; blum and mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.
in the cotraining case, (blum and mitchell 98) argue that the task should be to induce functions ii and f2 such that so ii and 12 must (1) correctly classify the labeled examples, and (2) must agree with each other on the unlabeled examples.
