feature weights were set using och’s mert algorithm (och, 2003).
for developers of statistical machine translation (smt) systems, an additional complication is the heterogeneous nature of smt components (word-alignment model, language model, translation model, etc.
for comparison to information-retrieval inspired baselines, eg (l¨u et al., 2007), we select sentences from out using language model perplexities from in.
however, for multinomial models like our lms and tms, there is a one to one correspondence between instances and features, eg the correspondence between a phrase pair (s, t) and its conditional multinomial probability p(s1t).
there has also been some work on adapting the word alignment model prior to phrase extraction (civera and juan, 2007; wu et al., 2005), and on dynamically choosing a dev set (xu et al., 2007).
this best instance-weighting model beats the equivalant model without instance weights by between 0.6 bleu and 1.8 bleu, and beats the log-linear baseline by a large margin.
we used a standard one-pass phrase-based system (koehn et al., 2003), with the following features: relative-frequency tm probabilities in both directions; a 4-gram lm with kneser-ney smoothing; word-displacement distortion model; and word count.
standard smt systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.
moving beyond directly related work, major themes in smt adaptation include the ir (hildebrand et al., 2005; l¨u et al., 2007; zhao et al., 2004) and mixture (finch and sumita, 2008; foster and kuhn, 2007; koehn and schroeder, 2007; l¨u et al., 2007) approaches for lms and tms described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (bertoldi and federico, 2009; ueffing et al., 2007; schwenk and senellart, 2009).
