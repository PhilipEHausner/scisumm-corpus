second, we treat the projected labels as features in an unsupervised model (§5), rather than using them directly for supervised training.
our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised pos tagging models.
our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages.
the supervised pos tagging accuracies (on this tagset) are shown in the last row of table 2.
unfortunately, the best completely unsupervised english pos tagger (that does not make use of a tagging dictionary) reaches only 76.1% accuracy (christodoulopoulos et al., 2010), making its practical usability questionable at best.
given the bilingual graph described in the previous section, we can use label propagation to project the english pos labels to the foreign language.
because we don’t have a separate development set, we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3.
to establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (§3.3).3 since we have no labeled foreign data, our goal is to project syntactic information from the english side to the foreign side.
to bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like english) when building tools for resource-poor foreign languages.1 we assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.
