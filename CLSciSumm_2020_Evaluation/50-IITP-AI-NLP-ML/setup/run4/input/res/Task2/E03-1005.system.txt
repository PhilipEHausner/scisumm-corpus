but even with cross-validation, ml-dop is outperformed by the much simpler dop1 model on both the atis and ovis treebanks (bod 2000b).
as our second experimental goal, we compared the models sl-dop and ls-dop explained in section 3.2.
goodman's main theorem is that this construction produces pcfg derivations isomorphic to dop derivations with equal probability.
collins & duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with dop1's subtrees, reporting a 5.1% relative reduction in error rate over the model in collins (1999) on the wsj.
in the following section first results of sl-dop and ls-dop with a compact pcfg-reduction. we will see that our new definition of best parse tree also outperforms the best results obtained in bod (2001).
in this paper, we will test a simple extension of goodman's compact pcfg-reduction of dop which has the same property as the normalization proposed in bod (2001) in that it assigns roughly equal weight to each node in the training data.
while sl-dop and ls-dop have been compared before in bod (2002), especially in the context of musical parsing, this paper presents the the dop approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.
together with a pcfgreduction of dop we obtain improved accuracy and efficiency on the wall street journal treebank our results show an 11% relative reduction in error rate over previous models, and an average processing time of 3.6 seconds per wsj sentence.
