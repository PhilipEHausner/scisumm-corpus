we plan to explore more powerful techniques for exploiting the diversity of parsing methods.
our original hope in combining these parsers is that their errors are independently distributed.
we used these three parsers to explore parser combination techniques.
we used section 23 as the development set for our combining techniques, and section 22 only for final testing.
finally we show the combining techniques degrade very little when a poor parser is added to the set.
we then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.
the combining algorithm is presented with the candidate parses and asked to choose which one is best.
the counts represent portions of the approximately 44000 constituents hypothesized by the parsers in the development set.
in equations 1 through 3 we develop the model for constructing our parse using naïve bayes classification.
we have presented two general approaches to studying parser combination: parser switching and parse hybridization.
for example, one parser could be more accurate at predicting noun phrases than the other parsers.
the development of a naïve bayes classifier involves learning how much each parser should be trusted for the decisions it makes.
these two principles guide experimentation in this framework, and together with the evaluation measures help us decide which specific type of substructure to combine.
the precision and recall measures (described in more detail in section 3) used in evaluating treebank parsing treat each constituent as a separate entity, a minimal unit of correctness.
the bayes models were able to achieve significantly higher precision than their non-parametric counterparts.
