for each language under consideration, petrov et al. (2011) provide a mapping a from the fine-grained language specific pos tags in the foreign treebank to the universal pos tags.
our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised pos tagging models.
we use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (berg- kirkpatrick et al., 2010).
for each trigram type x2 x3 x4 in a sequence x1 x2 x3 x4 x5, we count how many times that trigram type co-occurs with the different instantiations of each concept, and compute the point-wise mutual information (pmi) between the two.5 the similarity between two trigram types is given by summing over the pmi values over feature instantiations that they have in common.
second, we treat the projected labels as features in an unsupervised model (ยง5), rather than using them directly for supervised training.
across eight european languages, our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline, and 16.7% over vanilla hidden markov models induced with the expectation maximization algorithm.
our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages.
we describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language.
the supervised pos tagging accuracies (on this tagset) are shown in the last row of table 2.
