the three parsers were trained and tuned by their creators on various sections of the wsj portion of the penn treebank, leaving only sections 22 and 23 completely untouched during the development of any of the parsers.
the combining algorithm is presented with the candidate parses and asked to choose which one is best.
in the interest of testing the robustness of these combining techniques, we added a fourth, simple nonlexicalized pcfg parser.
we plan to explore more powerful techniques for exploiting the diversity of parsing methods.
it is the performance we could achieve if an omniscient observer told us which parser to pick for each of the sentences.
the corpus-based statistical parsing community has many fast and accurate automated parsing systems, including systems produced by collins (1997), charniak (1997) and ratnaparkhi (1997).
in equations 1 through 3 we develop the model for constructing our parse using naïve bayes classification.
the entries in this table can be compared with those of table 3 to see how the performance of the combining techniques degrades in the presence of an inferior parser.
our original hope in combining these parsers is that their errors are independently distributed.
these three parsers have given the best reported parsing results on the penn treebank wall street journal corpus (marcus et al., 1993).
the development of a naïve bayes classifier involves learning how much each parser should be trusted for the decisions it makes.
furthermore, we know one of the original parses will be the hypothesized parse, so the direct method of determining which one is best is to compute the probability of each of the candidate parses using the probabilistic model we developed in section 2.1.
