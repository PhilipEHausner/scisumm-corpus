anew document tuple w = (w1, ... , wl) is generated by first drawing a tuple-specific topic distribution from an asymmetric dirichlet prior with concentration parameter α and base measure m: then, for each language l, a latent topic assignment is drawn for each token in that language: finally, the observed tokens are themselves drawn using the language-specific topic parameters: wl ∼ p(wl  |zl,φl) = 11n φlwl |zl .
in this paper, we present the polylingual topic model (pltm).
topic models are a useful tool for analyzing large text collections, but have previously been applied in only monolingual, or at most bilingual, contexts.
the polylingual topic model (pltm) is an extension of latent dirichlet allocation (lda) (blei et al., 2003) for modeling polylingual document tuples.
bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the hm-bitam model (zhao and xing, 2007).
outside of the field of topic modeling, kawaba et al. (kawaba et al., 2008) use a wikipedia-based model to perform sentiment analysis of blog posts.
in finding a lowdimensional semantic representation, topic models deliberately smooth over much of the variation present in language.
both of these translation-focused topic models infer word-to-word alignments as part of their inference procedures, which would become exponentially more complex if additional languages were added.
maximum topic probability in document although the posterior distribution over topics for each tuple is not concentrated on one topic, it is worth checking that this is not simply because the model is assigning a single topic to the 1we use the r density function. tokens in each of the languages.
