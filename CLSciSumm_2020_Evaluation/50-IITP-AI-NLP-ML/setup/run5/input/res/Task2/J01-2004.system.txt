the paper first introduces key notions in language modeling and probabilistic parsing, and briefly reviews some previous approaches to using syntactic structure for language modeling.
the trigram model was also trained on sections 00-20 of the c&j corpus.
in the event that no complete parse is found, the highest initially ranked parse on the last nonempty priority queue is returned.
a left-toright parser whose derivations are not rooted, i.e., with derivations that can consist of disconnected tree fragments, such as an lr or shift-reduce parser, cannot incrementally calculate the probability of each prefix string being generated by the probabilistic grammar, because their derivations include probability mass from unrooted structures.
table 5 reports the word and sentence error rates for five different models: (i) the trigram model that comes with the lattices, trained on approximately 40m words, with a vocabulary of 20,000; (ii) the best-performing model from chelba (2000), which was interpolated with the lattice trigram at a -= 0.4; (iii) our parsing model, with the same training and vocabulary as the perplexity trials above; (iv) a trigram model with the same training and vocabulary as the parsing model; and (v) no language model at all.
we will then present empirical results in two domains: one to compare with previous work in the parsing literature, and the other to compare with previous work using parsing for language modeling for speech recognition, in particular with the chelba and jelinek results mentioned above.
we will call the manual parse gold and the parse that the parser returns test.
if p is the probability of the highest-ranked analysis on h1Â±1, then another analysis is discarded if its probability falls below pf(-y, ih,+11), where -y is an initial parameter, which we call the base beam factor.
