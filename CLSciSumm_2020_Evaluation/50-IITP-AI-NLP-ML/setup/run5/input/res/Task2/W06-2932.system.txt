an exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes np-hard when features are extended beyond a single edge.
furthermore, it made the system homogeneous in terms of learning algorithms since that is what is used to train our unlabeled parser (mcdonald and pereira, 2006).
we proceedings of the 10th conference on computational natural language learning (conll-x), pages 216–220, new york city, june 2006. c�2006 association for computational linguistics assume that all dependency graphs are trees but may be non-projective, both of which are true in the data sets we use.
however, the parser is fundamentally limited by the scope of local factorizations that make inference tractable.
that work extends the maximum spanning tree dependency parsing framework (mcdonald et al., 2005a; mcdonald et al., 2005b) to incorporate features over multiple edges in the dependency graph.
n/p: allow non-projective/force projective, s/a: sequential labeling/atomic labeling, m/b: include morphology features/no morphology features. assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available.
the first stage based on the unlabeled dependency parsing models described by mcdonald and pereira (2006) augmented with morphological features for a subset of the languages.
dependency graphs also encode much of the deep syntactic information needed for further processing.
these results show that the discriminative spanning tree parsing framework (mcdonald et al., 2005b; mcdonald and pereira, 2006) is easily adapted across all these languages.
this interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages.
