sentences and systems were randomly selected and randomly shuffled for presentation.
automatic scores are computed on a larger tested than manual scores (3064 sentences vs. 300–400 sentences). collected manual judgements, we do not necessarily have the same sentence judged for both systems (judges evaluate 5 systems out of the 8–10 participating systems).
the manual evaluation of scoring translation on a graded scale from 1–5 seems to be very hard to perform.
hence, we use the bootstrap resampling method described by koehn (2004).
while automatic measures are an invaluable tool for the day-to-day development of machine translation systems, they are only a imperfect substitute for human assessment of translation quality, or as the acronym bleu puts it, a bilingual evaluation understudy.
however, ince we extracted the test corpus automatically from web sources, the reference translation was not always accurate — due to sentence alignment errors, or because translators did not adhere to a strict sentence-by-sentence translation (say, using pronouns when referring to entities mentioned in the previous sentence).
this revealed interesting clues about the properties of automatic and manual scoring.
however, a recent study (callison-burch et al., 2006), pointed out that this correlation may not always be strong.
another way to view the judgements is that they are less quality judgements of machine translation systems per se, but rankings of machine translation systems.
in addition to the europarl test set, we also collected 29 editorials from the project syndicate website2, which are published in all the four languages of the shared task.
at the very least, we are creating a data resource (the manual annotations) that may the basis of future research in evaluation metrics.
