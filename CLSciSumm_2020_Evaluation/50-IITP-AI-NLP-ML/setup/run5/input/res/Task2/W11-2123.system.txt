for even larger models, storing counts (talbot and osborne, 2007; pauls and klein, 2011; guthrie and hepple, 2010) is a possibility.
all language model queries issued by machine translation decoders follow a left-to-right pattern, starting with either the begin of sentence token or null context for mid-sentence fragments.
language models are widely applied in natural language processing, and applications such as machine translation make very frequent queries.
time for moses itself to load, including loading the language model and phrase table, is included.
we present kenlm, a library that implements two data structures for efficient language model queries, reducing both time and costs.
lossy compressed models randlm (talbot and osborne, 2007) and sheffield (guthrie and hepple, 2010) offer better memory consumption at the expense of cpu and accuracy.
for the perplexity and translation tasks, we used srilm to build a 5-gram english language model on 834 million tokens from europarl v6 (koehn, 2005) and the 2011 workshop on machine translation news crawl corpus with duplicate lines removed.
this would result in better rest cost estimation and better pruning.10 in general, tighter, but well factored, integration between the decoder and language model should produce a significant speed improvement.
the binary language model from section 5.2 and text phrase table were forced into disk cache before each run.
this is most severe with randlm in the multi-threaded case, where each thread keeps a separate cache, exceeding the original model size.
this differs from other implementations (stolcke, 2002; pauls and klein, 2011) that use hash tables as nodes in a trie, as explained in the next section.
