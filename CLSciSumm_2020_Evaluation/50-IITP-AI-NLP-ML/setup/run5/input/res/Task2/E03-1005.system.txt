the dop model, on the other hand, was the first model (to the best of our knowledge) that proposed not to train a predefined grammar on a corpus, but to directly use corpus fragments as a grammar.
but even with cross-validation, ml-dop is outperformed by the much simpler dop1 model on both the atis and ovis treebanks (bod 2000b).
as our second experimental goal, we compared the models sl-dop and ls-dop explained in section 3.2.
this paper also re-affirmed that the coarsegrained approach of using all subtrees from a treebank outperforms the fine-grained approach of specifically modeling lexical-syntactic depen dencies (as e.g. in collins 1999 and charniak 2000).
dop1 parses new input by combining treebanksubtrees by means of a leftmost node-subsitution operation, indicated as 0.
thus the major innovations of dop are: 2. the use of arbitrarily large fragments rather than restricted ones both have gained or are gaining wide usage, and are also becoming relevant for theoretical linguistics (see bod et al. 2003a).
together with a pcfgreduction of dop we obtain improved accuracy and efficiency on the wall street journal treebank our results show an 11% relative reduction in error rate over previous models, and an average processing time of 3.6 seconds per wsj sentence.
goodman's main theorem is that this construction produces pcfg derivations isomorphic to dop derivations with equal probability.
this paper showed that a pcfg-reduction of dop in combination with a new notion of the best parse tree results in fast processing times and very competitive accuracy on the wall street journal treebank.
goodman then shows by simple induction that subderivations headed by a with external nonterminals at the roots and leaves, internal nonterminals elsewhere have probability 1/a.
