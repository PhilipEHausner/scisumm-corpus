in this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.
although the composition model in (5) is commonly used in the literature, from a linguistic perspective, the model in (6) is more appealing.
nlp tasks that could benefit from composition models include paraphrase identification and context-dependent language modeling (coccaro and jurafsky, 1998).
a hypothetical semantic space is illustrated in figure 1.
that day the office manager, who was drinking, hit the problem sales worker with a bottle, but it was not serious.
we formulated composition as a function of two vectors and introduced several models based on addition and multiplication.
importantly, additive models capture composition by considering all vector components representing the meaning of the verb and its subject, whereas multiplicative models consider a subset, namely non-zero components.
following previous work (bullinaria and levy, 2007), we optimized its parameters on a word-based semantic similarity task.
our results show that the multiplicative models are superior and correlate significantly with behavioral data.
it was not the sales manager who hit the bottle that day, but the office worker with the serious drinking problem. b.
our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).
although we have presented multiplicative and additive models separately, there is nothing inherent in our formulation that disallows their combination.
computational models of semantics which use symbolic logic representations (montague, 1974) can account naturally for the meaning of phrases or sentences.
experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments.
