following this method we repeatedly say times sample sets of sentences from the output of each system measure their bleu score and use these bleu scores as basis for estimating a confidence interval 
we can check what the consequences of less manual annotation of results would have been with half the number of manual judgements we can distinguish about of the systems less 
let say if we find one system doing better on of the blocks and worse on of the blocks is it significantly worse 
see figure for a screenshot of the evaluation tool 
however a recent study callison burch et al pointed out that this correlation may not always be strong 
given the closeness of most systems and the wide over lapping confidence intervals it is hard to make strong statements about the correlation between human judgements and automatic scoring methods such as bleu 
hence we use the bootstrap resampling method described by koehn 
while the bootstrap method is slightly more sensitive it is very much in line with the sign test on text blocks 
in addition to the europarl test set we also collected editorials from the project syndicate website which are published in all the four languages of the shared task 
we confirm the finding by callison burch et al that the rule based system of systran is not adequately appreciated by bleu 
the bleu score has been shown to correlate well with human judgement when statistical machine translation systems are compared doddington przybocki li 
