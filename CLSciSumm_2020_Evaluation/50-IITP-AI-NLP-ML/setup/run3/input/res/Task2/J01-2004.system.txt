a lexicalized probabilistic topdown parser is then presented which performs very well in terms of both the accuracy of returned parses and the efficiency with which they are found relative to the best broad coverage statistical parsers 
the parser error parser coverage and the uninterpolated model perplexity a all suffered substantially from a narrower search but the interpolated perplexity remained quite good even at the extremes 
in addition we show the average number of rule expansions considered per word that is the number of rule expansions for which a probability was calculated see roark and charniak and the average number of analyses advanced to the next priority queue per word 
this contrasts with our perplexity results reported above as well as with the recognition experiments in chelba where the best results resulted from interpolated models 
also the parser returns a set of candidate parses from which we have been choosing the top ranked if we use an oracle to choose the parse with the highest accuracy from among the candidates which averaged in number per sentence we find an average labeled precision recall of for sentences of length 
it was selected with the goal of high parser accuracy but in this new domain parser accuracy is a secondary measure of performance 
these context free rules can be interpreted as saying that a nonterminal symbol a expands into one or more either nonterminal or terminal symbols a x xk a sequence of context free rule expansions can be represented in a tree with parents expanding into one or more children below them in the tree 
