adaboost finds a weighted combination of simple weak classifiers where the weights are chosen to minimize a function that bounds the classification error on a set of training examples 
in addition to a heuristic based on decision list learning we also presented a boosting like framework that builds on ideas from blum and mitchell 
adaboost is given access to a weak learning algorithm which accepts as input the training examples along with a distribution over the instances 
the approach builds from an initial seed set for a category and is quite similar to the decision list approach described in yarowsky 
the task can be considered to be one component of the muc muc named entity task the other task is that of segmentation i e pulling possible people places and locations from text before sending them to the classifier 
formally let el be the number of classification errors of the first second learner on the training data and let eco be the number of unlabeled examples on which the two classifiers disagree 
the second algorithm builds on a boosting algorithm called adaboost freund and schapire schapire and singer 
in this work we extended the adaboost mh schapire and singer algorithm to the cotraining case 
several extensions of adaboost for multiclass problems have been suggested freund and schapire schapire and singer 
the following features were used full string x the full string e g for maury cooper full s tring maury cooper contains x if the spelling contains more than one word this feature applies for any words that the string contains e g maury cooper contributes two such features contains maury and contains cooper allcapl this feature appears if the spelling is a single word which is all capitals e g ibm would contribute this feature allcap this feature appears if the spelling is a single word which is all capitals or full periods and contains at least one period 
