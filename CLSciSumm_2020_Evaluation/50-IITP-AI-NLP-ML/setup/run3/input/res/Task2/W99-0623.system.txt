through parser combination we have reduced the precision error rate by and the recall error rate by compared to the best previously published result 
furthermore we know one of the original parses will be the hypothesized parse so the direct method of determining which one is best is to compute the probability of each of the candidate parses using the probabilistic model we developed in section 
the bayes models were able to achieve significantly higher precision than their non parametric counterparts 
the probabilistic version of this procedure is straightforward we once again assume independence among our various member parsers 
it is the performance we could achieve if an omniscient observer told us which parser to pick for each of the sentences 
the precision and recall measures described in more detail in section used in evaluating treebank parsing treat each constituent as a separate entity a minimal unit of correctness 
similarly figures and show how the isolated constituent precision varies by sentence length and the size of the span of the hypothesized constituent 
the maximum precision oracle is an upper bound on the possible gain we can achieve by parse hybridization 
these three parsers have given the best reported parsing results on the penn treebank wall street journal corpus marcus et al 
we used section as the development set for our combining techniques and section only for final testing 
their theoretical finding is simply stated classification error rate decreases toward the noise rate exponentially in the number of independent accurate classifiers 
