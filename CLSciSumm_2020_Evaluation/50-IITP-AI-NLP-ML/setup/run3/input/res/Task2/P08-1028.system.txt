nlp tasks that could benefit from composition models include paraphrase identification and context dependent language modeling coccaro and jurafsky 
we employed leave one out resampling weiss and kulikowski by correlating the data obtained from each participant with the ratings obtained from all other participants 
for example simplistic approaches to handling sentences such as john loves mary and mary loves john typically fail to make valid representations in one of two ways 
in addition bullinaria and levy found that these parameters perform well on a number of other tasks such as the synonymy task from the test ofenglish as a foreign language toefl 
the neighbors kintsch argues can strengthen features of the predicate that are appropriate for the argument of the predication animal stable village gallop jokey horse run unfortunately comparisons across vector composition models have been few and far between in the literature 
importantly additive models capture composition by considering all vector components representing the meaning of the verb and its subject whereas multiplicative models consider a subset namely non zero components 
analysis of similarity ratings the reliability of the collected judgments is important for our evaluation experiments we therefore performed several tests to validate the quality of the ratings 
the projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors 
specifically they belonged to different synsets and were maximally dissimilar as measured by the jiang and conrath measure our initial set of candidate materials consisted of verbs each paired with nouns and landmarks pairs of sentences in total 
