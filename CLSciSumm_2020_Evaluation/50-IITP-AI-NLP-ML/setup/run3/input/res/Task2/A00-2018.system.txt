in the previous sections we have concentrated on the relation of the parser to a maximumentropy approach the aspect of the parser that is most novel 
but let us look at how it works for a particular case in our parsing scheme 
for example in the penn treebank a vp with both main and auxiliary verbs has the structure shown in figure 
in particular we measure labeled precision lp and recall lr average number of crossbrackets per sentence cb percentage of sentences with zero cross brackets ocb and percentage of sentences with cross brackets cb 
that the previous three best parsers on this test all perform within a percentage point of each other despite quite different basic mechanisms led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training and to conjecture that perhaps we were at it 
we created a parser based upon the maximumentropy inspired model of the last section smoothed using standard deleted interpolation 
from our perspective perhaps the two most important numbers to come out of this research are the overall error reduction of over the results in and the intermediateresult improvement of nearly on labeled precision recall due to the simple idea of guessing the head s pre terminal before guessing the head 
for example in a second order markov grammar we conditioned the l label according to the distribution p l i l m t h h 
we have presented a lexicalized markov grammar parsing model that achieves using the now standard training testing development sections of the penn treebank an average precision recall of on sentences of length and on sentences of length 
