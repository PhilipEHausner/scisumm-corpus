for the perplexity and translation tasks we used srilm to build a gram english language model on million tokens from europarl v koehn and the workshop on machine translation news crawl corpus with duplicate lines removed 
for even larger models storing counts talbot and osborne pauls and klein guthrie and hepple is a possibility 
lossy compressed models randlm talbot and osborne and sheffield guthrie and hepple offer better memory consumption at the expense of cpu and accuracy 
performance improvements transfer to the moses koehn et al cdec dyer et al and joshua li et al translation systems where our code has been integrated 
it is generally considered to be fast pauls probabilities and non zero backoffs and klein with a default implementation based on hash tables within each trie node 
the binary language model from section and text phrase table were forced into disk cache before each run 
memory usage is likely much lower than ours fthe original paper germann et al provided only s of query timing and compared with sri when it exceeded available ram 
for example syntactic decoders koehn et al dyer et al li et al perform dynamic programming parametrized by both backward and forward looking state 
this differs from other implementations stolcke pauls and klein that use hash tables as nodes in a trie as explained in the next section 
this technique was introduced by clarkson and rosenfeld and is also implemented by irstlm and berkeleylm s compressed option 
raj and whittaker show that integers in a trie implementation can be compressed substantially 
