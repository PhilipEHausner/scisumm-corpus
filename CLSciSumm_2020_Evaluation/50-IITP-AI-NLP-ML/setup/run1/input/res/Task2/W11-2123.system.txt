performance improvements transfer to the moses (koehn et al., 2007), cdec (dyer et al., 2010), and joshua (li et al., 2009) translation systems where our code has been integrated.
the cost of storing these averages, in bits, is because there are comparatively few unigrams, we elected to store them byte-aligned and unquantized, making every query faster.
for example, syntactic decoders (koehn et al., 2007; dyer et al., 2010; li et al., 2009) perform dynamic programming parametrized by both backward- and forward-looking state.
this has the effect of randomly permuting vocabulary identifiers, meeting the requirements of interpolation search when vocabulary identifiers are used as keys.
the binary language model from section 5.2 and text phrase table were forced into disk cache before each run.
this is most severe with randlm in the multi-threaded case, where each thread keeps a separate cache, exceeding the original model size.
memory usage is likely much lower than ours. fthe original paper (germann et al., 2009) provided only 2s of query timing and compared with sri when it exceeded available ram.
while sorted arrays could be used to implement the same data structure as probing, effectively making m = 1, we abandoned this implementation because it is slower and larger than a trie implementation.
raj and whittaker (2003) show that integers in a trie implementation can be compressed substantially.
time starts when moses is launched and therefore includes model loading time.
irstlm (federico et al., 2008) is an open-source toolkit for building and querying language models.
for randlm and irstlm, the effect of caching can be seen on speed and memory usage.
