for example, in the penn treebank a vp with both main and auxiliary verbs has the structure shown in figure 3.
that the previous three best parsers on this test [5,9,17] all perform within a percentage point of each other, despite quite different basic mechanisms, led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training, and to conjecture that perhaps we were at it.
in particular, we measure labeled precision (lp) and recall (lr), average number of crossbrackets per sentence (cb), percentage of sentences with zero cross brackets (ocb), and percentage of sentences with < 2 cross brackets (2cb).
we created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.
the results of [13] achieved by combining the aforementioned three-best parsers also suggest that the limit on tree-bank trained parsers is much higher than previously thought.
this is in accord with our experience that developmentcorpus results are from 0.3% to 0.5% lower than those obtained on the test corpus.
thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g., whether it is a noun phrase (np), verb-phrase, etc.) and h (c) is the relevant history of c â€” information outside c that our probability model deems important in determining the probability in question.
however, collins in [10] does not stress the decision to guess the head's pre-terminal first, and it might be lost on the casual reader.
