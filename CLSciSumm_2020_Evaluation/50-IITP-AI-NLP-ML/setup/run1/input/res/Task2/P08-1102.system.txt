on the penn chinese treebank 5.0, we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline.
as all the sub-models, including the perceptron, are regarded as separate features of the outside-layer linear model, we can train them respectively with special algorithms.
the second was conducted on the penn chinese treebank 5.0 (ctb5.0) to test the performance of the cascaded model on segmentation and joint s&t.
besides this perceptron, other sub-models are trained and used as additional features of the outside-layer linear model.
the perceptron algorithm introduced into nlp by collins (2002), is a simple but effective discriminative training method.
as shown in figure 1, the character-based perceptron is used as the inside-layer linear model and sends its output to the outside-layer.
since the typical approach of discriminative models treats segmentation as a labelling problem by assigning each character a boundary tag (xue and shen, 2003), joint s&t can be conducted in a labelling fashion by expanding boundary tags to include pos information (ng and low, 2004).
to cope with this problem, we propose a cascaded linear model inspired by the log-linear model (och and ney, 2004) widely used in statistical machine translation to incorporate different kinds of knowledge sources.
we called them non-lexical-target because predications derived from them can predicate without considering the current character c0.
note that the templates of ng and low (2004) have already contained some lexical-target ones.
templates immediately borrowed from ng and low (2004) are listed in the upper column named non-lexical-target.
we found that lex outperforms non-lex with a margin of about 0.002 at each iteration, and its learning curve reaches a tableland at iteration 7.
