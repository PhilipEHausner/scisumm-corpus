moving beyond directly related work, major themes in smt adaptation include the ir (hildebrand et al., 2005; l¨u et al., 2007; zhao et al., 2004) and mixture (finch and sumita, 2008; foster and kuhn, 2007; koehn and schroeder, 2007; l¨u et al., 2007) approaches for lms and tms described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (bertoldi and federico, 2009; ueffing et al., 2007; schwenk and senellart, 2009).
we model po(s|t) using a map criterion over weighted phrase-pair counts: and from the similarity to (5), assuming y = 0, we see that wλ(s, t) can be interpreted as approximating pf(s, t)/po(s, t).
we used a standard one-pass phrase-based system (koehn et al., 2003), with the following features: relative-frequency tm probabilities in both directions; a 4-gram lm with kneser-ney smoothing; word-displacement distortion model; and word count.
however, for multinomial models like our lms and tms, there is a one to one correspondence between instances and features, eg the correspondence between a phrase pair (s, t) and its conditional multinomial probability p(s1t).
for comparison to information-retrieval inspired baselines, eg (l¨u et al., 2007), we select sentences from out using language model perplexities from in.
for developers of statistical machine translation (smt) systems, an additional complication is the heterogeneous nature of smt components (word-alignment model, language model, translation model, etc.
there has also been some work on adapting the word alignment model prior to phrase extraction (civera and juan, 2007; wu et al., 2005), and on dynamically choosing a dev set (xu et al., 2007).
