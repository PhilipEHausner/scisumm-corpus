in finding a lowdimensional semantic representation, topic models deliberately smooth over much of the variation present in language.
topic models have been used for analyzing topic trends in research literature (mann et al., 2006; hall et al., 2008), inferring captions for images (blei and jordan, 2003), social network analysis in email (mccallum et al., 2005), and expanding queries with topically related words in information retrieval (wei and croft, 2006).
we take a simpler approach that is more suitable for topically similar document tuples (where documents are not direct translations of one another) in more than two languages.
in order to make the task more difficult, we train a relatively coarse-grained pltm with 50 topics on the training set.
outside of the field of topic modeling, kawaba et al. (kawaba et al., 2008) use a wikipedia-based model to perform sentiment analysis of blog posts.
an important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct, non-comparable documents in multiple languages.
previous work on bilingual topic modeling has focused on machine translation applications, which rely on sentence-aligned parallel translations.
a recent extended abstract, developed concurrently by ni et al. (ni et al., 2009), discusses a multilingual topic model similar to the one presented here.
we argue that topic modeling is both a useful and appropriate tool for leveraging correspondences between semantically comparable documents in multiple different languages.
when applied to comparable document collections such as wikipedia, pltm supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.
