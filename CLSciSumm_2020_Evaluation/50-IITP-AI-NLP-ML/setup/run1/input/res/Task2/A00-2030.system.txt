in this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (lpcfg-hr) to information extraction.
although mathematically the model predicts tree elements in a top-down fashion, we search the space bottom-up using a chartbased search.
thus, each component of what would be the first three stages of our pipeline was based on the same general class of statistical model.
thus, we did not consider simply adding semantic labels to the existing penn treebank, which is drawn from a single source — the wall street journal — and is impoverished in articles about rocket launches.
because generative statistical models had already proven successful for each of the first three stages, we were optimistic that some of their properties — especially their ability to learn from large amounts of data, and their robustness when presented with unexpected inputs — would also benefit semantic analysis.
given a sentence to be analyzed, the search program must find the most likely semantic and syntactic interpretation.
at each step in the process, a choice is made from a statistical distribution, with the probability of each possible selection dependent on particular features of previously generated elements.
finally, our newly constructed parser, like that of (collins 1997), was based on a generative statistical model.
we have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (lpcfghr) can be used effectively for information extraction.
if we generalize the tree components (constituent labels, words, tags, etc.) and treat them all as simply elements, e, and treat all the conditioning factors as the history, h, we can write:
