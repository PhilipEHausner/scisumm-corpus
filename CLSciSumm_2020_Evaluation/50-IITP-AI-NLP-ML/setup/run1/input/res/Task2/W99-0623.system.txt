the bayes models were able to achieve significantly higher precision than their non-parametric counterparts.
the probabilistic version of this procedure is straightforward: we once again assume independence among our various member parsers.
we used section 23 as the development set for our combining techniques, and section 22 only for final testing.
the precision and recall measures (described in more detail in section 3) used in evaluating treebank parsing treat each constituent as a separate entity, a minimal unit of correctness.
there are simply not enough votes remaining to allow any of the crossing structures to enter the hypothesized constituent set.
when this metric is less than 0.5, we expect to incur more errors' than we will remove by adding those constituents to the parse.
we show the results of three of the experiments we conducted to measure isolated constituent precision under various partitioning schemes.
in equations 1 through 3 we develop the model for constructing our parse using na√Øve bayes classification.
in the cases where isolated constituent precision is larger than 0.5 the affected portion of the hypotheses is negligible.
we have presented two general approaches to studying parser combination: parser switching and parse hybridization.
our original hope in combining these parsers is that their errors are independently distributed.
in each figure the upper graph shows the isolated constituent precision and the bottom graph shows the corresponding number of hypothesized constituents.
we plan to explore more powerful techniques for exploiting the diversity of parsing methods.
the counts represent portions of the approximately 44000 constituents hypothesized by the parsers in the development set.
finally we show the combining techniques degrade very little when a poor parser is added to the set.
