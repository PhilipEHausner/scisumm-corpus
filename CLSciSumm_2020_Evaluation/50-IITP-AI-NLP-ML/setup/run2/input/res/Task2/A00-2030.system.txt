chiba, (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.
although mathematically the model predicts tree elements in a top-down fashion, we search the space bottom-up using a chartbased search.
given a sentence to be analyzed, the search program must find the most likely semantic and syntactic interpretation.
in this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (lpcfg-hr) to information extraction.
for example, an error made during part-of-speech-tagging may cause a future error in syntactic analysis, which may in turn cause a semantic interpretation failure.
thus, we did not consider simply adding semantic labels to the existing penn treebank, which is drawn from a single source — the wall street journal — and is impoverished in articles about rocket launches.
thus, each component of what would be the first three stages of our pipeline was based on the same general class of statistical model.
if we generalize the tree components (constituent labels, words, tags, etc.) and treat them all as simply elements, e, and treat all the conditioning factors as the history, h, we can write:
there is no opportunity for a later stage, such as parsing, to influence or correct an earlier stage such as part-of-speech tagging.
given multiple constituents that cover identical spans in the chart, only those constituents with probabilities within a while our focus throughout the project was on te and tr, we became curious about how well the model did at part-of-speech tagging, syntactic parsing, and at name finding.
