our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the pcfg derivations.
hence, we take the probability of the event fmnh analyzed as rel vb to be this means that we generate f and mnh independently depending on their corresponding pos tags, and the context (as well as the syntactic relation between the two) is modeled via the derivation resulting in a sequence rel vb spanning the form fmnh. based on linear context.
such resources exist for hebrew (itai et al., 2006), but unfortunately use a tagging scheme which is incompatible with the one of the hebrew treebank.s for this reason, we use a data-driven morphological analyzer derived from the training data similar to (cohen and smith, 2007).
both (tsarfaty, 2006; cohen and smith, 2007) have shown that a single integrated framework outperforms a completely streamlined implementation, yet neither has shown a single generative model which handles both tasks.
using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.
morphological disambiguators that consider a token in context (an utterance) and propose the most likely morphological analysis of an utterance (including segmentation) were presented by bar-haim et al. (2005), adler and elhadad (2006), shacham and wintner (2007), and achieved good results (the best segmentation result so far is around 98%).
