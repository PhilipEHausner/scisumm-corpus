we have presented a lexicalized markov grammar parsing model that achieves (using the now standard training/testing/development sections of the penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.
it makes no use of special maximum-entropyinspired features (though their presence made it much easier to perform these experiments), it does not guess the pre-terminal before guessing the lexical head, and it uses a tree-bank grammar rather than a markov grammar.
for example, in the penn treebank a vp with both main and auxiliary verbs has the structure shown in figure 3.
in the previous sections we have concentrated on the relation of the parser to a maximumentropy approach, the aspect of the parser that is most novel.
but let us look at how it works for a particular case in our parsing scheme.
we present a new parser for parsing down to penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less, and for of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the wall street journal treebank.
we created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.
in particular, we measure labeled precision (lp) and recall (lr), average number of crossbrackets per sentence (cb), percentage of sentences with zero cross brackets (ocb), and percentage of sentences with < 2 cross brackets (2cb).
that the previous three best parsers on this test [5,9,17] all perform within a percentage point of each other, despite quite different basic mechanisms, led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training, and to conjecture that perhaps we were at it.
