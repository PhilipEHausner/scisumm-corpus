anew document tuple w = (w1, ... , wl) is generated by first drawing a tuple-specific topic distribution from an asymmetric dirichlet prior with concentration parameter α and base measure m: then, for each language l, a latent topic assignment is drawn for each token in that language: finally, the observed tokens are themselves drawn using the language-specific topic parameters: wl ∼ p(wl  |zl,φl) = 11n φlwl |zl .
topic models have been used for analyzing topic trends in research literature (mann et al., 2006; hall et al., 2008), inferring captions for images (blei and jordan, 2003), social network analysis in email (mccallum et al., 2005), and expanding queries with topically related words in information retrieval (wei and croft, 2006).
although the pltm is clearly not a substitute for a machine translation system—it has no way to represent syntax or even multi-word phrases—it is clear from the examples in figure 2 that the sets of high probability words in different languages for a given topic are likely to include translations.
in order to make the task more difficult, we train a relatively coarse-grained pltm with 50 topics on the training set.
bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the hm-bitam model (zhao and xing, 2007).
outside of the field of topic modeling, kawaba et al. (kawaba et al., 2008) use a wikipedia-based model to perform sentiment analysis of blog posts.
in finding a lowdimensional semantic representation, topic models deliberately smooth over much of the variation present in language.
