the following features were used: full-string=x the full string (e.g., for maury cooper, full- s tring=maury_cooper). contains(x) if the spelling contains more than one word, this feature applies for any words that the string contains (e.g., maury cooper contributes two such features, contains (maury) and contains (cooper) . allcapl this feature appears if the spelling is a single word which is all capitals (e.g., ibm would contribute this feature). allcap2 this feature appears if the spelling is a single word which is all capitals or full periods, and contains at least one period.
in addition to a heuristic based on decision list learning, we also presented a boosting-like framework that builds on ideas from (blum and mitchell 98).
the second algorithm builds on a boosting algorithm called adaboost (freund and schapire 97; schapire and singer 98).
adaboost finds a weighted combination of simple (weak) classifiers, where the weights are chosen to minimize a function that bounds the classification error on a set of training examples.
the key point is that the second constraint can be remarkably powerful in reducing the complexity of the learning problem.
the numbers falling into the location, person, organization categories were 186, 289 and 402 respectively.
the approach builds from an initial seed set for a category, and is quite similar to the decision list approach described in (yarowsky 95).
several extensions of adaboost for multiclass problems have been suggested (freund and schapire 97; schapire and singer 98).
adaboost is given access to a weak learning algorithm, which accepts as input the training examples, along with a distribution over the instances.
