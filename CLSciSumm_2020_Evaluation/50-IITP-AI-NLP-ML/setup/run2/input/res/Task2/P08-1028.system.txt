in this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.
the neighbors, kintsch argues, can ‘strengthen features of the predicate that are appropriate for the argument of the predication’. animal stable village gallop jokey horse 0 6 2 10 4 run 1 8 4 4 0 unfortunately, comparisons across vector composition models have been few and far between in the literature.
nlp tasks that could benefit from composition models include paraphrase identification and context-dependent language modeling (coccaro and jurafsky, 1998).
the merits of different approaches are illustrated with a few hand picked examples and parameter values and large scale evaluations are uniformly absent (see frank et al. (2007) for a criticism of kintsch’s (2001) evaluation standards).
that day the office manager, who was drinking, hit the problem sales worker with a bottle, but it was not serious.
the projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.
an extreme form of this differential in the contribution of constituents is where one of the vectors, say u, contributes nothing at all to the combination: admittedly the model in (8) is impoverished and rather simplistic, however it can serve as a simple baseline against which to compare more sophisticated models.
we employed leave-one-out resampling (weiss and kulikowski, 1991), by correlating the data obtained from each participant with the ratings obtained from all other participants.
a hypothetical semantic space is illustrated in figure 1.
it was not the sales manager who hit the bottle that day, but the office worker with the serious drinking problem. b.
