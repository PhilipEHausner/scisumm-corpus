for even larger models, storing counts (talbot and osborne, 2007; pauls and klein, 2011; guthrie and hepple, 2010) is a possibility.
for the perplexity and translation tasks, we used srilm to build a 5-gram english language model on 834 million tokens from europarl v6 (koehn, 2005) and the 2011 workshop on machine translation news crawl corpus with duplicate lines removed.
performance improvements transfer to the moses (koehn et al., 2007), cdec (dyer et al., 2010), and joshua (li et al., 2009) translation systems where our code has been integrated.
the binary language model from section 5.2 and text phrase table were forced into disk cache before each run.
it is generally considered to be fast (pauls 29 − 1 probabilities and 2' − 2 non-zero backoffs. and klein, 2011), with a default implementation based on hash tables within each trie node.
for example, syntactic decoders (koehn et al., 2007; dyer et al., 2010; li et al., 2009) perform dynamic programming parametrized by both backward- and forward-looking state.
all language model queries issued by machine translation decoders follow a left-to-right pattern, starting with either the begin of sentence token or null context for mid-sentence fragments.
this technique was introduced by clarkson and rosenfeld (1997) and is also implemented by irstlm and berkeleylm’s compressed option.
time starts when moses is launched and therefore includes model loading time.
we present kenlm, a library that implements two data structures for efficient language model queries, reducing both time and costs.
lossy compressed models randlm (talbot and osborne, 2007) and sheffield (guthrie and hepple, 2010) offer better memory consumption at the expense of cpu and accuracy.
