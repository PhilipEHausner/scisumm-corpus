a lexicalized probabilistic topdown parser is then presented which performs very well in terms of both the accuracy of returned parses and the efficiency with which they are found relative to the best broad coverage statistical parsers 
these perplexity improvements are particularly promising because the parser is providing information that is in some sense orthogonal to the information provided by a trigram model as evidenced by the robust improvements to the baseline trigram when the two models are interpolated 
table reports the word and sentence error rates for five different models i the trigram model that comes with the lattices trained on approximately m words with a vocabulary of ii the best performing model from chelba which was interpolated with the lattice trigram at a iii our parsing model with the same training and vocabulary as the perplexity trials above iv a trigram model with the same training and vocabulary as the parsing model and v no language model at all 
the paper first introduces key notions in language modeling and probabilistic parsing and briefly reviews some previous approaches to using syntactic structure for language modeling 
the trigram model was also trained on sections of the c j corpus 
we will call the manual parse gold and the parse that the parser returns test 
the parser error parser coverage and the uninterpolated model perplexity a all suffered substantially from a narrower search but the interpolated perplexity remained quite good even at the extremes 
string prefix probabilities can be straightforwardly used to compute conditional word probabilities by definition stolcke and segal and jurafsky et al used these basic ideas to estimate bigram probabilities from hand written pcfgs which were then used in language models 
