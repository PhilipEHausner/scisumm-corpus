this is a standard adaptation problem for smt 
when out is large and distinct its contribution can be controlled by training separate in and out models and weighting their combination 
in future work we plan to try this approach with more competitive smt systems and to extend instance weighting to other standard smt components such as the lm lexical phrase weights and lexicalized distortion 
we will also directly compare with a baseline similar to the matsoukas et al approach in order to measure the benefit from weighting phrase pairs or ngrams rather than full sentences 
for developers of statistical machine translation smt systems an additional complication is the heterogeneous nature of smt components word alignment model language model translation model etc 
this is appropriate in cases where it is sanctioned by bayes law such as multiplying lm and tm probabilities but for adaptation a more suitable framework is often a mixture model in which each event may be generated from some domain 
even when there is training data available in the domain of interest there is often additional data from other domains that could in principle be used to improve performance 
this has led previous workers to adopt ad hoc linear weighting schemes finch and sumita foster and kuhn l u et al 
however for multinomial models like our lms and tms there is a one to one correspondence between instances and features eg the correspondence between a phrase pair s t and its conditional multinomial probability p s t 
