at the very least we are creating a data resource the manual annotations that may the basis of future research in evaluation metrics 
while automatic measures are an invaluable tool for the day to day development of machine translation systems they are only a imperfect substitute for human assessment of translation quality or as the acronym bleu puts it a bilingual evaluation understudy 
the manual evaluation of scoring translation on a graded scale from seems to be very hard to perform 
also the argument has been made that machine translation performance should be evaluated via task based evaluation metrics i e how much it assists performing a useful task such as supporting human translators or aiding the analysis of texts 
lack of correct reference translations was pointed out as a short coming of our evaluation 
due to many similarly performing systems we are not able to draw strong conclusions on the question of correlation of manual and automatic evaluation metrics 
we asked participants to each judge sentences in terms of fluency and adequacy the most commonly used manual evaluation metrics 
another way to view the judgements is that they are less quality judgements of machine translation systems per se but rankings of machine translation systems 
the development of automatic scoring methods is an open field of research 
however a recent study callison burch et al pointed out that this correlation may not always be strong 
the bleu score has been shown to correlate well with human judgement when statistical machine translation systems are compared doddington przybocki li 
