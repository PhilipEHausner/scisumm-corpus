formally let el be the number of classification errors of the first second learner on the training data and let eco be the number of unlabeled examples on which the two classifiers disagree 
 sentences of new york times text were parsed using the parser of collins word sequences that met the following criteria were then extracted as named entity examples whose head is a singular noun tagged nn 
before describing the unsupervised case we first describe the supervised version of the algorithm input to the learning algorithm n labeled examples of the form xi y y is the label of the ith example given that there are k possible labels y is a member of y xi is a set of mi features x xi 
several extensions of adaboost for multiclass problems have been suggested freund and schapire schapire and singer 
the task can be considered to be one component of the muc muc named entity task the other task is that of segmentation i e pulling possible people places and locations from text before sending them to the classifier 
adaboost finds a weighted combination of simple weak classifiers where the weights are chosen to minimize a function that bounds the classification error on a set of training examples 
finally we would like to note that it is possible to devise similar algorithms based with other objective functions than the one given in equ 
in the cotraining case blum and mitchell argue that the task should be to induce functions ii and f such that so ii and must correctly classify the labeled examples and must agree with each other on the unlabeled examples 
