in this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations 
moreover the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments mcdonald and word association norms denhire and lemaire 
the projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors 
although the composition model in is commonly used in the literature from a linguistic perspective the model in is more appealing 
the resulting vector is sparser but expresses more succinctly the meaning of the predicate argument structure and thus allows semantic similarity to be modelled more accurately 
importantly additive models capture composition by considering all vector components representing the meaning of the verb and its subject whereas multiplicative models consider a subset namely non zero components 
analysis of similarity ratings the reliability of the collected judgments is important for our evaluation experiments we therefore performed several tests to validate the quality of the ratings 
to give a concrete example circular convolution is an instance of the general multiplicative model which breaks this constraint by allowing uj to contribute to pi for example according to the addition of the two vectors representing horse and run in figure would yield horse run 
a hypothetical semantic space is illustrated in figure 
the neighbors kintsch argues can strengthen features of the predicate that are appropriate for the argument of the predication animal stable village gallop jokey horse run unfortunately comparisons across vector composition models have been few and far between in the literature 
