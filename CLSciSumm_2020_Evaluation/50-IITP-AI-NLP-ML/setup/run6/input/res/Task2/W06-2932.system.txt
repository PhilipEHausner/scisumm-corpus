this interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non projectivity in freer word order languages 
however the parser is fundamentally limited by the scope of local factorizations that make inference tractable 
an exact projective and an approximate non projective parsing algorithm are presented since it is shown that nonprojective dependency parsing becomes np hard when features are extended beyond a single edge 
furthermore it made the system homogeneous in terms of learning algorithms since that is what is used to train our unlabeled parser mcdonald and pereira 
dependency graphs also encode much of the deep syntactic information needed for further processing 
we proceedings of the th conference on computational natural language learning conll x pages new york city june c association for computational linguistics assume that all dependency graphs are trees but may be non projective both of which are true in the data sets we use 
thus a joint model of parsing and labeling could not easily include them without some form of re ranking or approximate parameter estimation 
the first stage based on the unlabeled dependency parsing models described by mcdonald and pereira augmented with morphological features for a subset of the languages 
furthermore for arabic and spanish we used lemmas instead of inflected word forms again based on performance on held out data 
however recently their has been a revived interest in parsing models that produce dependency graph representations of sentences which model words and their arguments through directed edges hudson mel cuk 
