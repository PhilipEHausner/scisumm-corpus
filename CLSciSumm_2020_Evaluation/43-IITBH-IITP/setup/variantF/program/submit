We evaluate the time and memory consumption of each data structure by computing perplexity on 4 billion tokens from the English Gigaword corpus (Parker et al.  2009).
All language model queries issued by machine translation decoders follow a left-to-right pattern  starting with either the begin of sentence token or null context for mid-sentence fragments.
For the perplexity and translation tasks  we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn  2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.
The TRIE model continues to use the least memory of ing (-P) with MAP POPULATE  the default.
Time for Moses itself to load  including loading the language model and phrase table  is included.