Citance Number,Citation Marker,Citation Marker Offset,Citation Offset,Citation Text,Citing Article,Discourse Facet,Reference Article,Reference Offset,Reference Text,Reference Text Clean,fileName
1,2008,0,0,"Following Jiang et al (2008), we describe segmentation and Joint S& amp; T as below: For a given Chinese sentence appearing as a character sequence: C 1: n= C 1 C 2.",C08-1049,Method_Citation,P08-1102,'86',"<S sid=""86"" ssid=""11"">Line 4 scans words of all possible lengths l (l = 1.. min(i, K), where i points to the current considering character).</S>","<S sid=""86"" ssid=""11"">Line 4 scans words of all possible lengths l (l = 1.. min(i, K), where i points to the current considering character).</S>",P08-1102.csv
2,2008,0,0,"As described in Ng and Low (2004 )andJiang et al (2008), we use s indicating a single character word, while b, m and e indicating the be gin, middle and end of a word respectively",C08-1049,Method_Citation,P08-1102,'25',"<S sid=""25"" ssid=""21"">According to Ng and Low (2004), the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.</S>","<S sid=""25"" ssid=""21"">According to Ng and Low (2004), the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.</S>",P08-1102.csv
3,2008,0,0,plates called lexical-target in the column below areintroduced by Jiang et al (2008),C08-1049,Hypothesis_Citation,P08-1102,'71',"<S sid=""71"" ssid=""22"">Using W = w1:m to denote the word sequence, T = t1:m to denote the corresponding POS sequence, P (T |W) to denote the probability that W is labelled as T, and P(W|T) to denote the probability that T generates W, we can define the cooccurrence model as follows: &#955;wt and &#955;tw denote the corresponding weights of the two components.</S>","<S sid=""71"" ssid=""22"">Using W = w1:m to denote the word sequence, T = t1:m to denote the corresponding POS sequence, P (T |W) to denote the probability that W is labelled as T, and P(W|T) to denote the probability that T generates W, we can define the cooccurrence model as follows: &#955;wt and &#955;tw denote the corresponding weights of the two components.</S>",P08-1102.csv
4,2008,0,0,"For CTB-5, we refer to the split by Duan et al (2007) as CTB-5d, and to the split by Jiang et al (2008) as CTB-5j",P12-1110,Results_Citation,P08-1102,'51',"<S sid=""51"" ssid=""2"">Additional features most widely used are related to word or POS ngrams.</S>","<S sid=""51"" ssid=""2"">Additional features most widely used are related to word or POS ngrams.</S>",P08-1102.csv
5,2008,0,0,Jiang et al (2008) proposes a cascaded linear model for joint Chinese word segmentation and POS tagging,D12-1126,Method_Citation,P08-1102,'1',"<S sid=""1"" ssid=""1"">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S>","<S sid=""1"" ssid=""1"">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S>",P08-1102.csv
6,2008,0,0,"We use the feature templates the same as Jiang et al, (2008) to extract features form E model",C10-1135,Results_Citation,P08-1102,'46',"<S sid=""46"" ssid=""18"">Following Collins, we use a function GEN(x) generating all candidate results of an input x , a representation 4) mapping each training example (x, y) &#8712; X &#215; Y to a feature vector 4)(x, y) &#8712; Rd, and a parameter vector &#945;&#65533; &#8712; Rd corresponding to the feature vector. d means the dimension of the vector space, it equals to the amount of features in the model.</S>","<S sid=""46"" ssid=""18"">Following Collins, we use a function GEN(x) generating all candidate results of an input x , a representation 4) mapping each training example (x, y) &#8712; X &#215; Y to a feature vector 4)(x, y) &#8712; Rd, and a parameter vector &#945;&#65533; &#8712; Rd corresponding to the feature vector. d means the dimension of the vector space, it equals to the amount of features in the model.</S>",P08-1102.csv
8,"Jiangetal., 2008a",0,0,"approach, where basic processing units are characters which compose words (Jiangetal., 2008a)",P12-1025,Method_Citation,P08-1102,'67',"<S sid=""67"" ssid=""18"">Language model (LM) provides linguistic probabilities of a word sequence.</S>","<S sid=""67"" ssid=""18"">Language model (LM) provides linguistic probabilities of a word sequence.</S>",P08-1102.csv
9,2008b,0,0,"The solid lines show the 1-best result, which is wrong. Jiang et al (2008b) stress the problems in re ranking phase",C10-2096,Results_Citation,P08-1102,'123',"<S sid=""123"" ssid=""34"">Without it, the F-measure on segmentation and Joint S&amp;T both suffer a decrement of 0.2 points.</S>","<S sid=""123"" ssid=""34"">Without it, the F-measure on segmentation and Joint S&amp;T both suffer a decrement of 0.2 points.</S>",P08-1102.csv
10,"Jiang et al, 2008a",0,0,"6.1.1 Baseline Forest-based System We first segment the Chinese sentences into the1-best segmentations using a state-of-the-art system (Jiang et al, 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results",C10-2096,Results_Citation,P08-1102,'46',"<S sid=""46"" ssid=""18"">Following Collins, we use a function GEN(x) generating all candidate results of an input x , a representation 4) mapping each training example (x, y) &#8712; X &#215; Y to a feature vector 4)(x, y) &#8712; Rd, and a parameter vector &#945;&#65533; &#8712; Rd corresponding to the feature vector. d means the dimension of the vector space, it equals to the amount of features in the model.</S>","<S sid=""46"" ssid=""18"">Following Collins, we use a function GEN(x) generating all candidate results of an input x , a representation 4) mapping each training example (x, y) &#8712; X &#215; Y to a feature vector 4)(x, y) &#8712; Rd, and a parameter vector &#945;&#65533; &#8712; Rd corresponding to the feature vector. d means the dimension of the vector space, it equals to the amount of features in the model.</S>",P08-1102.csv
11,"Jiang et al, 2008a",0,0,"6.1.2 Lattice-forest SystemWe first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al, 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm",C10-2096,Results_Citation,P08-1102,'93',"<S sid=""93"" ssid=""4"">In all experiments, we use the averaged parameters for the perceptrons, and F-measure as the accuracy measure.</S>","<S sid=""93"" ssid=""4"">In all experiments, we use the averaged parameters for the perceptrons, and F-measure as the accuracy measure.</S>",P08-1102.csv
12,"Jiang et al, 2008",0,0,"However, when we repeat the work of (Jiang et al, 2008), which reports to achieve the state-of-art performance in the data-sets that we adopt, it has been found that some features (e.g., C0) are unnoticeably trained several times in their model (which are implicitly generated from different feature templates used in the paper)",C10-1132,Results_Citation,P08-1102,'33',"<S sid=""33"" ssid=""5"">In following subsections, we describe the feature templates and the perceptron training algorithm.</S>","<S sid=""33"" ssid=""5"">In following subsections, we describe the feature templates and the perceptron training algorithm.</S>",P08-1102.csv
13,"Jiang et al, 2008",0,0,"Unicode/CP936 1.1M/55K 104K/13K 0.035 Table 3: Corpus statistics for the second SIGHAN Bakeoff appears twice, which is generated from two different templates Cn (with n=0, generates C0) and [C0Cn] (used in (Jiang et al, 2008), with n=0, generates [C0C0])",C10-1132,Results_Citation,P08-1102,'93',"<S sid=""93"" ssid=""4"">In all experiments, we use the averaged parameters for the perceptrons, and F-measure as the accuracy measure.</S>","<S sid=""93"" ssid=""4"">In all experiments, we use the averaged parameters for the perceptrons, and F-measure as the accuracy measure.</S>",P08-1102.csv
14,"Jiang et al, 2008",0,0,"As all the features adopted in (Jiang et al, 2008) possess binary values, if a binary feature is repeated n times, then it should behave like a real-valued feature with its value to be? n?, at least in principle",C10-1132,Results_Citation,P08-1102,'19',"<S sid=""19"" ssid=""15"">In addition, as these knowledge sources are regarded as separate features, we can train their corresponding models independently with each other.</S>","<S sid=""19"" ssid=""15"">In addition, as these knowledge sources are regarded as separate features, we can train their corresponding models independently with each other.</S>",P08-1102.csv
15,"Jiang et al, 2008",0,0,"Inspired by (Jiang et al, 2008), we set the real d Although Table 5 has shown that the proposed all the value of C0 to be 2.0, the value of C-1C0anC0C1 to be 3.0, and the values of all other features to be 1.0 for the character-based discriminative-plus model",C10-1132,Results_Citation,P08-1102,'46',"<S sid=""46"" ssid=""18"">Following Collins, we use a function GEN(x) generating all candidate results of an input x , a representation 4) mapping each training example (x, y) &#8712; X &#215; Y to a feature vector 4)(x, y) &#8712; Rd, and a parameter vector &#945;&#65533; &#8712; Rd corresponding to the feature vector. d means the dimension of the vector space, it equals to the amount of features in the model.</S>","<S sid=""46"" ssid=""18"">Following Collins, we use a function GEN(x) generating all candidate results of an input x , a representation 4) mapping each training example (x, y) &#8712; X &#215; Y to a feature vector 4)(x, y) &#8712; Rd, and a parameter vector &#945;&#65533; &#8712; Rd corresponding to the feature vector. d means the dimension of the vector space, it equals to the amount of features in the model.</S>",P08-1102.csv
17,"Jiang et al, 2008",0,0,"Last, (Jiang et al, 2008) 5 adds repeated features implicitly based on (Ng and Low, 2004)",C10-1132,Results_Citation,P08-1102,'26',"<S sid=""26"" ssid=""22"">In order to perform POS tagging at the same time, we expand boundary tags to include POS information by attaching a POS to the tail of a boundary tag as a postfix following Ng and Low (2004).</S>","<S sid=""26"" ssid=""22"">In order to perform POS tagging at the same time, we expand boundary tags to include POS information by attaching a POS to the tail of a boundary tag as a postfix following Ng and Low (2004).</S>",P08-1102.csv
20,Jiang et al2008a,0,0,"Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al2010), cascaded linear model (Jiang et al2008a) ,perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), re ranking (Jiang et al2008b)",D12-1046,Results_Citation,P08-1102,'95',"<S sid=""95"" ssid=""6"">For convenience of comparing with others, we focus only on the close test, which means that any extra resource is forbidden except the designated training corpus.</S>","<S sid=""95"" ssid=""6"">For convenience of comparing with others, we focus only on the close test, which means that any extra resource is forbidden except the designated training corpus.</S>",P08-1102.csv
