Citance Number,Citation Marker,Citation Marker Offset,Citation Offset,Citation Text,Citing Article,Discourse Facet,Reference Article,Reference Offset,Reference Text,Reference Text Clean,fileName
1,2001,0,0,"Second, their language models were used to rescore n-best speech lists (supplied by Brian Roark, see Roark (2001))",W05-0104,Method_Citation,J01-2004,'76',"<S sid=""76"" ssid=""34"">The leftfactorization transform that we use is identical to what is called right binarization in Roark and Johnson (1999).</S>","<S sid=""76"" ssid=""34"">The leftfactorization transform that we use is identical to what is called right binarization in Roark and Johnson (1999).</S>",J01-2004.csv
2,2001,0,0,"Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition. These models have in common that they explicitly or implicitly use a context-free grammar induced from a tree bank, with the exception of Chelba and Jelinek (2000)",P08-1013,Results_Citation,J01-2004,'248',"<S sid=""248"" ssid=""4"">In principle, if two models are tested on the same test corpus, the model that assigns the lower perplexity to the test corpus is the model closest to the true distribution of the language, and thus better as a prior model for speech recognition.</S>","<S sid=""248"" ssid=""4"">In principle, if two models are tested on the same test corpus, the model that assigns the lower perplexity to the test corpus is the model closest to the true distribution of the language, and thus better as a prior model for speech recognition.</S>",J01-2004.csv
4,"Roark, 2001a",0,0,"Theperceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn tree bank",P04-1015,Implication_Citation,J01-2004,'32',"<S sid=""32"" ssid=""20"">A second key feature of our approach is that top-down guidance improves the efficiency of the search as more and more conditioning events are extracted from the derivation for use in the probabilistic model.</S>","<S sid=""32"" ssid=""20"">A second key feature of our approach is that top-down guidance improves the efficiency of the search as more and more conditioning events are extracted from the derivation for use in the probabilistic model.</S>",J01-2004.csv
5,"Roark, 2001a",0,0,"We implemented the perceptron approach with the same feature set as that of an existing generative model (Roark, 2001a), and show that the per ceptron model gives performance competitive to that of the generative model on parsing the Penn tree bank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search",P04-1015,Method_Citation,J01-2004,'33',"<S sid=""33"" ssid=""21"">Because the rooted partial derivation is fully connected, all of the conditioning information that might be extracted from the top-down left context has already been specified, and a conditional probability model built on this information will not impose any additional burden on the search.</S>","<S sid=""33"" ssid=""21"">Because the rooted partial derivation is fully connected, all of the conditioning information that might be extracted from the top-down left context has already been specified, and a conditional probability model built on this information will not impose any additional burden on the search.</S>",J01-2004.csv
6,2001a,0,0,"One way around this problem is to adopt a two-pass approach, where GEN (x) is the top N analyses under some initial model, as in the re ranking approach of Collins (2000) .In the current paper we explore alternatives to rerank ing approaches, namely heuristic methods for finding the arg max, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999)",P04-1015,Method_Citation,J01-2004,'76',"<S sid=""76"" ssid=""34"">The leftfactorization transform that we use is identical to what is called right binarization in Roark and Johnson (1999).</S>","<S sid=""76"" ssid=""34"">The leftfactorization transform that we use is identical to what is called right binarization in Roark and Johnson (1999).</S>",J01-2004.csv
7,2001a,0,0,"approach The parser is an incremental beam-search parser very similar to the sort described in Roark (2001a; 2004), with some changes in the search strategy to accommodate the perceptron feature weights",P04-1015,Implication_Citation,J01-2004,'39',"<S sid=""39"" ssid=""27"">The top-down guidance that is provided makes this approach quite efficient in practice.</S>","<S sid=""39"" ssid=""27"">The top-down guidance that is provided makes this approach quite efficient in practice.</S>",J01-2004.csv
9,2001a,0,0,"Unlike in Roark (2001a; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word",P04-1015,Implication_Citation,J01-2004,'267',"<S sid=""267"" ssid=""23"">In addition, we show the average number of rule expansions considered per word, that is, the number of rule expansions for which a probability was calculated (see Roark and Charniak [2000]), and the average number of analyses advanced to the next priority queue per word.</S>","<S sid=""267"" ssid=""23"">In addition, we show the average number of rule expansions considered per word, that is, the number of rule expansions for which a probability was calculated (see Roark and Charniak [2000]), and the average number of analyses advanced to the next priority queue per word.</S>",J01-2004.csv
10,"Roark, 2001",0,0,"A good example of this is the Roark parser (Roark, 2001) which works left-to right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning",P05-1022,Results_Citation,J01-2004,'113',"<S sid=""113"" ssid=""17"">In empirical trials, Goddeau used the top two stack entries to condition the word probability.</S>","<S sid=""113"" ssid=""17"">In empirical trials, Goddeau used the top two stack entries to condition the word probability.</S>",J01-2004.csv
11,"Roark, 2001",0,0,"At the end one has a beam-width? s number of best parses (Roark, 2001) .The Collins parser (Collins, 1997) does use dynamic programming in its search",P05-1022,Method_Citation,J01-2004,'76',"<S sid=""76"" ssid=""34"">The leftfactorization transform that we use is identical to what is called right binarization in Roark and Johnson (1999).</S>","<S sid=""76"" ssid=""34"">The leftfactorization transform that we use is identical to what is called right binarization in Roark and Johnson (1999).</S>",J01-2004.csv
12,"Roark, 2001",0,0,"To put this in perspective, Roark (Roark, 2001) reports oracle results of 0.941 (with the same experimental setup) using his parser to return a variable number of parses",P05-1022,Method_Citation,J01-2004,'274',"<S sid=""274"" ssid=""30"">This structure is evaluated for precision and recall, which is entirely appropriate for these incomplete as well as complete parses.</S>","<S sid=""274"" ssid=""30"">This structure is evaluated for precision and recall, which is entirely appropriate for these incomplete as well as complete parses.</S>",J01-2004.csv
13,"Roark, 2001",0,0,"We ran the first stage parser with 4-timesoverparsing for each string in 7The n? best lists were provided by Brian Roark (Roark, 2001) 8A local-tree is an explicit expansion of an edge and its children",P04-1006,Method_Citation,J01-2004,'76',"<S sid=""76"" ssid=""34"">The leftfactorization transform that we use is identical to what is called right binarization in Roark and Johnson (1999).</S>","<S sid=""76"" ssid=""34"">The leftfactorization transform that we use is identical to what is called right binarization in Roark and Johnson (1999).</S>",J01-2004.csv
14,"Roark, 2001a",0,0,"Incremental top down and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models",P05-1063,Method_Citation,J01-2004,'247',"<S sid=""247"" ssid=""3"">Perplexity is a standard measure within the speech recognition community for comparing language models.</S>","<S sid=""247"" ssid=""3"">Perplexity is a standard measure within the speech recognition community for comparing language models.</S>",J01-2004.csv
15,"Roark, 2001",0,0,"Levy, on the other hand ,argued that studies of probabilistic parsing reveal that typically a small number of analyses are as signed the majority of probability mass (Roark, 2001)",W10-2009,Hypothesis_Citation,J01-2004,'335',"<S sid=""335"" ssid=""91"">While all three seem to be similarly improved by the addition of structural context (e.g., parents and siblings), the addition of c-commanding heads has only a moderate effect on the parser accuracy, but a very large effect on the perplexity.</S>","<S sid=""335"" ssid=""91"">While all three seem to be similarly improved by the addition of structural context (e.g., parents and siblings), the addition of c-commanding heads has only a moderate effect on the parser accuracy, but a very large effect on the perplexity.</S>",J01-2004.csv
17,2001,0,0,"For example, in Demberg and Keller (2008), trials were run deriving surprisal from the Roark (2001) parser under two different conditions: fully lex icalized parsing, and fully unlexicalized parsing (to pre-terminal part-of-speech tags)",D09-1034,Hypothesis_Citation,J01-2004,'143',"<S sid=""143"" ssid=""47"">It has been shown repeatedly&#8212;e.g., Briscoe and Carroll (1993), Charniak (1997), Collins (1997), Inui et al. (1997), Johnson (1998)&#8212;that conditioning the probabilities of structures on the context within which they appear, for example on the lexical head of a constituent (Charniak 1997; Collins 1997), on the label of its parent nonterminal (Johnson 1998), or, ideally, on both and many other things besides, leads to a much better parsing model and results in higher parsing accuracies.</S>","<S sid=""143"" ssid=""47"">It has been shown repeatedly&#8212;e.g., Briscoe and Carroll (1993), Charniak (1997), Collins (1997), Inui et al. (1997), Johnson (1998)&#8212;that conditioning the probabilities of structures on the context within which they appear, for example on the lexical head of a constituent (Charniak 1997; Collins 1997), on the label of its parent nonterminal (Johnson 1998), or, ideally, on both and many other things besides, leads to a much better parsing model and results in higher parsing accuracies.</S>",J01-2004.csv
18,2001,0,0,"We modified the Roark (2001) parser to calculate the discussed measures 1, and the empirical results in? 4 show several things, including: 1) using a fully lexicalized parser to calculate syntactic surprisal and entropy provides higher predictive utility for reading times than these measures calculated via unlexicalized parsing (as in Demberg and Keller); and 2) syntactic entropy is a useful predictor of reading time",D09-1034,Method_Citation,J01-2004,'350',"<S sid=""350"" ssid=""106"">If there is little loss of probability mass, the sum should be close to one.</S>","<S sid=""350"" ssid=""106"">If there is little loss of probability mass, the sum should be close to one.</S>",J01-2004.csv
19,2001,0,0,"In this section, we review relevant details of the Roark (2001) incremental top-down parser, as configured for use here",D09-1034,Method_Citation,J01-2004,'278',"<S sid=""278"" ssid=""34"">Section 22 (41,817 words, 1,700 sentences) served as the development corpus, on which the parser was tested until stable versions were ready to run on the test data, to avoid developing the parser to fit the specific test data.</S>","<S sid=""278"" ssid=""34"">Section 22 (41,817 words, 1,700 sentences) served as the development corpus, on which the parser was tested until stable versions were ready to run on the test data, to avoid developing the parser to fit the specific test data.</S>",J01-2004.csv
20,2001,0,0,"At each word in the string, the Roark (2001) top-down parser provides access to the weighted set of partial analyses in the beam; the set of complete derivations consistent with these is not immediately accessible, hence additional work is re quired to calculate such measures. Let H (D) be the entropy over a set of derivations D, calculated as follows: H (D)=? X D? D? (D) P D?? D? (D?) log? (D) P D?? D? (D?) (10) If the set of derivations D= D (G, W [1, i]) is a set of partial derivations for string W [1, i], then H (D) is a measure of uncertainty over the partial derivations ,i.e., the uncertainty regarding the correct analysis of what has already been processed",D09-1034,Method_Citation,J01-2004,'230',"<S sid=""230"" ssid=""134"">The terminal may be the left corner of the topmost nonterminal on the stack of the analysis or it might be the left corner of the nth nonterminal, after the top n - 1 nonterminals have rewritten to E. Of course, we cannot expect to have adequate statistics for each nonterminal/word pair that we encounter, so we smooth to the POS.</S>","<S sid=""230"" ssid=""134"">The terminal may be the left corner of the topmost nonterminal on the stack of the analysis or it might be the left corner of the nth nonterminal, after the top n - 1 nonterminals have rewritten to E. Of course, we cannot expect to have adequate statistics for each nonterminal/word pair that we encounter, so we smooth to the POS.</S>",J01-2004.csv
