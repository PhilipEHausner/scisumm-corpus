Citance Number,Citation Marker,Citation Marker Offset,Citation Offset,Citation Text,Citing Article,Discourse Facet,Reference Article,Reference Offset,Reference Text,Reference Text Clean,fileName
1,"Collins and Singer, 1999",0,0,"Co-Training has been used before in applications like word-sense disambiguation (Yarowsky, 1995), web-page classification (Blum and Mitchell, 1998) and named entity identification (Collins and Singer, 1999)",N01-1023,Hypothesis_Citation,W99-0613,'33',"<S sid=""33"" ssid=""27"">The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98).</S>","<S sid=""33"" ssid=""27"">The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98).</S>",W99-0613.csv
2,"Collins and Singer, 1999",0,0,"They also discuss an application of classifying web pages by using their method of mutually constrained models. (Collins and Singer, 1999) further extend the use of classifiers that have mutual constraints by adding terms toAdaBoost which force the classifiers to agree (called Co Boosting)",N01-1023,Method_Citation,W99-0613,'247',"<S sid=""247"" ssid=""14"">N, portion of examples on which both classifiers give a label rather than abstaining), and the proportion of these examples on which the two classifiers agree.</S>","<S sid=""247"" ssid=""14"">N, portion of examples on which both classifiers give a label rather than abstaining), and the proportion of these examples on which the two classifiers agree.</S>",W99-0613.csv
3,Collins and Singer 1999,0,0,"Recent methods for English NER focus on machine-learning algorithms such as DL-CoTrain, CoBoost [Collins and Singer 1999], HMM [Daniel M. Bikel 1997], maximum entropy model [Borthwick, et al 1999] and so on",W03-1509,Implication_Citation,W99-0613,'227',"<S sid=""227"" ssid=""6"">The likelihood of the observed data under the model is where P(yi, xi) is defined as in (9).</S>","<S sid=""227"" ssid=""6"">The likelihood of the observed data under the model is where P(yi, xi) is defined as in (9).</S>",W99-0613.csv
4,"Collins and Singer, 1999",0,0,"DL-CoTrain, (Collins and Singer, 1999), learns capitalized proper name NEs from a syn tactically analyzed corpus",C02-1154,Method_Citation,W99-0613,'188',"<S sid=""188"" ssid=""55"">This procedure is repeated for T rounds while alternating between the two classifiers.</S>","<S sid=""188"" ssid=""55"">This procedure is repeated for T rounds while alternating between the two classifiers.</S>",W99-0613.csv
5,"Collins and Singer, 1999",0,0,"(Collins and Singer, 1999) also makes use of competing categories (person, organization, and location), which cover 96% of all the instances itset out to classify",C02-1154,Method_Citation,W99-0613,'183',"<S sid=""183"" ssid=""50"">We first define &amp;quot;pseudo-labels&amp;quot;,-yt, as follows: = Yi t sign(g 0\ 2&#8212; kx2,m &lt; i &lt; n Thus the first m labels are simply copied from the labeled examples, while the remaining (n &#8212; m) examples are taken as the current output of the second classifier.</S>","<S sid=""183"" ssid=""50"">We first define &amp;quot;pseudo-labels&amp;quot;,-yt, as follows: = Yi t sign(g 0\ 2&#8212; kx2,m &lt; i &lt; n Thus the first m labels are simply copied from the labeled examples, while the remaining (n &#8212; m) examples are taken as the current output of the second classifier.</S>",W99-0613.csv
6,"Collins and Singer, 1999",0,0,"In (Collins and Singer, 1999) Collins and Singer show that unlabeled data can be used to reduce the level of supervision required for named entity classification",W06-2204,Method_Citation,W99-0613,'1',"<S sid=""1"" ssid=""1"">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>","<S sid=""1"" ssid=""1"">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>",W99-0613.csv
8,"Collins and Singer, 1999",0,0,"Collins and Singer (1999) for example report that 88% of the named entities occurring in their data set belong to these three categories (Collins and Singer, 1999)",W03-1022,Hypothesis_Citation,W99-0613,'226',"<S sid=""226"" ssid=""5"">For the purposes of EM, the &amp;quot;observed&amp;quot; data is {(xi, Ya&#8226; &#8226; &#8226; (xrn, Yrn), xfil, and the hidden data is {ym+i y}.</S>","<S sid=""226"" ssid=""5"">For the purposes of EM, the &amp;quot;observed&amp;quot; data is {(xi, Ya&#8226; &#8226; &#8226; (xrn, Yrn), xfil, and the hidden data is {ym+i y}.</S>",W99-0613.csv
9,"Collinsand Singer, 1999",0,0,"While EM has worked quite well for a few tasks, notably ma chine translations (starting with the IBM models 1-5 (Brown et al, 1993), it has not had success inmost others, such as part-of-speech tagging (Merialdo, 1991), named-entity recognition (Collinsand Singer, 1999) and context-free-grammar induction (numerous attempts, too many to mention)",E09-1018,Results_Citation,W99-0613,'204',"<S sid=""204"" ssid=""71"">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S>","<S sid=""204"" ssid=""71"">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S>",W99-0613.csv
11,"Collins and Singer, 1999",0,0,"In addition, we would also like to explore the semi-supervised techniques such as co-training and self-training (Collins and Singer, 1999)",W07-1712,Hypothesis_Citation,W99-0613,'203',"<S sid=""203"" ssid=""70"">Several extensions of AdaBoost for multiclass problems have been suggested (Freund and Schapire 97; Schapire and Singer 98).</S>","<S sid=""203"" ssid=""70"">Several extensions of AdaBoost for multiclass problems have been suggested (Freund and Schapire 97; Schapire and Singer 98).</S>",W99-0613.csv
12,"Collins and Singer, 1999",0,0,"Collins et al (Collins and Singer, 1999) proposed two algorithms for NER by modifying Yarowsky ?smethod (Yarowsky, 1995) and the framework suggested by (Blum and Mitchell, 1998)",W09-2208,Implication_Citation,W99-0613,'6',"<S sid=""6"" ssid=""6"">The second algorithm extends ideas from boosting algorithms, designed for supervised learning tasks, to the framework suggested by (Blum and Mitchell 98).</S>","<S sid=""6"" ssid=""6"">The second algorithm extends ideas from boosting algorithms, designed for supervised learning tasks, to the framework suggested by (Blum and Mitchell 98).</S>",W99-0613.csv
13,"Collins and Singer, 1999",0,0,"This approach was shown to perform well on real-world natural language problems (Collins and Singer, 1999)",W06-2207,Hypothesis_Citation,W99-0613,'203',"<S sid=""203"" ssid=""70"">Several extensions of AdaBoost for multiclass problems have been suggested (Freund and Schapire 97; Schapire and Singer 98).</S>","<S sid=""203"" ssid=""70"">Several extensions of AdaBoost for multiclass problems have been suggested (Freund and Schapire 97; Schapire and Singer 98).</S>",W99-0613.csv
15,"Collins and Singer, 1999",0,0,"(6) Similarly to (Collins and Singer, 1999) we used T= 0.95 for all experiments reported here",W06-2207,Results_Citation,W99-0613,'3',"<S sid=""3"" ssid=""3"">The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type.</S>","<S sid=""3"" ssid=""3"">The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type.</S>",W99-0613.csv
16,1999,0,0,We use Collins and Singer (1999) for our exact specification of Yarowsky.2 It uses DL rule scores ?fj?| ?fj |+ |? f |+ L (1) where  is a smoothing constant,P12-1065,Implication_Citation,W99-0613,'227',"<S sid=""227"" ssid=""6"">The likelihood of the observed data under the model is where P(yi, xi) is defined as in (9).</S>","<S sid=""227"" ssid=""6"">The likelihood of the observed data under the model is where P(yi, xi) is defined as in (9).</S>",W99-0613.csv
