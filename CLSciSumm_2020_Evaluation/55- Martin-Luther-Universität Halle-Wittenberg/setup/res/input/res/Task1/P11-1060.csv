Citance Number,Citation Marker,Citation Marker Offset,Citation Offset,Citation Text,Citing Article,Discourse Facet,Reference Article,Reference Offset,Reference Text,Reference Text Clean,fileName
1,2011,0,0,"Clarkeet al (2010) and Liang et al (2011) describe approaches for learning semantic parsers from questions paired with database answers, while Goldwasser et al (2011) presents work on unsupervised learning",D11-1039,Aim_Citation,P11-1060,'130',"<S sid=""130"" ssid=""15"">For GEO, there are 22 prototype words; for JOBS, there are 5.</S>","<S sid=""130"" ssid=""15"">For GEO, there are 22 prototype words; for JOBS, there are 5.</S>",P11-1060.csv
2,2011,0,0,"In particular, Clarke et al (2010) and Liang et al (2011) proposed methods to learn from question answer pairs alone, which represents a significant advance",P13-1092,Method_Citation,P11-1060,'41',"<S sid=""41"" ssid=""17"">We say a value v is consistent for a node x if there exists a solution that assigns v to x.</S>","<S sid=""41"" ssid=""17"">We say a value v is consistent for a node x if there exists a solution that assigns v to x.</S>",P11-1060.csv
3,2011,0,0,"To handle syntax-semantics mismatch, GUSP introduces a novel dependency-based meaning representation 1Clarke et al (2010) and Liang et al (2011) used the annotated logical forms to compute answers for their experiments",P13-1092,Implication_Citation,P11-1060,'17',"<S sid=""17"" ssid=""13"">The dominant paradigm in compositional semantics is Montague semantics, which constructs lambda calculus forms in a bottom-up manner.</S>","<S sid=""17"" ssid=""13"">The dominant paradigm in compositional semantics is Montague semantics, which constructs lambda calculus forms in a bottom-up manner.</S>",P11-1060.csv
4,2011,0,0,"More recently, Liang et al (2011 )proposedDCS for dependency-based compositional semantics, which represents a semantic parse as a tree with nodes representing database elements and operations, and edges representing relational joins",P13-1092,Implication_Citation,P11-1060,'135',"<S sid=""135"" ssid=""20"">SEMRESP requires a lexicon of 1.42 words per non-value predicate, WordNet features, and syntactic parse trees; DCS requires only words for the domain-independent predicates (overall, around 0.5 words per non-value predicate), POS tags, and very simple indicator features.</S>","<S sid=""135"" ssid=""20"">SEMRESP requires a lexicon of 1.42 words per non-value predicate, WordNet features, and syntactic parse trees; DCS requires only words for the domain-independent predicates (overall, around 0.5 words per non-value predicate), POS tags, and very simple indicator features.</S>",P11-1060.csv
5,"Liang et al, 2011",0,0,"GUSP represents meaning by a semantic tree, which is similar to DCS (Liang et al, 2011)",P13-1092,Results_Citation,P11-1060,'155',"<S sid=""155"" ssid=""40"">The errors that the system makes stem from mul- The other major focus of this work is program tiple sources, including errors in the POS tags (e.g., induction&#8212;inferring logical forms from their denostates is sometimes tagged as a verb, which triggers tations.</S>","<S sid=""155"" ssid=""40"">The errors that the system makes stem from mul- The other major focus of this work is program tiple sources, including errors in the POS tags (e.g., induction&#8212;inferring logical forms from their denostates is sometimes tagged as a verb, which triggers tations.</S>",P11-1060.csv
6,2011,0,0,"Matuszek et al [2010], Liang et al [2011] and Chen and Mooney [2011] describe models that learn compositional semantics, but word meanings are symbolic structures rather than patterns of features in the external world",W12-2802,Results_Citation,P11-1060,'8',"<S sid=""8"" ssid=""4"">On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives.</S>","<S sid=""8"" ssid=""4"">On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives.</S>",P11-1060.csv
7,"Liang et al, 2011",0,0,"It is well-studied in NLP, and a wide variety of methods have been proposed to tackle it ,e.g. rule-based (Popescu et al, 2003), super vised (Zelle, 1995), unsupervised (Goldwasser et al., 2011), and response-based (Liang et al, 2011)",P13-2009,Implication_Citation,P11-1060,'139',"<S sid=""139"" ssid=""24"">All other systems require logical forms as training data, whereas ours does not.</S>","<S sid=""139"" ssid=""24"">All other systems require logical forms as training data, whereas ours does not.</S>",P11-1060.csv
8,Liangetal2011,0,0,"One line of work eliminates the need for an annotated logical form, instead using only the correct answer for a database query (Liangetal2011) or even a binary correct/incorrect signal (Clarke et al2010)",D12-1069,Method_Citation,P11-1060,'133',"<S sid=""133"" ssid=""18"">Table 2 shows that our system using lexical triggers L (henceforth, DCS) outperforms SEMRESP (78.9% over 73.2%).</S>","<S sid=""133"" ssid=""18"">Table 2 shows that our system using lexical triggers L (henceforth, DCS) outperforms SEMRESP (78.9% over 73.2%).</S>",P11-1060.csv
9,2011,0,0,"For example, Liang et al (2011) constructs a latent parse similar in structure to a dependency grammar, but representing a logical form",N12-1049,Results_Citation,P11-1060,'15',"<S sid=""15"" ssid=""11"">Unlike standard semantic parsing, our end goal is only to generate the correct y, so we are free to choose the representation for z.</S>","<S sid=""15"" ssid=""11"">Unlike standard semantic parsing, our end goal is only to generate the correct y, so we are free to choose the representation for z.</S>",P11-1060.csv
10,2011,0,0,Clarke et al (2010) and Liang et al (2011) trained systems on question and answer pairs by automatically finding semantic interpretations of the questions that would generate the correct answers,P12-1045,Method_Citation,P11-1060,'153',"<S sid=""153"" ssid=""38"">Trace inspired by Discourse Representation Theory (DRT) predicates can be inserted anywhere, but the fea- (Kamp and Reyle, 1993; Kamp et al., 2005), where tures favor some insertions depending on the words variables are discourse referents.</S>","<S sid=""153"" ssid=""38"">Trace inspired by Discourse Representation Theory (DRT) predicates can be inserted anywhere, but the fea- (Kamp and Reyle, 1993; Kamp et al., 2005), where tures favor some insertions depending on the words variables are discourse referents.</S>",P11-1060.csv
11,"Liang et al,2011",0,0,"Dependency-based Compositional Semantics (DCS) provides an intuitive way to model semantics of questions, by using simple dependency-like trees (Liang et al, 2011)",P14-1008,Results_Citation,P11-1060,'155',"<S sid=""155"" ssid=""40"">The errors that the system makes stem from mul- The other major focus of this work is program tiple sources, including errors in the POS tags (e.g., induction&#8212;inferring logical forms from their denostates is sometimes tagged as a verb, which triggers tations.</S>","<S sid=""155"" ssid=""40"">The errors that the system makes stem from mul- The other major focus of this work is program tiple sources, including errors in the POS tags (e.g., induction&#8212;inferring logical forms from their denostates is sometimes tagged as a verb, which triggers tations.</S>",P11-1060.csv
12,"Liang et al, 2011",0,0,"DCS trees has been proposed to represent natural language semantics with a structure similar to dependency trees (Liang et al, 2011) (Figure 1)",P14-1008,Implication_Citation,P11-1060,'21',"<S sid=""21"" ssid=""17"">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>","<S sid=""21"" ssid=""17"">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>",P11-1060.csv
13,"Liang et al, 2011",0,0,"are explained in? 2.5. 5http: //nlp.stanford.edu/software/corenlp.shtml 6 In (Liang et al, 2011) DCS trees are learned from QApairs and database entries",P14-1008,Results_Citation,P11-1060,'167',"<S sid=""167"" ssid=""52"">In DCS, we start with lexical triggers, which are 6 Conclusion more basic than CCG lexical entries.</S>","<S sid=""167"" ssid=""52"">In DCS, we start with lexical triggers, which are 6 Conclusion more basic than CCG lexical entries.</S>",P11-1060.csv
14,"Liang et al, 2011",0,0,"as in the sentence? Tropi cal storm Debby is blamed for death?, which is a tropical storm, is Debby, etc. Technically, each germ in a DCS tree indicates a variable when the DCS tree is translated to a FOL formula, and the abstract denotation of the germ corresponds to the set of consistent values (Liang et al, 2011) of that variable",P14-1008,Results_Citation,P11-1060,'126',"<S sid=""126"" ssid=""11"">During development, we further held out a random 30% of the training sets for validation.</S>","<S sid=""126"" ssid=""11"">During development, we further held out a random 30% of the training sets for validation.</S>",P11-1060.csv
15,2011,0,0,Clarke et al (2010) and Liang et al (2011) replace semantic annotations in the training set with target answers which are more easily available,D11-1140,Results_Citation,P11-1060,'145',"<S sid=""145"" ssid=""30"">Initially, the weights are zero, so the beam search is essentially unguided.</S>","<S sid=""145"" ssid=""30"">Initially, the weights are zero, so the beam search is essentially unguided.</S>",P11-1060.csv
16,"Liang et al, 2011",0,0,"and Collins, 2005, 2007),? -WASP (Wong and Mooney, 2007), UBL (Kwiatkowski et al, 2010) systems and DCS (Liang et al, 2011)",D11-1140,Implication_Citation,P11-1060,'139',"<S sid=""139"" ssid=""24"">All other systems require logical forms as training data, whereas ours does not.</S>","<S sid=""139"" ssid=""24"">All other systems require logical forms as training data, whereas ours does not.</S>",P11-1060.csv
17,2011,0,0,"In general, every plural NPpotentially introduces an implicit universal, ranging 1For example, Liang et al (2011) in their state-of-the-art statistical semantic parser within the domain of natural language queries to databases, explicitly devise quantifier scoping in the semantic model",P13-1007,Results_Citation,P11-1060,'145',"<S sid=""145"" ssid=""30"">Initially, the weights are zero, so the beam search is essentially unguided.</S>","<S sid=""145"" ssid=""30"">Initially, the weights are zero, so the beam search is essentially unguided.</S>",P11-1060.csv
18,2011,0,0,"DD-ADMM may be useful in other frameworks involving logical constraints, such as the models for compositional semantics presented by Liang et al (2011)",D11-1022,Hypothesis_Citation,P11-1060,'127',"<S sid=""127"" ssid=""12"">Our lexical triggers L include the following: (i) predicates for a small set of &#8776; 20 function words (e.g., (most, argmax)), (ii) (x, x) for each value predicate x in w (e.g., (Boston, Boston)), and (iii) predicates for each POS tag in {JJ, NN, NNS} (e.g., (JJ, size), (JJ, area), etc.</S>","<S sid=""127"" ssid=""12"">Our lexical triggers L include the following: (i) predicates for a small set of &#8776; 20 function words (e.g., (most, argmax)), (ii) (x, x) for each value predicate x in w (e.g., (Boston, Boston)), and (iii) predicates for each POS tag in {JJ, NN, NNS} (e.g., (JJ, size), (JJ, area), etc.</S>",P11-1060.csv
19,2011,0,0,"In fact, for any CFG G, it 1See Liang et al (2011) for work in representing lambda calculus expressions with trees",P12-1051,Method_Citation,P11-1060,'43',"<S sid=""43"" ssid=""19"">Computation We can compute the denotation JzKw of a DCS tree z by exploiting dynamic programming on trees (Dechter, 2003).</S>","<S sid=""43"" ssid=""19"">Computation We can compute the denotation JzKw of a DCS tree z by exploiting dynamic programming on trees (Dechter, 2003).</S>",P11-1060.csv
