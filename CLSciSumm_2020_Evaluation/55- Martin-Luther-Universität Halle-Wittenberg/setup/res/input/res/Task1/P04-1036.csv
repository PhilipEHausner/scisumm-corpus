Citance Number,Citation Marker,Citation Marker Offset,Citation Offset,Citation Text,Citing Article,Discourse Facet,Reference Article,Reference Offset,Reference Text,Reference Text Clean,fileName
1,"McCarthy et al, 2004",0,0,"The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding con text into account (McCarthy et al, 2004)",W04-0837,Implication_Citation,P04-1036,'8',"<S sid=""8"" ssid=""1"">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>","<S sid=""8"" ssid=""1"">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>",P04-1036.csv
2,"McCarthy et al, 2004",0,0,"Association for Computational Linguistics for the Semantic Analysis of Text, Barcelona, Spain, July 2004 SENSEVAL-3: Third International Workshop on the Evaluation of Systems PoS precision recall baseline Noun 95 73 45 Verb 79 43 22 Adjective 88 59 44 Adverb 91 72 59 All PoS 90 63 41Table 2: The SENSEVAL-2 first sense on the SEN SEVAL-2 English all-words data system can be tuned to a given genre or domain (McCarthy et al, 2004) and also because there will be words that occur with insufficient frequency inthe hand-tagged resources available",W04-0837,Results_Citation,P04-1036,'25',"<S sid=""25"" ssid=""18"">However, the most accurate WSD systems are those which require manually sense tagged data in the first place, and their accuracy depends on the quantity of training examples (Yarowsky and Florian, 2002) available.</S>","<S sid=""25"" ssid=""18"">However, the most accurate WSD systems are those which require manually sense tagged data in the first place, and their accuracy depends on the quantity of training examples (Yarowsky and Florian, 2002) available.</S>",P04-1036.csv
3,"McCarthy et al, 2004",0,0,"The method is described in (McCarthy et al, 2004), which we summarise here",W04-0837,Method_Citation,P04-1036,'136',"<S sid=""136"" ssid=""13"">The words included in this experiment are not a random sample, since we anticipated different predominant senses in the SPORTS and FINANCE domains for these words.</S>","<S sid=""136"" ssid=""13"">The words included in this experiment are not a random sample, since we anticipated different predominant senses in the SPORTS and FINANCE domains for these words.</S>",P04-1036.csv
5,"McCarthy et al, 2004",0,0,"McCarthy et al (2004) use a corpus and word similarities to induce a ranking of word senses from an untagged corpus to be used in WSD.We build upon this previous research, and pro pose an unsupervised WSD method in which senses for two grammatically related words in the sentence will be connected through directed edges",I08-2105,Method_Citation,P04-1036,'163',"<S sid=""163"" ssid=""11"">Lapata and Brew (2004) have recently also highlighted the importance of a good prior in WSD.</S>","<S sid=""163"" ssid=""11"">Lapata and Brew (2004) have recently also highlighted the importance of a good prior in WSD.</S>",P04-1036.csv
6,"McCarthy et al, 2004",0,0,"Previous re search in inducing sense rankings from an untagged corpus (McCarthy et al, 2004), and inducing selectional preferences at the word level (for other applications) (Erk, 2007) will provide the starting point for research in this direction",I08-2105,Hypothesis_Citation,P04-1036,'161',"<S sid=""161"" ssid=""9"">Our approach is complementary to this.</S>","<S sid=""161"" ssid=""9"">Our approach is complementary to this.</S>",P04-1036.csv
7,2004,0,0,"McCarthy et al (2004) report a disambiguation precision of 53.0% and recall of 49.0% on the Senseval-2 test data, using an approach that derives sense ranking based on word similarity and distributional analysis in a corpus",I08-2105,Results_Citation,P04-1036,'152',"<S sid=""152"" ssid=""29"">We see that both domains have a similarly high percentage of factotum (domain independent) labels, but as we would expect, the other peaks correspond to the economy label for the FINANCE corpus, and the sports label for the SPORTS corpus. inant senses for 38 polysemous words ranked using the SPORTS and FINANCE corpus.</S>","<S sid=""152"" ssid=""29"">We see that both domains have a similarly high percentage of factotum (domain independent) labels, but as we would expect, the other peaks correspond to the economy label for the FINANCE corpus, and the sports label for the SPORTS corpus. inant senses for 38 polysemous words ranked using the SPORTS and FINANCE corpus.</S>",P04-1036.csv
8,"McCarthy et al, 2004",0,0,"Research by (McCarthy et al, 2004) highlighted that the sense priors of a word in a corpus depend on the domain from which the corpus is drawn",P06-1012,Method_Citation,P04-1036,'133',"<S sid=""133"" ssid=""10"">We acquired thesauruses for these corpora using the procedure described in section 2.1.</S>","<S sid=""133"" ssid=""10"">We acquired thesauruses for these corpora using the procedure described in section 2.1.</S>",P04-1036.csv
9,"McCarthy et al, 2004",0,0,"In addition, we implemented the unsupervised method of (McCarthy et al, 2004), which cal cu lates a prevalence score for each sense of a word to predict the predominant sense",P06-1012,Results_Citation,P04-1036,'103',"<S sid=""103"" ssid=""1"">In order to see how well the automatically acquired predominant sense performs on a WSD task from which the WordNet sense ordering has not been taken, we use the SENSEVAL-2 all-words data (Palmer et al., 2001).</S>","<S sid=""103"" ssid=""1"">In order to see how well the automatically acquired predominant sense performs on a WSD task from which the WordNet sense ordering has not been taken, we use the SENSEVAL-2 all-words data (Palmer et al., 2001).</S>",P04-1036.csv
11,2004,0,0,"McCarthy et al (2004) reported that the best results were obtained using k= 50 neighbors and the Wordnet Similarityjcn measure (Jiang and Conrath, 1997)",P10-1155,Results_Citation,P04-1036,'45',"<S sid=""45"" ssid=""1"">In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998).</S>","<S sid=""45"" ssid=""1"">In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998).</S>",P04-1036.csv
12,2004,0,0,"In doing so, we provide first results on the application to French parsing of WordNet automatic sense ranking (ASR), using the method of McCarthy et al (2004)",W12-3401,Method_Citation,P04-1036,'165',"<S sid=""165"" ssid=""13"">Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus, whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence.</S>","<S sid=""165"" ssid=""13"">Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus, whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence.</S>",P04-1036.csv
13,2004,0,0,"To define an appropriate categorical distribution over synsets for each 2 lemma x in our source vocabulary, we first use the WordNet resource to identify the set Sx of different senses of x. We then use a distributional thesaurus to perform ASR, which determines the prevalence with respect to x of each sense s? Sx, following the approach of McCarthy et al (2004)",W12-3401,Implication_Citation,P04-1036,'71',"<S sid=""71"" ssid=""27"">This is transformed from a distance measure in the WN-Similarity package by taking the reciprocal:</S>","<S sid=""71"" ssid=""27"">This is transformed from a distance measure in the WN-Similarity package by taking the reciprocal:</S>",P04-1036.csv
14,2004,0,0,"As explained in Section 2.2, ASR is performed using the method of McCarthy et al (2004)",W12-3401,Results_Citation,P04-1036,'45',"<S sid=""45"" ssid=""1"">In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998).</S>","<S sid=""45"" ssid=""1"">In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998).</S>",P04-1036.csv
16,"McCarthy et al, 2004",0,0,"This approach is commonly used as a baseline for word sense disambiguation (McCarthy et al, 2004)",S12-1097,Implication_Citation,P04-1036,'142',"<S sid=""142"" ssid=""19"">The results for 10 of the words from the qualitative experiment are summarized in table 3 with the WordNet sense number for each word supplied alongside synonyms or hypernyms from WordNet for readability.</S>","<S sid=""142"" ssid=""19"">The results for 10 of the words from the qualitative experiment are summarized in table 3 with the WordNet sense number for each word supplied alongside synonyms or hypernyms from WordNet for readability.</S>",P04-1036.csv
17,"McCarthy et al, 2004",0,0,"More radical solutions than sense grouping that have been proposed are to restrict the task to determining predominant sense in a given domain (McCarthy et al, 2004), or to work directly with para phrases (McCarthy and Navigli, 2009)",W10-2803,Results_Citation,P04-1036,'167',"<S sid=""167"" ssid=""15"">In this work the lists of neighbours are themselves clustered to bring out the various senses of the word.</S>","<S sid=""167"" ssid=""15"">In this work the lists of neighbours are themselves clustered to bring out the various senses of the word.</S>",P04-1036.csv
18,2004,0,0,"In addition, we can also infer a positive contribution of the frequency of a sense with the choice of the first synset returned by Word net resulting in a reasonable WSD heuristic (which is compatible with the results by McCarthy et al (2004))",W08-2107,Method_Citation,P04-1036,'168',"<S sid=""168"" ssid=""16"">They evaluate using the lin measure described above in section 2.2 to determine the precision and recall of these discovered classes with respect to WordNet synsets.</S>","<S sid=""168"" ssid=""16"">They evaluate using the lin measure described above in section 2.2 to determine the precision and recall of these discovered classes with respect to WordNet synsets.</S>",P04-1036.csv
19,"McCarthy et al, 2004",0,0,"It is worthwhile to remark here that, being the IBLE algorithm fully unsupervised, improving the most frequent baseline is an excellent result, rarely achieved in the literature on unsupervised methods for WSD (McCarthy et al, 2004)",D07-1026,Implication_Citation,P04-1036,'8',"<S sid=""8"" ssid=""1"">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>","<S sid=""8"" ssid=""1"">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>",P04-1036.csv
20,"McCarthy et al, 2004",0,0,"The first, most frequent sense (MFS) (McCarthy et al, 2004), is widely used baseline for supervised WSD systems",W12-2429,Method_Citation,P04-1036,'164',"<S sid=""164"" ssid=""12"">They used syntactic evidence to find a prior distribution for verb classes, based on (Levin, 1993), and incorporate this in a WSD system.</S>","<S sid=""164"" ssid=""12"">They used syntactic evidence to find a prior distribution for verb classes, based on (Levin, 1993), and incorporate this in a WSD system.</S>",P04-1036.csv
