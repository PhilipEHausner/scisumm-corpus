Citance Number,Citation Marker,Citation Marker Offset,Citation Offset,Citation Text,Citing Article,Discourse Facet,Reference Article,Reference Offset,Reference Text,Reference Text Clean,fileName
1,"Foster et al, 2010",0,0,"Another popular task in SMT is domain adaptation (Foster et al, 2010)",P11-2074,Method_Citation,D10-1044,'36',"<S sid=""36"" ssid=""33"">Section 5 covers relevant previous work on SMT adaptation, and section 6 concludes.</S>","<S sid=""36"" ssid=""33"">Section 5 covers relevant previous work on SMT adaptation, and section 6 concludes.</S>",D10-1044.csv
2,"Foster et al, 2010",0,0,"In addition, discriminative weighting methods were proposed to assign appropriate weights to the sentences from training corpus (Matsoukas et al, 2009) or the phrase pairs of phrase table (Foster et al, 2010)",P12-1048,Results_Citation,D10-1044,'16',"<S sid=""16"" ssid=""13"">First, we aim to explicitly characterize examples from OUT as belonging to general language or not.</S>","<S sid=""16"" ssid=""13"">First, we aim to explicitly characterize examples from OUT as belonging to general language or not.</S>",D10-1044.csv
3,"Foster et al., 2010",0,0,"Domain knowledge also has the potential to improve open-text applications such as summarization (Ceylan et al 2010) and machine translation (Foster et al., 2010) .Research in Word Sense Disambiguation (Navigli, 2009, WSD), the task aimed at the automatic labeling of text with word senses, has been oriented towards domain text understanding for several years now",D12-1129,Hypothesis_Citation,D10-1044,'141',"<S sid=""141"" ssid=""10"">Moving beyond directly related work, major themes in SMT adaptation include the IR (Hildebrand et al., 2005; L&#168;u et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L&#168;u et al., 2007) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (Bertoldi and Federico, 2009; Ueffing et al., 2007; Schwenk and Senellart, 2009).</S>","<S sid=""141"" ssid=""10"">Moving beyond directly related work, major themes in SMT adaptation include the IR (Hildebrand et al., 2005; L&#168;u et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L&#168;u et al., 2007) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (Bertoldi and Federico, 2009; Ueffing et al., 2007; Schwenk and Senellart, 2009).</S>",D10-1044.csv
4,2010,0,0,"Yasuda et al (2008) and Foster et al (2010) ranked the sentence pairs in the general-domain corpus according to the perplexity scores of sentences, which are computed with respect to in-domain language models",P14-2093,Method_Citation,D10-1044,'62',"<S sid=""62"" ssid=""26"">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>","<S sid=""62"" ssid=""26"">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>",D10-1044.csv
5,2010,0,0,"However, such confounding factors do not affect the optimization algorithm, which works with a fixed set of phrase pairs, and merely varies? .Our main technical contributions are as fol lows: Additionally to perplexity optimization for linear interpolation, which was first applied by Foster et al (2010), we propose perplexity optimization for weighted counts (equation 3), and a modified implementation of linear interpolation. Also, we independently perform perplexity minimization for all four features of the standard SMTtranslation model: the phrase translation probabilities p (t|s) and p (s|t), and the lexical weights lex (t|s) and lex (s|t)",E12-1055,Method_Citation,D10-1044,'148',"<S sid=""148"" ssid=""5"">Mixing, smoothing, and instance-feature weights are learned at the same time using an efficient maximum-likelihood procedure that relies on only a small in-domain development corpus.</S>","<S sid=""148"" ssid=""5"">Mixing, smoothing, and instance-feature weights are learned at the same time using an efficient maximum-likelihood procedure that relies on only a small in-domain development corpus.</S>",D10-1044.csv
6,2010,0,0,"Matsoukas et al (2009) propose an approach where each sentence is weighted according to a classifier, and Foster et al (2010) ex tend this approach by weighting individual phrase pairs",E12-1055,Results_Citation,D10-1044,'94',"<S sid=""94"" ssid=""31"">In addition to using the simple features directly, we also trained an SVM classifier with these features to distinguish between IN and OUT phrase pairs.</S>","<S sid=""94"" ssid=""31"">In addition to using the simple features directly, we also trained an SVM classifier with these features to distinguish between IN and OUT phrase pairs.</S>",D10-1044.csv
7,2010,0,0,"These more fine-grained methods need not be seen as alternatives to coarse-grained ones. Foster et al (2010) combine the two, applying linear interpolation to combine the instance 542 weighted out-of-domain model with an in-domain model",E12-1055,Method_Citation,D10-1044,'62',"<S sid=""62"" ssid=""26"">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>","<S sid=""62"" ssid=""26"">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>",D10-1044.csv
8,Foster et al 2010,0,0,Note that both data sets have a relatively high ratio of in-domain to out-of-domain parallel training data (1:20 for DE? EN and 1:5 for HT? EN) Previous research has been performed with ratios of 1:100 (Foster et al 2010) or 1:400 (Axelrod et al 2011),E12-1055,Method_Citation,D10-1044,'37',"<S sid=""37"" ssid=""1"">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S>","<S sid=""37"" ssid=""1"">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S>",D10-1044.csv
9,Foster et al 2010,0,0,"We expand on work by (Foster et al 2010) in establishing translation model perplexity minimization as a robust baseline for a weighted combination of translationmodels.15 We demonstrate perplexity optimization for weighted counts, which are a natural extension of unadapted MLE training, but are of little prominence in domain adaptation research",E12-1055,Method_Citation,D10-1044,'55',"<S sid=""55"" ssid=""19"">This suggests a direct parallel to (1): where &#732;p(s, t) is a joint empirical distribution extracted from the IN dev set using the standard procedure.2 An alternative form of linear combination is a maximum a posteriori (MAP) combination (Bacchiani et al., 2004).</S>","<S sid=""55"" ssid=""19"">This suggests a direct parallel to (1): where &#732;p(s, t) is a joint empirical distribution extracted from the IN dev set using the standard procedure.2 An alternative form of linear combination is a maximum a posteriori (MAP) combination (Bacchiani et al., 2004).</S>",D10-1044.csv
10,"Foster et al, 2010",0,0,"In addition to the basic approach of concatenation of in-domain and out-of-domain data, we also trained a log-linear mixture model (Foster and Kuhn, 2007) 940 as well as the linear mixture model of (Foster et al, 2010) for conditional phrase-pair probabilities over IN and OUT",P12-1099,Results_Citation,D10-1044,'54',"<S sid=""54"" ssid=""18"">However, we note that the final conditional estimates p(s|t) from a given phrase table maximize the likelihood of joint empirical phrase pair counts over a word-aligned corpus.</S>","<S sid=""54"" ssid=""18"">However, we note that the final conditional estimates p(s|t) from a given phrase table maximize the likelihood of joint empirical phrase pair counts over a word-aligned corpus.</S>",D10-1044.csv
11,2010,0,0,m ?mpm (e? |f?) Our technique for setting? m is similar to that outlined in Foster et al (2010),P12-1099,Implication_Citation,D10-1044,'106',"<S sid=""106"" ssid=""10"">The corpora for both settings are summarized in table 1.</S>","<S sid=""106"" ssid=""10"">The corpora for both settings are summarized in table 1.</S>",D10-1044.csv
12,"Foster et al., 2010",0,0,"m ?mpm (e? |f?) For efficiency and stability, we use the EMalgorithm to find??, rather than L-BFGS as in (Foster et al., 2010)",P12-1099,Implication_Citation,D10-1044,'53',"<S sid=""53"" ssid=""17"">This has led previous workers to adopt ad hoc linear weighting schemes (Finch and Sumita, 2008; Foster and Kuhn, 2007; L&#168;u et al., 2007).</S>","<S sid=""53"" ssid=""17"">This has led previous workers to adopt ad hoc linear weighting schemes (Finch and Sumita, 2008; Foster and Kuhn, 2007; L&#168;u et al., 2007).</S>",D10-1044.csv
13,2010,0,0,"Foster et al (2010), however, uses a different approach to select related sentences from OUT",P12-1099,Results_Citation,D10-1044,'31',"<S sid=""31"" ssid=""28"">For comparison to information-retrieval inspired baselines, eg (L&#168;u et al., 2007), we select sentences from OUT using language model perplexities from IN.</S>","<S sid=""31"" ssid=""28"">For comparison to information-retrieval inspired baselines, eg (L&#168;u et al., 2007), we select sentences from OUT using language model perplexities from IN.</S>",D10-1044.csv
14,2010,0,0,Foster et al (2010) propose asimilar method for machine translation that uses features to capture degrees of generality,P12-1099,Implication_Citation,D10-1044,'21',"<S sid=""21"" ssid=""18"">This highly effective approach is not directly applicable to the multinomial models used for core SMT components, which have no natural method for combining split features, so we rely on an instance-weighting approach (Jiang and Zhai, 2007) to downweight domain-specific examples in OUT.</S>","<S sid=""21"" ssid=""18"">This highly effective approach is not directly applicable to the multinomial models used for core SMT components, which have no natural method for combining split features, so we rely on an instance-weighting approach (Jiang and Zhai, 2007) to downweight domain-specific examples in OUT.</S>",D10-1044.csv
15,"Foster et al, 2010",0,0,"As in (Foster et al, 2010), this approach works at the level of phrase pairs",P13-1126,Implication_Citation,D10-1044,'23',"<S sid=""23"" ssid=""20"">Our second contribution is to apply instance weighting at the level of phrase pairs.</S>","<S sid=""23"" ssid=""20"">Our second contribution is to apply instance weighting at the level of phrase pairs.</S>",D10-1044.csv
16,2010,0,0,"The ranking of the sentences in a general-domain corpus according to in-domain perplexity has also been applied to machine translation by both Yasuda et al (2008), and Foster et al (2010)",D11-1033,Method_Citation,D10-1044,'67',"<S sid=""67"" ssid=""4"">We extend the Matsoukas et al approach in several ways.</S>","<S sid=""67"" ssid=""4"">We extend the Matsoukas et al approach in several ways.</S>",D10-1044.csv
17,2010,0,0,"Foster et al (2010) do not mention what percentage of the corpus they select for their IR-baseline, but they concatenate the data to their in-domain corpus and re port a decrease in performance",D11-1033,Implication_Citation,D10-1044,'5',"<S sid=""5"" ssid=""2"">Even when there is training data available in the domain of interest, there is often additional data from other domains that could in principle be used to improve performance.</S>","<S sid=""5"" ssid=""2"">Even when there is training data available in the domain of interest, there is often additional data from other domains that could in principle be used to improve performance.</S>",D10-1044.csv
18,2010,0,0,"Foster et al (2010) further perform this on extracted phrase pairs, not just sentences",D11-1033,Results_Citation,D10-1044,'68',"<S sid=""68"" ssid=""5"">First, we learn weights on individual phrase pairs rather than sentences.</S>","<S sid=""68"" ssid=""5"">First, we learn weights on individual phrase pairs rather than sentences.</S>",D10-1044.csv
19,"Foster et al, 2010",0,0,"To address the first shortcoming, we adapt and extend some simple but effective phrase features as the input features for new DNN feature learning, and these features have been shown significant improvement for SMT, such as, phrase pair similarity (Zhao et al, 2004), phrase frequency, phrase length (Hopkins and May, 2011), and phrase generative probability (Foster et al, 2010), which also show further improvement for new phrase feature learning in our experiments",P14-1012,Method_Citation,D10-1044,'133',"<S sid=""133"" ssid=""2"">It is difficult to directly compare the Matsoukas et al results with ours, since our out-of-domain corpus is homogeneous; given heterogeneous training data, however, it would be trivial to include Matsoukas-style identity features in our instance-weighting model.</S>","<S sid=""133"" ssid=""2"">It is difficult to directly compare the Matsoukas et al results with ours, since our out-of-domain corpus is homogeneous; given heterogeneous training data, however, it would be trivial to include Matsoukas-style identity features in our instance-weighting model.</S>",D10-1044.csv
