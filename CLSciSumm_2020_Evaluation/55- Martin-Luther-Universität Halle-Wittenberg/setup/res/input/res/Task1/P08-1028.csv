Citance Number,Citation Marker,Citation Marker Offset,Citation Offset,Citation Text,Citing Article,Discourse Facet,Reference Article,Reference Offset,Reference Text,Reference Text Clean,fileName
1,2008,0,0,"Mitchell and Lapata (2008) propose a framework to represent the meaning of the combination p+ a as a function f operating on four components: c= f (p, a, R, K) (3) R is the relation holding between p and a, and Kadditional knowledge",D08-1094,Implication_Citation,P08-1028,'96',"<S sid=""96"" ssid=""9"">Any adequate model of composition must be able to represent argument-verb meaning.</S>","<S sid=""96"" ssid=""9"">Any adequate model of composition must be able to represent argument-verb meaning.</S>",P08-1028.csv
4,2008,0,0,"While works such asthe SDSM model suffer from the problem of sparsity in composing structures beyond bi grams and trigrams, methods such as Mitchell and Lapata (2008) and (Socher et al, 2012) and Grefenstetteand Sadrzadeh (2011) are restricted by significant model biases in representing semantic com position by generic algebraic operations",P14-1060,Method_Citation,P08-1028,'191',"<S sid=""191"" ssid=""3"">Despite the popularity of additive models, our experimental results showed the superiority of models utilizing multiplicative combinations, at least for the sentence similarity task attempted here.</S>","<S sid=""191"" ssid=""3"">Despite the popularity of additive models, our experimental results showed the superiority of models utilizing multiplicative combinations, at least for the sentence similarity task attempted here.</S>",P08-1028.csv
6,2008,0,0,"Mitchell and Lapata (2008), henceforth M& amp; L, propose a general framework in which meaning representations for complex expressions are computed compositionally by combining the vector representations of the individual words of the complex expression",P10-1097,Method_Citation,P08-1028,'35',"<S sid=""35"" ssid=""8"">The tensor product u &#174; v is a matrix whose components are all the possible products uivj of the components of vectors u and v. A major difficulty with tensor products is their dimensionality which is higher than the dimensionality of the original vectors (precisely, the tensor product has dimensionality m x n).</S>","<S sid=""35"" ssid=""8"">The tensor product u &#174; v is a matrix whose components are all the possible products uivj of the components of vectors u and v. A major difficulty with tensor products is their dimensionality which is higher than the dimensionality of the original vectors (precisely, the tensor product has dimensionality m x n).</S>",P08-1028.csv
7,2008,0,0,"Interestingly, Mitchell and Lapata (2008) came to the same result in a different setting",P10-1097,Method_Citation,P08-1028,'111',"<S sid=""111"" ssid=""24"">Our items were converted into simple sentences (all in past tense) by adding articles where appropriate.</S>","<S sid=""111"" ssid=""24"">Our items were converted into simple sentences (all in past tense) by adding articles where appropriate.</S>",P08-1028.csv
8,2008,0,0,"And Mitchell and Lapata (2008) propose a model for vector composition, focusing on the different functions that might be used to combine the constituent vectors",D11-1094,Method_Citation,P08-1028,'21',"<S sid=""21"" ssid=""17"">Central in these models is the notion of compositionality &#8212; the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them.</S>","<S sid=""21"" ssid=""17"">Central in these models is the notion of compositionality &#8212; the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them.</S>",P08-1028.csv
9,2008,0,0,"Mitchell and Lapata (2008) provide a general framework for semantic vector composition: p= f (u, v, R, K) (1) 295 where u and v are the vectors to be composed, R is syntactic context, K is a semantic knowledge base, and p is a resulting composed vector (or tensor)",W11-0131,Method_Citation,P08-1028,'73',"<S sid=""73"" ssid=""21"">Relaxing the assumption of symmetry in the case of the simple additive model produces a model which weighs the contribution of the two components differently: This allows additive models to become more syntax aware, since semantically important constituents can participate more actively in the composition.</S>","<S sid=""73"" ssid=""21"">Relaxing the assumption of symmetry in the case of the simple additive model produces a model which weighs the contribution of the two components differently: This allows additive models to become more syntax aware, since semantically important constituents can participate more actively in the composition.</S>",P08-1028.csv
10,2008,0,0,"As Mitchell and Lapata (2008) did, let us temporarily suspend discussion on what semantics populate our vectors for now",W11-0131,Method_Citation,P08-1028,'48',"<S sid=""48"" ssid=""21"">The idea is to add not only the vectors representing the predicate and its argument but also the neighbors associated with both of them.</S>","<S sid=""48"" ssid=""21"">The idea is to add not only the vectors representing the predicate and its argument but also the neighbors associated with both of them.</S>",P08-1028.csv
11,2008,0,0,"Mitchell and Lapata (2008) propose a framework to define the composition c= f (a, b, r, K) where r is the relation between a and b, and K is the additional knowledge used to define composition",P13-2083,Method_Citation,P08-1028,'73',"<S sid=""73"" ssid=""21"">Relaxing the assumption of symmetry in the case of the simple additive model produces a model which weighs the contribution of the two components differently: This allows additive models to become more syntax aware, since semantically important constituents can participate more actively in the composition.</S>","<S sid=""73"" ssid=""21"">Relaxing the assumption of symmetry in the case of the simple additive model produces a model which weighs the contribution of the two components differently: This allows additive models to become more syntax aware, since semantically important constituents can participate more actively in the composition.</S>",P08-1028.csv
12,"Mitchell and Lapata, 2008",0,0,"As our final set of baselines, we extend two simple techniques proposed by (Mitchell and Lapata, 2008) that use element-wise addition and multiplication operators to perform composition",P13-2083,Method_Citation,P08-1028,'136',"<S sid=""136"" ssid=""49"">We believe that this level of agreement is satisfactory given that naive subjects are asked to provide judgments on fine-grained semantic distinctions (see Table 1).</S>","<S sid=""136"" ssid=""49"">We believe that this level of agreement is satisfactory given that naive subjects are asked to provide judgments on fine-grained semantic distinctions (see Table 1).</S>",P08-1028.csv
13,2008,0,0,"Althoughthis model has been shown to successfully simulate single and multiple-word priming (McDonald and Brew 2004), it failed to predict processing costs in the Embra eye-tracking corpus (McDonald and Shillcock 2003) .In this work we model semantic constraint using the representational framework put forward in Mitchell and Lapata (2008)",P10-1021,Method_Citation,P08-1028,'197',"<S sid=""197"" ssid=""9"">We anticipate that more substantial correlations can be achieved by implementing more sophisticated models from within the framework outlined here.</S>","<S sid=""197"" ssid=""9"">We anticipate that more substantial correlations can be achieved by implementing more sophisticated models from within the framework outlined here.</S>",P08-1028.csv
14,2008,0,0,"Assuming that h is a linear function of the Cartesian product of u and v allows to specify additive models which are by far the most common method of vector combination in the literature: hi =ui+ vi (3) Alternatively, we can assume that h is a linear function of the tensor product of u and v, and thus derive models based on multiplication: hi =ui? vi (4) Mitchell and Lapata (2008) show that several additive and multiplicative models can be formulated under this framework, including the well known tensor products (Smolensky 1990) and circular convolution (Plate 1995)",P10-1021,Results_Citation,P08-1028,'40',"<S sid=""40"" ssid=""13"">Noisy versions of the original vectors can be recovered by means of circular correlation which is the approximate inverse of circular convolution.</S>","<S sid=""40"" ssid=""13"">Noisy versions of the original vectors can be recovered by means of circular correlation which is the approximate inverse of circular convolution.</S>",P08-1028.csv
15,2008,0,0,"Mitchell and Lapata (2008) introduce a whole family of models of compositionality based on vector addition and point wise-multiplication (and a weighted combination of both), evaluated on a sentence similarity task inspired by Kintsch (2001)",W11-0115,Results_Citation,P08-1028,'188',"<S sid=""188"" ssid=""22"">Also note that in contrast to the combined model, the multiplicative model does not have any free parameters and hence does not require optimization for this particular task.</S>","<S sid=""188"" ssid=""22"">Also note that in contrast to the combined model, the multiplicative model does not have any free parameters and hence does not require optimization for this particular task.</S>",P08-1028.csv
16,2008,0,0,"The approach proposed by Guevara (2010) is really only an extension of the full additive model of Mitchell and Lapata (2008), the only difference being that adopting a supervised learning methodology ensures that the weight parameters in the function are estimated optimally by linear regression",W11-0115,Results_Citation,P08-1028,'198',"<S sid=""198"" ssid=""10"">In particular, the general class of multiplicative models (see equation (4)) appears to be a fruitful area to explore.</S>","<S sid=""198"" ssid=""10"">In particular, the general class of multiplicative models (see equation (4)) appears to be a fruitful area to explore.</S>",P08-1028.csv
17,2008,0,0,"For example, Mitchell and Lapata (2008) use their models to approximate the human ratings in their sentence similarity dataset",W11-0115,Method_Citation,P08-1028,'191',"<S sid=""191"" ssid=""3"">Despite the popularity of additive models, our experimental results showed the superiority of models utilizing multiplicative combinations, at least for the sentence similarity task attempted here.</S>","<S sid=""191"" ssid=""3"">Despite the popularity of additive models, our experimental results showed the superiority of models utilizing multiplicative combinations, at least for the sentence similarity task attempted here.</S>",P08-1028.csv
18,2008,0,0,We use other WSM settings following Mitchell and Lapata (2008),W11-1310,Implication_Citation,P08-1028,'109',"<S sid=""109"" ssid=""22"">These were further pretested to allow the selection of a subset of items showing clear variations in sense as we wanted to have a balanced set of similar and dissimilar sentences.</S>","<S sid=""109"" ssid=""22"">These were further pretested to allow the selection of a subset of items showing clear variations in sense as we wanted to have a balanced set of similar and dissimilar sentences.</S>",P08-1028.csv
19,2008,0,0,Mitchell and Lapata (2008) observed that a simple multiplication function modelled compositionality better than addition,W11-1310,Method_Citation,P08-1028,'190',"<S sid=""190"" ssid=""2"">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>","<S sid=""190"" ssid=""2"">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>",P08-1028.csv
20,"Mitchell and Lapata, 2008",0,0,"We use the compositionality functions, simple addition and simple multiplication to build compositional vectors Vwr1+wr2 and Vwr1 ?wr2. These are as described in (Mitchell and Lapata, 2008)",W11-1310,Results_Citation,P08-1028,'151',"<S sid=""151"" ssid=""64"">In our case, these are the subject noun and the intransitive verb.</S>","<S sid=""151"" ssid=""64"">In our case, these are the subject noun and the intransitive verb.</S>",P08-1028.csv
