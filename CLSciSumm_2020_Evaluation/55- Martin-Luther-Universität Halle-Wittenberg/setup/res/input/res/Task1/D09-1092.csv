Citance Number,Citation Marker,Citation Marker Offset,Citation Offset,Citation Text,Citing Article,Discourse Facet,Reference Article,Reference Offset,Reference Text,Reference Text Clean,fileName
1,"Mimno et al, 2009",0,0,"This configuration is similar to PolyLDA (Mimno et al, 2009) or LinkLDA (Yano et al, 2009), such that utterances from different parties are treated as different languages or blog-post and comments pairs",P14-1004,Method_Citation,D09-1092,'3',"<S sid=""3"" ssid=""3"">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S>","<S sid=""3"" ssid=""3"">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S>",D09-1092.csv
2,"Mimno et al, 2009",0,0,"Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al, 2009), modeling topic-aligned articles in different languages (Mimno et al, 2009), and word sense induction (Brody and Lapata, 2009)",P10-1044,Results_Citation,D09-1092,'9',"<S sid=""9"" ssid=""5"">In this paper, we present the polylingual topic model (PLTM).</S>","<S sid=""9"" ssid=""5"">In this paper, we present the polylingual topic model (PLTM).</S>",D09-1092.csv
3,"Mimno et al, 2009",0,0,"(Mimno et al, 2009) retrieve a list of potential translations simply by selecting a small number N of the most probable words in both languages and then add the Cartesian product of these sets for every topic to a set of candidate translations",P11-2084,Results_Citation,D09-1092,'184',"<S sid=""184"" ssid=""18"">Interestingly, we find that almost all languages in our corpus, including several pairs that have historically been in conflict, show average JS divergences of between approximately 0.08 and 0.12 for T = 400, consistent with our findings for EuroParl translations.</S>","<S sid=""184"" ssid=""18"">Interestingly, we find that almost all languages in our corpus, including several pairs that have historically been in conflict, show average JS divergences of between approximately 0.08 and 0.12 for T = 400, consistent with our findings for EuroParl translations.</S>",D09-1092.csv
4,"Mimno et al, 2009",0,0,"Our Wikipedia-based topic similarity feature, w (f, e), is similar in spirit to polylingualtopic models (Mimno et al 2009), but it is scalable to full bilingual lexicon induction",E12-1014,Method_Citation,D09-1092,'41',"<S sid=""41"" ssid=""7"">(3) The graphical model is shown in figure 1.</S>","<S sid=""41"" ssid=""7"">(3) The graphical model is shown in figure 1.</S>",D09-1092.csv
5,"Mimno et al, 2009",0,0,"of English document and the second half of its aligned foreign language document (Mimno et al,2009)",D11-1086,Implication_Citation,D09-1092,'179',"<S sid=""179"" ssid=""13"">Even with these restrictions, the size of the corpus is 148.5 million words.</S>","<S sid=""179"" ssid=""13"">Even with these restrictions, the size of the corpus is 148.5 million words.</S>",D09-1092.csv
6,"Mimno et al, 2009",0,0,"Since the PLTM is not a contribution of this paper, we refer the interested reader to (Mimno et al, 2009) for more details",N12-1007,Method_Citation,D09-1092,'31',"<S sid=""31"" ssid=""7"">They also provide little analysis of the differences between polylingual and single-language topic models.</S>","<S sid=""31"" ssid=""7"">They also provide little analysis of the differences between polylingual and single-language topic models.</S>",D09-1092.csv
7,2009,0,0,"Evaluation Corpus The automatic evaluation of cross-lingual co reference systems requires annotated 10Mimno et al (2009) showed that so long as the proportion of topically-aligned to non-aligned documents exceeded 0.25, the topic distributions (as measured by mean Jensen-Shannon Divergence between distributions) did not degrade significantly",N12-1007,Method_Citation,D09-1092,'19',"<S sid=""19"" ssid=""15"">We employ a set of direct translations, the EuroParl corpus, to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S>","<S sid=""19"" ssid=""15"">We employ a set of direct translations, the EuroParl corpus, to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S>",D09-1092.csv
8,"Mimno et al, 2009",0,0,"Similarly, Polylingual Topic Models (PLTM) (Mimno et al, 2009) generalized LDA to tuples of documents from multiple languages",D10-1025,Method_Citation,D09-1092,'17',"<S sid=""17"" ssid=""13"">We argue that topic modeling is both a useful and appropriate tool for leveraging correspondences between semantically comparable documents in multiple different languages.</S>","<S sid=""17"" ssid=""13"">We argue that topic modeling is both a useful and appropriate tool for leveraging correspondences between semantically comparable documents in multiple different languages.</S>",D09-1092.csv
9,"Mimno et al, 2009",0,0,"Our baseline joint PLSA model (JPLSA) is closely related to the poly-lingual LDA model of (Mimno et al, 2009)",D10-1025,Method_Citation,D09-1092,'60',"<S sid=""60"" ssid=""9"">Details by language are shown in Table 1.</S>","<S sid=""60"" ssid=""9"">Details by language are shown in Table 1.</S>",D09-1092.csv
10,"Mimno et al, 2009",0,0,"We describe the model for two languages, but it is straightforward to generalize to more than two languages, as in (Mimno et al, 2009)",D10-1025,Hypothesis_Citation,D09-1092,'143',"<S sid=""143"" ssid=""92"">We also do not count morphological variants: the model finds EN &#8220;rules&#8221; and DE &#8220;vorschriften,&#8221; but the lexicon contains only &#8220;rule&#8221; and &#8220;vorschrift.&#8221; Results remain strong as we increase K. With K = 3, T = 800, 1349 of the 7200 candidate pairs for Spanish appeared in the lexicon. topic in different languages translations of each other?</S>","<S sid=""143"" ssid=""92"">We also do not count morphological variants: the model finds EN &#8220;rules&#8221; and DE &#8220;vorschriften,&#8221; but the lexicon contains only &#8220;rule&#8221; and &#8220;vorschrift.&#8221; Results remain strong as we increase K. With K = 3, T = 800, 1349 of the 7200 candidate pairs for Spanish appeared in the lexicon. topic in different languages translations of each other?</S>",D09-1092.csv
11,"Mimno et al, 2009",0,0,"j=1 P (z2j|?) P (w 2 j| ?z2j) The difference between the JPLSA model and the poly-lingual topic model of (Mimno et al, 2009) is that we merge the vocabularies in the two languages and learn topic-specific word distributions over these merged vocabularies, instead of having pairs of topic-specific word distributions, one for each language, like in (Mimno et al, 2009)",D10-1025,Implication_Citation,D09-1092,'29',"<S sid=""29"" ssid=""5"">A recent extended abstract, developed concurrently by Ni et al. (Ni et al., 2009), discusses a multilingual topic model similar to the one presented here.</S>","<S sid=""29"" ssid=""5"">A recent extended abstract, developed concurrently by Ni et al. (Ni et al., 2009), discusses a multilingual topic model similar to the one presented here.</S>",D09-1092.csv
12,"Mimno et al, 2009",0,0,"Another difference between our model and the poly-lingual LDA model of (Mimno et al, 2009) is that we use maximum aposteriori (MAP) instead of Bayesian inference",D10-1025,Method_Citation,D09-1092,'163',"<S sid=""163"" ssid=""112"">Performance continues to improve with longer documents, most likely due to better topic inference.</S>","<S sid=""163"" ssid=""112"">Performance continues to improve with longer documents, most likely due to better topic inference.</S>",D09-1092.csv
13,"Mimno et al, 2009",0,0,"For computing distance we used the L1-norm of the difference, which worked a bit better than the Jensen Shannon divergence between the topic vectors used in (Mimno et al, 2009)",D10-1025,Method_Citation,D09-1092,'89',"<S sid=""89"" ssid=""38"">Analytically calculating the probability of a set of held-out document tuples given &#934;1, ... , &#934;L and &#945;m is intractable, due to the summation over an exponential number of topic assignments for these held-out documents.</S>","<S sid=""89"" ssid=""38"">Analytically calculating the probability of a set of held-out document tuples given &#934;1, ... , &#934;L and &#945;m is intractable, due to the summation over an exponential number of topic assignments for these held-out documents.</S>",D09-1092.csv
15,"Mimno et al, 2009",0,0,"In previously reported work, (Mimno et al, 2009) evaluate parallel document retrieval using PLTM on Europarl speeches in English and Spanish, using training and test sets of size similar to ours",D10-1025,Implication_Citation,D09-1092,'177',"<S sid=""177"" ssid=""11"">In the English version of Wikipedia we dropped all articles that were not linked to by any other language in our set.</S>","<S sid=""177"" ssid=""11"">In the English version of Wikipedia we dropped all articles that were not linked to by any other language in our set.</S>",D09-1092.csv
16,"Mimno et al, 2009",0,0,"We assume that a higher dimensionality can lead to a better repartitioning of the vocabulary over the topics. Multilingual LDA has been used before in natural language processing ,e.g .polylingual topic models (Mimno et al, 2009) or multilingual topic models for unaligned text (Boyd-Graber and Blei, 2009)",W12-3117,Implication_Citation,D09-1092,'29',"<S sid=""29"" ssid=""5"">A recent extended abstract, developed concurrently by Ni et al. (Ni et al., 2009), discusses a multilingual topic model similar to the one presented here.</S>","<S sid=""29"" ssid=""5"">A recent extended abstract, developed concurrently by Ni et al. (Ni et al., 2009), discusses a multilingual topic model similar to the one presented here.</S>",D09-1092.csv
17,2009,0,0,"ji =wjk? M j i? m k=1w j k, (1) where M j is the topic distribution of document j and wk is the number of occurrences of phrase pair Xk in document j. Mimno et al (2009) extend the original concept of LDA to support polylingual topic models (PLTM), both on parallel (such as EuroParl) and partly comparable documents (such as Wikipediaarticles)",W11-2133,Method_Citation,D09-1092,'110',"<S sid=""110"" ssid=""59"">Continuing with the example above, one might extract a set of connected Wikipedia articles related to the focus of the journal and then train PLTM on a joint corpus consisting of journal papers and Wikipedia articles.</S>","<S sid=""110"" ssid=""59"">Continuing with the example above, one might extract a set of connected Wikipedia articles related to the focus of the journal and then train PLTM on a joint corpus consisting of journal papers and Wikipedia articles.</S>",D09-1092.csv
18,2009,0,0,Tuple-specific topic distributions arelearned using LDA with distinct topic-word concentration parameters? l. Mimno et al (2009) show that PLTM sufficiently aligns topics in parallel corpora,W11-2133,Method_Citation,D09-1092,'194',"<S sid=""194"" ssid=""3"">We also demonstrated that relatively small numbers of topically comparable document tuples are sufficient to align topics between languages in non-comparable corpora.</S>","<S sid=""194"" ssid=""3"">We also demonstrated that relatively small numbers of topically comparable document tuples are sufficient to align topics between languages in non-comparable corpora.</S>",D09-1092.csv
19,"Mimno et al, 2009",0,0,"A good candidate for multilingual topic analyses are polylin gual topic models (Mimno et al, 2009), which learn topics for multiple languages, creating tuples of language specific distributions over monolingual vocabularies for each topic",P14-2110,Method_Citation,D09-1092,'187',"<S sid=""187"" ssid=""21"">To demonstrate the wide variation in topics, we calculated the proportion of tokens in each language assigned to each topic.</S>","<S sid=""187"" ssid=""21"">To demonstrate the wide variation in topics, we calculated the proportion of tokens in each language assigned to each topic.</S>",D09-1092.csv
20,2009,0,0,"3csLDATo train a polylingual topic model on social media, we make two modifications to the model of Mimno et al (2009): add a token specific language variable, and a process for identifying aligned top ics. First ,polylingual topic models require parallel or comparable corpora in which each document has an assigned language",P14-2110,Results_Citation,D09-1092,'170',"<S sid=""170"" ssid=""4"">First, we explore whether comparable document tuples support the alignment of fine-grained topics, as demonstrated earlier using parallel documents.</S>","<S sid=""170"" ssid=""4"">First, we explore whether comparable document tuples support the alignment of fine-grained topics, as demonstrated earlier using parallel documents.</S>",D09-1092.csv
