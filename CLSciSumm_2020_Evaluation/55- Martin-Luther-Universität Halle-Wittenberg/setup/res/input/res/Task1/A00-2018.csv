Citance Number,Citation Marker,Citation Marker Offset,Citation Offset,Citation Text,Citing Article,Discourse Facet,Reference Article,Reference Offset,Reference Text,Reference Text Clean,fileName
2,"Charniak, 2000",0,0,"As a benchmark VPC extraction system, we use the Charniak parser (Charniak, 2000)",N10-1002,Method_Citation,A00-2018,'113',"<S sid=""113"" ssid=""4"">We take as our starting point the parser labled Char97 in Figure 1 [5], as that is the program from which our current parser derives.</S>","<S sid=""113"" ssid=""4"">We take as our starting point the parser labled Char97 in Figure 1 [5], as that is the program from which our current parser derives.</S>",A00-2018.csv
3,"Charniak, 2000",0,0,"Each of these scores can be calculated from a provided syntactic parse tree, and to generate these we made use of the Charniakparser (Charniak, 2000), also trained on the Switch board tree bank",W11-0610,Method_Citation,A00-2018,'113',"<S sid=""113"" ssid=""4"">We take as our starting point the parser labled Char97 in Figure 1 [5], as that is the program from which our current parser derives.</S>","<S sid=""113"" ssid=""4"">We take as our starting point the parser labled Char97 in Figure 1 [5], as that is the program from which our current parser derives.</S>",A00-2018.csv
4,"Charniak, 2000",0,0,"We then use Charniak? s parser (Charniak, 2000) to generate the most likely parse tree for each English target sentence in the training corpus",W06-3119,Implication_Citation,A00-2018,'170',"<S sid=""170"" ssid=""61"">That is, the PCFG grammar rules are read directly off the training corpus.</S>","<S sid=""170"" ssid=""61"">That is, the PCFG grammar rules are read directly off the training corpus.</S>",A00-2018.csv
5,"Charniak, 2000",0,0,"We were interested in the occurrence of features such as type and number of premodifiers, presence and type of post modifiers, and form of name reference for people. We constructed a large, automatically annotated corpus by merging the output of Charniak? s statistical parser (Charniak, 2000) with that of the IBM named entity recognition system Nominator (Wacholder et al,1997)",N03-2024,Method_Citation,A00-2018,'19',"<S sid=""19"" ssid=""8"">The method we use follows that of [10].</S>","<S sid=""19"" ssid=""8"">The method we use follows that of [10].</S>",A00-2018.csv
6,"Charniak, 2000",0,0,"After getting a set of basic clusters, we pass them to an existing statistical parser (Charniak, 2000) and rule-based tree normalizer to obtain a GLARFstructure for each sentence in every article",N06-1039,Implication_Citation,A00-2018,'137',"<S sid=""137"" ssid=""28"">However, Collins in [10] does not stress the decision to guess the head's pre-terminal first, and it might be lost on the casual reader.</S>","<S sid=""137"" ssid=""28"">However, Collins in [10] does not stress the decision to guess the head's pre-terminal first, and it might be lost on the casual reader.</S>",A00-2018.csv
7,2000,0,0,"The levels of accuracy and robustness recently achieved by statistical parsers (e.g. Collins (1999), Charniak (2000)) have led to their use in a number of NLP applications, such as question-answering (Pasca and Harabagiu, 2001), machine translation (Charniak et al, 2003), sentence simplification (Carroll et al, 1999), and a linguist? s search engine (Resnik and Elkiss, 2003)",C04-1180,Method_Citation,A00-2018,'19',"<S sid=""19"" ssid=""8"">The method we use follows that of [10].</S>","<S sid=""19"" ssid=""8"">The method we use follows that of [10].</S>",A00-2018.csv
8,"Charniak, 2000",0,0,"In CoNLL-2005, full parsing trees are provided by two full parsers: the Collins parser (Collins, 1999) and the Charniak parser (Charniak, 2000)",W05-0638,Implication_Citation,A00-2018,'116',"<S sid=""116"" ssid=""7"">This is as opposed to the &amp;quot;Markovgrammar&amp;quot; approach used in the current parser.</S>","<S sid=""116"" ssid=""7"">This is as opposed to the &amp;quot;Markovgrammar&amp;quot; approach used in the current parser.</S>",A00-2018.csv
9,"Charniak, 2000",0,0,"We also use a standard statistical parser (Charniak, 2000) to provide syntactic analysis",P05-1065,Results_Citation,A00-2018,'40',"<S sid=""40"" ssid=""9"">In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.</S>","<S sid=""40"" ssid=""9"">In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.</S>",A00-2018.csv
10,"Charniak, 2000",0,0,"For each article, we calculated the per cent age of a) all word instances (tokens) and b) all unique words (types) not on these lists, resulting in three token OOV rate features and three type OOV rate features per article. The parse features are generated using the Charniak parser (Charniak, 2000) trained on the standard Wall Street Journal Treebank corpus",P05-1065,Implication_Citation,A00-2018,'126',"<S sid=""126"" ssid=""17"">This is indicated in Figure 2, where the model labeled &amp;quot;Best&amp;quot; has precision of 89.8% and recall of 89.6% for an average of 89.7%, 0.4% lower than the results on the official test corpus.</S>","<S sid=""126"" ssid=""17"">This is indicated in Figure 2, where the model labeled &amp;quot;Best&amp;quot; has precision of 89.8% and recall of 89.6% for an average of 89.7%, 0.4% lower than the results on the official test corpus.</S>",A00-2018.csv
11,2000,0,0,"The evaluation of the transformed output of the parsers of Charniak (2000) and Collins (1999) gives 90 %unlabelled and 84 %labelled accuracy with respect to dependencies, when measured against a dependency corpus derived from the Penn Treebank.The paper is organized as follows",P04-1040,Method_Citation,A00-2018,'120',"<S sid=""120"" ssid=""11"">Second, Char97 uses unsupervised learning in that the original system was run on about thirty million words of unparsed text, the output was taken as &amp;quot;correct&amp;quot;, and statistics were collected on the resulting parses.</S>","<S sid=""120"" ssid=""11"">Second, Char97 uses unsupervised learning in that the original system was run on about thirty million words of unparsed text, the output was taken as &amp;quot;correct&amp;quot;, and statistics were collected on the resulting parses.</S>",A00-2018.csv
12,2000,0,0,"Blaheta and Charniak (2000) presented the first method for assigning Pennfunctional tags to constituents identified by a parser. Pattern-matching approaches were used in (John son, 2002) and (Jijkoun, 2003) to recover non-local dependencies in phrase trees",P04-1040,Results_Citation,A00-2018,'157',"<S sid=""157"" ssid=""48"">Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure.</S>","<S sid=""157"" ssid=""48"">Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure.</S>",A00-2018.csv
13,2000,0,0,"As an alternative to hard coded heuristics, Blaheta and Charniak (2000) proposed to recover the Penn functional tags automatically",P04-1040,Method_Citation,A00-2018,'91',"<S sid=""91"" ssid=""2"">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>","<S sid=""91"" ssid=""2"">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>",A00-2018.csv
17,2000,0,0,"The parser of Charniak (2000) is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents",N06-1022,Results_Citation,A00-2018,'13',"<S sid=""13"" ssid=""2"">Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g., whether it is a noun phrase (np), verb-phrase, etc.) and H (c) is the relevant history of c &#8212; information outside c that our probability model deems important in determining the probability in question.</S>","<S sid=""13"" ssid=""2"">Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g., whether it is a noun phrase (np), verb-phrase, etc.) and H (c) is the relevant history of c &#8212; information outside c that our probability model deems important in determining the probability in question.</S>",A00-2018.csv
18,2000,0,0,"Most recently, McDonald et al (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000))",N06-1022,Method_Citation,A00-2018,'167',"<S sid=""167"" ssid=""58"">Including this information within a standard deleted-interpolation model causes a 0.6% decrease from the results using the less conventional model.</S>","<S sid=""167"" ssid=""58"">Including this information within a standard deleted-interpolation model causes a 0.6% decrease from the results using the less conventional model.</S>",A00-2018.csv
19,"Charniak, 2000",0,0,"The feature set contains complex information extracted automatically from candidate syntax trees generated by parsing (Charniak, 2000), trees that will be improved by more accurate PP-attachment decisions",H05-1035,Method_Citation,A00-2018,'142',"<S sid=""142"" ssid=""33"">We believe that two factors contribute to this performance gain.</S>","<S sid=""142"" ssid=""33"">We believe that two factors contribute to this performance gain.</S>",A00-2018.csv
20,2000,0,0,"Note that the dependency figures of Dienes lag behind even the parsed results for Johnson? s model; this may well be due to the fact that Dienes built his model as an extension of Collins (1999), which lags behind Charniak (2000) by about 1.3-1.5% .Manual investigation of errors on English gold standard data revealed two major issues that suggest further potential for improvement in performance without further increase in algorithmic complexity or training set size",P04-1042,Implication_Citation,A00-2018,'49',"<S sid=""49"" ssid=""18"">First, as already implicit in our discussion, factoring the probability computation into a sequence of values corresponding to various &amp;quot;features&amp;quot; suggests that the probability model should be easily changeable &#8212; just change the set of features used.</S>","<S sid=""49"" ssid=""18"">First, as already implicit in our discussion, factoring the probability computation into a sequence of values corresponding to various &amp;quot;features&amp;quot; suggests that the probability model should be easily changeable &#8212; just change the set of features used.</S>",A00-2018.csv
