Citance Number,Citation Marker,Citation Marker Offset,Citation Offset,Citation Text,Citing Article,Discourse Facet,Reference Article,Reference Offset,Reference Text,Reference Text Clean,fileName
1,"Heafield, 2011",0,0,"We used common tools for phrase-based translation? Moses (Koehn et al, 2007) decoder and tools, SRILM (Stolcke, 2002) and KenLM (Heafield, 2011) for language modelling and GIZA++ (Och and Ney, 2000) for word alignments",W11-2138,Method_Citation,W11-2123,'58',"<S sid=""58"" ssid=""36"">Given a key k, it estimates the position If the estimate is exact (A[pivot] = k), then the algorithm terminates succesfully.</S>","<S sid=""58"" ssid=""36"">Given a key k, it estimates the position If the estimate is exact (A[pivot] = k), then the algorithm terminates succesfully.</S>",W11-2123.csv
2,"Heafield, 2011",0,0,"The language model was com piled into KenLM probing format (Heafield, 2011) and placed in RAM while text phrase tables were forced into the disk cache before each run",P14-2022,Implication_Citation,W11-2123,'223',"<S sid=""223"" ssid=""42"">The binary language model from Section 5.2 and text phrase table were forced into disk cache before each run.</S>","<S sid=""223"" ssid=""42"">The binary language model from Section 5.2 and text phrase table were forced into disk cache before each run.</S>",W11-2123.csv
3,"Heafield, 2011",0,0,"Thus given afragment tf consisting of a sequence of target to kens, we compute LM scores for (i) < s& gt ;tf, (ii )tf and (iii )tf < /s& gt; and use the best score (only) for pruning.2 While this increases the number ofLM queries, we exploit the language model state in formation in KenLM (Heafield, 2011) to optimize the queries by saving the scores for the unchanged states",W12-3145,Method_Citation,W11-2123,'216',"<S sid=""216"" ssid=""35"">Though we are not able to calculate their memory usage on our model, results reported in their paper suggest lower memory consumption than TRIE on large-scale models, at the expense of CPU time.</S>","<S sid=""216"" ssid=""35"">Though we are not able to calculate their memory usage on our model, results reported in their paper suggest lower memory consumption than TRIE on large-scale models, at the expense of CPU time.</S>",W11-2123.csv
4,"Heafield, 2011",0,0,"Our translation system uses cdec (Dyer et al,2010), an implementation of the hierarchical phrase based translation model (Chiang, 2007) that uses the KenLM library (Heafield, 2011) for language model inference",W12-3131,Results_Citation,W11-2123,'47',"<S sid=""47"" ssid=""25"">For 2 &lt; n &lt; N, we use a hash table mapping from the n-gram to the probability and backoff3.</S>","<S sid=""47"" ssid=""25"">For 2 &lt; n &lt; N, we use a hash table mapping from the n-gram to the probability and backoff3.</S>",W11-2123.csv
5,"Heafield, 2011",0,0,"The three data sets in use in this paper are summarised in Table 1.The translation systems consisted of phrase tables and lexicalised reordering tables estimated using the standard Moses (Koehn et al, 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime",W12-3154,Method_Citation,W11-2123,'46',"<S sid=""46"" ssid=""24"">Unigram lookup is dense so we use an array of probability and backoff values.</S>","<S sid=""46"" ssid=""24"">Unigram lookup is dense so we use an array of probability and backoff values.</S>",W11-2123.csv
6,"Heafield, 2011",0,0,"The features used are basic lexical features, word penalty and a 3-gram Language Model (Heafield, 2011)",P12-2058,Method_Citation,W11-2123,'220',"<S sid=""220"" ssid=""39"">Process statistics are already collected by the kernel (and printing them has no meaningful impact on performance).</S>","<S sid=""220"" ssid=""39"">Process statistics are already collected by the kernel (and printing them has no meaningful impact on performance).</S>",W11-2123.csv
7,2011,0,0,Inference was carried out using the language modeling library described by Heafield (2011),W11-2139,Method_Citation,W11-2123,'123',"<S sid=""123"" ssid=""27"">The 1-bit sign is almost always negative and the 8-bit exponent is not fully used on the range of values, so in practice this corresponds to quantization ranging from 17 to 20 total bits.</S>","<S sid=""123"" ssid=""27"">The 1-bit sign is almost always negative and the 8-bit exponent is not fully used on the range of values, so in practice this corresponds to quantization ranging from 17 to 20 total bits.</S>",W11-2123.csv
8,"Heafield, 2011",0,0,"We used the MADA ATB segmentation for Arabic (Roth et al, 2008) and true casing for English, phrases of maximal length 7, KneserNey smoothing, and lexicalized reordering (Koehn et al, 2005), and a 5-gram language model, trained on GigaWordv.5 using KenLM (Heafield, 2011)",P13-2003,Hypothesis_Citation,W11-2123,'4',"<S sid=""4"" ssid=""4"">Our code is thread-safe, and integrated into the Moses, cdec, and Joshua translation systems.</S>","<S sid=""4"" ssid=""4"">Our code is thread-safe, and integrated into the Moses, cdec, and Joshua translation systems.</S>",W11-2123.csv
9,2011,0,0,The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language mod els by Heafield (2011) and Pauls and Klein (2011)? both language model implementations are now integrated with Joshua,W12-3134,Method_Citation,W11-2123,'237',"<S sid=""237"" ssid=""56"">Moses keeps language models and many other resources in static variables, so these are still resident in memory.</S>","<S sid=""237"" ssid=""56"">Moses keeps language models and many other resources in static variables, so these are still resident in memory.</S>",W11-2123.csv
10,2011,0,0,Our quantization approach follows Federico and Bertoldi (2006) and Heafield (2011) in partitioning the value histogram into 256 equal-sized buckets,W12-3134,Hypothesis_Citation,W11-2123,'4',"<S sid=""4"" ssid=""4"">Our code is thread-safe, and integrated into the Moses, cdec, and Joshua translation systems.</S>","<S sid=""4"" ssid=""4"">Our code is thread-safe, and integrated into the Moses, cdec, and Joshua translation systems.</S>",W11-2123.csv
11,2011,0,0,"With the help of the respective original authors, the language model implementations by Heafield (2011) and Pauls and Klein (2011) have been integrated with Joshua, dropping support for the slower and more difficult to compile SRILM toolkit (Stolcke, 2002)",W12-3134,Results_Citation,W11-2123,'210',"<S sid=""210"" ssid=""29"">In fact, we found that enabling IRSTLM&#8217;s cache made it slightly slower, so results in Table 1 use IRSTLM without caching.</S>","<S sid=""210"" ssid=""29"">In fact, we found that enabling IRSTLM&#8217;s cache made it slightly slower, so results in Table 1 use IRSTLM without caching.</S>",W11-2123.csv
12,"Heafield, 2011",0,0,"This was used to create a KenLM (Heafield, 2011)",W12-3160,Method_Citation,W11-2123,'226',"<S sid=""226"" ssid=""45"">It did 402 queries/ms using 1.80 GB. cMemory use increased during scoring due to batch processing (MIT) or caching (Rand).</S>","<S sid=""226"" ssid=""45"">It did 402 queries/ms using 1.80 GB. cMemory use increased during scoring due to batch processing (MIT) or caching (Rand).</S>",W11-2123.csv
13,"Heafield, 2011",0,0,"In the Opinum system we query the M p, M n mod els with the KenLM (Heafield, 2011) open-source library because it answers the queries very quickly and has a short loading time, which is suitable fora web application",W12-3706,Results_Citation,W11-2123,'174',"<S sid=""174"" ssid=""46"">However, lazy mapping is generally slow because queries against uncached pages must wait for the disk.</S>","<S sid=""174"" ssid=""46"">However, lazy mapping is generally slow because queries against uncached pages must wait for the disk.</S>",W11-2123.csv
14,"Heafield, 2011",0,0,"Our base line is a factored phrase based SMT system that uses the Moses toolkit (Koehn et al, 2007) for translation model training and decoding, GIZA++ (Ochand Ney, 2003) for word alignment, SRILM (Stolcke, 2002) an KenLM (Heafield, 2011) for language modelling and minimum error rate training (Och, 2003) to tune model feature weights",W11-2147,Method_Citation,W11-2123,'222',"<S sid=""222"" ssid=""41"">Since our destructor is an efficient call to munmap, bypassing the destructor favors only other packages.</S>","<S sid=""222"" ssid=""41"">Since our destructor is an efficient call to munmap, bypassing the destructor favors only other packages.</S>",W11-2123.csv
15,"Heafield, 2011",0,0,"For language modeling, we computed 5-gram models using IRSTLM7 (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011)",E12-1083,Method_Citation,W11-2123,'216',"<S sid=""216"" ssid=""35"">Though we are not able to calculate their memory usage on our model, results reported in their paper suggest lower memory consumption than TRIE on large-scale models, at the expense of CPU time.</S>","<S sid=""216"" ssid=""35"">Though we are not able to calculate their memory usage on our model, results reported in their paper suggest lower memory consumption than TRIE on large-scale models, at the expense of CPU time.</S>",W11-2123.csv
16,"Heafield, 2011",0,0,"Furthermore, the extraction of grammars for training is done in a leave-one-out fashion (Zollmann and Sima ?an,2005) where rules are extracted for a parallel sentence pair only if the same rules are found in other sentences of the corpus as well.3-gram (news-commentary) and 5-gram (Europarl) language models are trained on the data described in Table 1, using the SRILM toolkit (Stolcke, 2002) and binarized for efficient querying using kenlm (Heafield, 2011)",P12-1002,Method_Citation,W11-2123,'237',"<S sid=""237"" ssid=""56"">Moses keeps language models and many other resources in static variables, so these are still resident in memory.</S>","<S sid=""237"" ssid=""56"">Moses keeps language models and many other resources in static variables, so these are still resident in memory.</S>",W11-2123.csv
17,"Heafield, 2011",0,0,"n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3",D12-1108,Results_Citation,W11-2123,'47',"<S sid=""47"" ssid=""25"">For 2 &lt; n &lt; N, we use a hash table mapping from the n-gram to the probability and backoff3.</S>","<S sid=""47"" ssid=""25"">For 2 &lt; n &lt; N, we use a hash table mapping from the n-gram to the probability and backoff3.</S>",W11-2123.csv
18,"Heafield, 2011",0,0,"Research efforts to increase search efficiency for phrase-based MT (Koehn et al, 2003) have explored several directions, ranging from generalizing the stack decoding algorithm (Ortiz et al, 2006) to additional early pruning techniques (Delaney et al, 2006), (Moore and Quirk, 2007) and more efficient language model (LM) querying (Heafield, 2011)",P12-2006,Results_Citation,W11-2123,'73',"<S sid=""73"" ssid=""51"">Therefore, for n-gram wn1 , all leftward extensions wn0 are an adjacent block in the n + 1-gram array.</S>","<S sid=""73"" ssid=""51"">Therefore, for n-gram wn1 , all leftward extensions wn0 are an adjacent block in the n + 1-gram array.</S>",W11-2123.csv
19,"Heafield, 2011",0,0,"For English language modeling, we use English Giga word Corpus with 5-gram LM using the KenLM toolkit (Heafield, 2011)",P13-2073,Method_Citation,W11-2123,'156',"<S sid=""156"" ssid=""28"">As noted in Section 1, our code finds the longest matching entry wnf for query p(wn|s(wn&#8722;1 f ) The probability p(wn|wn&#8722;1 f ) is stored with wnf and the backoffs are immediately accessible in the provided state s(wn&#8722;1 When our code walks the data structure to find wnf , it visits wnn, wnn&#8722;1, ... , wnf .</S>","<S sid=""156"" ssid=""28"">As noted in Section 1, our code finds the longest matching entry wnf for query p(wn|s(wn&#8722;1 f ) The probability p(wn|wn&#8722;1 f ) is stored with wnf and the backoffs are immediately accessible in the provided state s(wn&#8722;1 When our code walks the data structure to find wnf , it visits wnn, wnn&#8722;1, ... , wnf .</S>",W11-2123.csv
20,"Heafield, 2011",0,0,"For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with KneserNey smoothing",P13-1109,Method_Citation,W11-2123,'150',"<S sid=""150"" ssid=""22"">Section 4.1 explained that state s is stored by applications with partial hypotheses to determine when they can be recombined.</S>","<S sid=""150"" ssid=""22"">Section 4.1 explained that state s is stored by applications with partial hypotheses to determine when they can be recombined.</S>",W11-2123.csv
