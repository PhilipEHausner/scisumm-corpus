The trie data structure is commonly used for language modeling.
We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.
For the perplexity and translation tasks  we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn  2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.
In a model we built with default settings  1.2% of n + 1-grams were missing their ngram suffix.
We run the baseline Moses system for the French-English track of the 2011 Workshop on Machine Translation 9 translating the 3003-sentence test set.